[
  {
    "objectID": "rf-grid-search.html#data",
    "href": "rf-grid-search.html#data",
    "title": "6  Grid Search",
    "section": "6.1 Data",
    "text": "6.1 Data\nWe use data obtained from UCI Yeh (2016), presenting research customers’ default payments in Taiwan. This dataset contains \\(n = 30,000\\) instances and 23 numeric features. The outcome variable, corresponding to the observed default payment in next month, is positive in 22.12% of cases.\nLet us load the data.\n\ndata_credit &lt;- read.csv2(\n  \"data/default_credit.csv\", \n  header = TRUE, skip = 1\n)\ncolnames(data_credit)[25] &lt;- \"d\"\ndata_credit &lt;- data_credit |&gt; select(-ID)\ndata_credit &lt;- data_credit |&gt; as_tibble( )\ndata_credit &lt;- data_credit |&gt; \n  mutate(\n    across(\n      c(\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \n        \"PAY_4\", \"PAY_5\", \"PAY_6\"), \n      ~as.factor(.x)\n    )\n  )\n\nFollowing the methodology outlined in Subasi A. (2019), we employ the Synthetic Minority Over-sampling Technique (SMOTE) at a rate of 200% to rebalance the data. To do so, we use the smote() function from {performanceEstimation}.\n\nlibrary(performanceEstimation)\n\n\n# need of a factor variable to apply SMOTE function\nset.seed(123)\ndata_credit$d &lt;- as.factor(data_credit$d)\nnew_df &lt;- performanceEstimation::smote(d ~ ., data_credit, perc.over = 2)\n\ndata_credit &lt;- new_df\ndata_credit$d &lt;- as.numeric(data_credit$d) # change in the labels\ndata_credit &lt;- data_credit |&gt; mutate(d = ifelse(d == 1, 1, 2))\ndata_credit &lt;- data_credit |&gt; mutate(d = d-1)\n\nLet us save this dataset for later use in the simulations.\n\nwrite.csv(\n  data_credit, \n  file = \"data/data_credit_smote.csv\",\n  row.names = FALSE\n)\n\nWe split the data into two sets:\n\na training set on which we will train random forests (with 50% of observations)\nthe remaining data which will be further split into a calibration set and a test set.\n\n\nset.seed(1234)\nind_train &lt;- sample(1:nrow(data_credit), size = .5*nrow(data_credit))\ntb_train &lt;- data_credit |&gt; slice(ind_train)\ntb_rest &lt;- data_credit |&gt; slice(-ind_train)\n\nLet us save those files for later use in the smimulations.\n\nwrite.csv(tb_train, \"data/data_credit_smote_train.csv\", row.names=FALSE)\nwrite.csv(tb_rest, \"data/data_credit_smote_rest.csv\", row.names=FALSE)"
  },
  {
    "objectID": "rf-grid-search.html#grid-with-hyperparameters",
    "href": "rf-grid-search.html#grid-with-hyperparameters",
    "title": "6  Grid Search",
    "section": "6.2 Grid With Hyperparameters",
    "text": "6.2 Grid With Hyperparameters\nWe conduct a grid search to find the set of hyperparameters that optimize a criterion:\n\nthe out-of-bag mean squared error (MSE) for the regressor\nthe error rate for the classifier.\n\n\ngrid_params &lt;- \n  expand_grid(\n    num_trees = c(100,300, 500),\n    mtry = seq(1,(ncol(tb_train)/2)),\n    nodesize = c(5, 10, 15, 20),\n    splitrule = \"gini\"\n  )\ngrid_params\n\n# A tibble: 144 × 4\n   num_trees  mtry nodesize splitrule\n       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;    \n 1       100     1        5 gini     \n 2       100     1       10 gini     \n 3       100     1       15 gini     \n 4       100     1       20 gini     \n 5       100     2        5 gini     \n 6       100     2       10 gini     \n 7       100     2       15 gini     \n 8       100     2       20 gini     \n 9       100     3        5 gini     \n10       100     3       10 gini     \n# ℹ 134 more rows"
  },
  {
    "objectID": "rf-grid-search.html#helper-functions",
    "href": "rf-grid-search.html#helper-functions",
    "title": "6  Grid Search",
    "section": "6.3 Helper Functions",
    "text": "6.3 Helper Functions\nLet us define some helper functions to compute the MSE, the accuracy (with a probability threshold of .5) and the AUC (using {pROC}).\n\n#' Mean Squared Error\nmse_function &lt;- function(pred, obs) mean((pred - obs)^2)\n\n#' Accuracy with threshold 0.5\naccuracy_function &lt;- function(pred, obs, threshold = 0.5) {\n  mean((as.numeric(as.character(pred)) &gt; 0.5) == obs)\n}\n\n#' AUC (no threshold)\nauc_function &lt;- function(pred, obs){\n  auc(obs, pred)\n}"
  },
  {
    "objectID": "rf-grid-search.html#estimations",
    "href": "rf-grid-search.html#estimations",
    "title": "6  Grid Search",
    "section": "6.4 Estimations",
    "text": "6.4 Estimations\nLet us now go through the hyperparameter grid.\n\n6.4.1 Regression\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  mse_oob_rf_reg &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        d ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      out_of_bag &lt;- map(.x = 1:nrow(tb_train), .f = ~which(rf[[\"inbag\"]][.x,] == 0))\n      rf_pred_all &lt;- predict(rf, tb_train,\n                             predict.all = TRUE,\n                             type = \"response\")$individual\n      rf_pred &lt;- unlist(map(.x = 1:nrow(tb_train), .f = ~mean(rf_pred_all[.x,out_of_bag[[.x]]])))\n      \n      oob_err &lt;- mse_function(pred = rf_pred, obs = tb_train |&gt; pull(d))\n      mse_oob &lt;- oob_err\n      \n      # Progress bar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        mse_oob = mse_oob\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nLet us order the computed out-of-bag MSE by ascending values:\n\nbest_params_rf_reg &lt;- \n  mse_oob_rf_reg |&gt; list_rbind() |&gt; \n  arrange(mse_oob)\n\nWe save the results for later use.\n\nwrite.csv(\n  best_params_rf_reg, \n  file = \"output/best_params_rf_reg.csv\",\n  row.names = FALSE\n)\n\n\nbest_params_rf_reg\n\n    mtry nodesize num_trees splitrule   mse_oob\n1     12        5       500      gini 0.1161355\n2     11        5       500      gini 0.1167286\n3     10        5       500      gini 0.1167594\n4     11        5       300      gini 0.1168635\n5     10        5       300      gini 0.1170744\n6      9        5       300      gini 0.1172085\n7      9        5       500      gini 0.1172791\n8     12        5       300      gini 0.1173561\n9      8        5       500      gini 0.1175019\n10     8        5       300      gini 0.1181500\n11     7        5       500      gini 0.1182748\n12     6        5       500      gini 0.1184314\n13     7        5       300      gini 0.1187873\n14     6        5       300      gini 0.1194305\n15     5        5       500      gini 0.1195314\n16    11        5       100      gini 0.1195434\n17    12        5       100      gini 0.1198150\n18     9        5       100      gini 0.1198567\n19    10        5       100      gini 0.1201343\n20     4        5       500      gini 0.1206271\n21     7        5       100      gini 0.1206874\n22     5        5       300      gini 0.1206969\n23    12       10       500      gini 0.1208994\n24    11       10       500      gini 0.1210198\n25    11       10       300      gini 0.1211552\n26     8        5       100      gini 0.1211788\n27    12       10       300      gini 0.1212391\n28    10       10       500      gini 0.1213147\n29     4        5       300      gini 0.1214632\n30     9       10       500      gini 0.1217760\n31    10       10       300      gini 0.1220816\n32     9       10       300      gini 0.1222290\n33     6        5       100      gini 0.1222443\n34     8       10       300      gini 0.1222482\n35     8       10       500      gini 0.1225779\n36     7       10       500      gini 0.1226872\n37     3        5       500      gini 0.1227032\n38     5        5       100      gini 0.1230636\n39     3        5       300      gini 0.1230863\n40    11       10       100      gini 0.1232123\n41    12       10       100      gini 0.1234719\n42     4        5       100      gini 0.1234766\n43     7       10       300      gini 0.1236268\n44     6       10       500      gini 0.1236749\n45    10       10       100      gini 0.1238468\n46     6       10       300      gini 0.1241986\n47    12       15       500      gini 0.1243726\n48     8       10       100      gini 0.1245897\n49     5       10       500      gini 0.1248403\n50    11       15       500      gini 0.1248550\n51     9       10       100      gini 0.1248996\n52    11       15       300      gini 0.1250312\n53    12       15       300      gini 0.1251028\n54    10       15       500      gini 0.1252682\n55     5       10       300      gini 0.1253665\n56     7       10       100      gini 0.1255997\n57     3        5       100      gini 0.1257361\n58     9       15       500      gini 0.1258641\n59     6       10       100      gini 0.1259584\n60    10       15       300      gini 0.1259758\n61     4       10       500      gini 0.1263621\n62     9       15       300      gini 0.1264203\n63    12       15       100      gini 0.1265025\n64     8       15       500      gini 0.1265721\n65     2        5       500      gini 0.1266220\n66     8       15       300      gini 0.1267140\n67     4       10       300      gini 0.1267876\n68    10       15       100      gini 0.1268396\n69    11       15       100      gini 0.1269556\n70     2        5       300      gini 0.1271676\n71     7       15       500      gini 0.1273421\n72     7       15       300      gini 0.1276667\n73     5       10       100      gini 0.1279446\n74    12       20       500      gini 0.1280429\n75     9       15       100      gini 0.1280750\n76    12       20       300      gini 0.1281329\n77     6       15       500      gini 0.1281506\n78    11       20       300      gini 0.1281588\n79     3       10       500      gini 0.1281836\n80    10       20       500      gini 0.1282266\n81    11       20       500      gini 0.1282600\n82     6       15       300      gini 0.1282876\n83     3       10       300      gini 0.1285083\n84     8       15       100      gini 0.1289850\n85     5       15       500      gini 0.1290990\n86     9       20       500      gini 0.1291231\n87     5       15       300      gini 0.1291516\n88    10       20       300      gini 0.1291917\n89     4       10       100      gini 0.1292167\n90     2        5       100      gini 0.1296366\n91     8       20       500      gini 0.1298449\n92     9       20       300      gini 0.1299142\n93     7       15       100      gini 0.1302080\n94     6       15       100      gini 0.1302180\n95     8       20       300      gini 0.1302658\n96    11       20       100      gini 0.1303368\n97    12       20       100      gini 0.1303956\n98     7       20       500      gini 0.1305237\n99     4       15       500      gini 0.1306045\n100    7       20       300      gini 0.1307034\n101    4       15       300      gini 0.1314075\n102    3       10       100      gini 0.1314194\n103   10       20       100      gini 0.1314446\n104    9       20       100      gini 0.1314797\n105    6       20       500      gini 0.1316348\n106    6       20       300      gini 0.1318650\n107    5       15       100      gini 0.1322313\n108    2       10       500      gini 0.1323110\n109    8       20       100      gini 0.1323435\n110    7       20       100      gini 0.1327727\n111    2       10       300      gini 0.1328086\n112    5       20       500      gini 0.1329164\n113    4       15       100      gini 0.1329842\n114    3       15       500      gini 0.1330115\n115    5       20       300      gini 0.1330483\n116    3       15       300      gini 0.1333896\n117    6       20       100      gini 0.1334507\n118    4       20       500      gini 0.1343606\n119    5       20       100      gini 0.1346723\n120    4       20       300      gini 0.1348001\n121    3       15       100      gini 0.1352052\n122    2       10       100      gini 0.1355645\n123    3       20       500      gini 0.1366326\n124    4       20       100      gini 0.1369212\n125    3       20       300      gini 0.1369492\n126    2       15       500      gini 0.1370411\n127    2       15       300      gini 0.1371923\n128    3       20       100      gini 0.1389632\n129    2       15       100      gini 0.1396311\n130    2       20       500      gini 0.1408191\n131    2       20       300      gini 0.1411691\n132    2       20       100      gini 0.1429509\n133    1        5       300      gini 0.1517564\n134    1        5       500      gini 0.1517581\n135    1        5       100      gini 0.1533601\n136    1       10       500      gini 0.1552974\n137    1       10       300      gini 0.1556152\n138    1       10       100      gini 0.1567145\n139    1       15       300      gini 0.1579995\n140    1       15       500      gini 0.1585141\n141    1       20       500      gini 0.1600574\n142    1       20       300      gini 0.1609307\n143    1       15       100      gini 0.1613020\n144    1       20       100      gini 0.1623087\n\n\n\n\n6.4.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  mse_oob_rf_classif &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        as.factor(d) ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      err_oob &lt;- rf$err.rate[num_trees,1]\n      \n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        err_oob = err_oob\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nLet us order the computed out-of-bag error rate by ascending values:\n\nbest_params_rf_classif &lt;- \n  mse_oob_rf_classif |&gt; list_rbind() |&gt; \n  arrange(err_oob)\n\nWe save the results for later use.\n\nwrite.csv(\n  best_params_rf_classif,\n  file = \"output/best_params_rf_classif.csv\", \n  row.names = FALSE\n  )\n\n\nbest_params_rf_classif\n\n    mtry nodesize num_trees splitrule   err_oob\n1     12        5       300      gini 0.1567209\n2     11        5       500      gini 0.1580126\n3     10        5       500      gini 0.1580556\n4      9        5       300      gini 0.1585723\n5     10        5       300      gini 0.1586584\n6     11        5       300      gini 0.1591751\n7      9        5       500      gini 0.1595195\n8     12        5       500      gini 0.1596056\n9      7        5       500      gini 0.1596917\n10     8        5       500      gini 0.1600792\n11     6        5       300      gini 0.1614570\n12     6        5       500      gini 0.1621459\n13     8        5       300      gini 0.1627056\n14    12        5       100      gini 0.1631361\n15     7        5       300      gini 0.1634375\n16     5        5       500      gini 0.1643417\n17    10        5       100      gini 0.1652458\n18     9        5       100      gini 0.1653320\n19    11       10       300      gini 0.1661069\n20     5        5       300      gini 0.1664944\n21     6        5       100      gini 0.1665375\n22     4        5       500      gini 0.1666236\n23    12       10       500      gini 0.1672264\n24    11        5       100      gini 0.1673125\n25     4        5       300      gini 0.1673555\n26    10       10       500      gini 0.1677000\n27     7        5       100      gini 0.1677430\n28    11       10       500      gini 0.1677861\n29    12       10       300      gini 0.1682167\n30     8        5       100      gini 0.1685180\n31     9       10       300      gini 0.1692930\n32     8       10       500      gini 0.1692930\n33     9       10       500      gini 0.1693791\n34     3        5       500      gini 0.1698958\n35     5        5       100      gini 0.1706708\n36     8       10       300      gini 0.1712736\n37     3        5       300      gini 0.1715319\n38    10       10       300      gini 0.1717902\n39     7       10       500      gini 0.1721777\n40     7       10       300      gini 0.1730388\n41    11       10       100      gini 0.1731680\n42     6       10       300      gini 0.1731680\n43     6       10       500      gini 0.1732972\n44    12       10       100      gini 0.1736416\n45     4        5       100      gini 0.1742444\n46    10       10       100      gini 0.1749333\n47    11       15       500      gini 0.1754930\n48    11       15       300      gini 0.1758805\n49    12       15       300      gini 0.1760958\n50     9       15       500      gini 0.1762249\n51    10       15       500      gini 0.1762680\n52     9       10       100      gini 0.1763110\n53    12       15       500      gini 0.1763541\n54     8       10       100      gini 0.1766124\n55     5       10       500      gini 0.1766555\n56     5       10       300      gini 0.1770860\n57    10       15       300      gini 0.1772152\n58     2        5       500      gini 0.1777749\n59     3        5       100      gini 0.1778180\n60     4       10       500      gini 0.1778180\n61     7       10       100      gini 0.1779041\n62     6       10       100      gini 0.1781624\n63     9       15       300      gini 0.1785499\n64     2        5       300      gini 0.1794110\n65     8       15       300      gini 0.1799707\n66     8       15       500      gini 0.1800138\n67     7       15       500      gini 0.1801429\n68     4       10       300      gini 0.1804443\n69     7       15       300      gini 0.1809610\n70    12       15       100      gini 0.1820804\n71    12       20       500      gini 0.1821235\n72     5       10       100      gini 0.1822957\n73    11       15       100      gini 0.1823818\n74     4       10       100      gini 0.1826401\n75    10       15       100      gini 0.1827693\n76     6       15       300      gini 0.1829846\n77     3       10       500      gini 0.1831568\n78     2        5       100      gini 0.1833290\n79    12       20       300      gini 0.1835443\n80    11       20       300      gini 0.1835874\n81     6       15       500      gini 0.1838887\n82    11       20       500      gini 0.1838887\n83     5       15       500      gini 0.1840610\n84     8       15       100      gini 0.1841471\n85     9       20       300      gini 0.1843193\n86     7       15       100      gini 0.1844054\n87     9       20       500      gini 0.1844915\n88     3       10       300      gini 0.1850512\n89    10       20       300      gini 0.1851373\n90     9       15       100      gini 0.1852665\n91    10       20       500      gini 0.1855679\n92     4       15       500      gini 0.1862998\n93    12       20       100      gini 0.1864721\n94     8       20       500      gini 0.1865582\n95    11       20       100      gini 0.1867304\n96     8       20       300      gini 0.1873332\n97     5       15       300      gini 0.1876345\n98     6       15       100      gini 0.1877637\n99     7       20       500      gini 0.1881512\n100    7       20       300      gini 0.1883665\n101    9       20       100      gini 0.1884095\n102    3       10       100      gini 0.1885818\n103    2       10       500      gini 0.1897443\n104    4       15       300      gini 0.1900887\n105   10       20       100      gini 0.1901317\n106    8       20       100      gini 0.1905192\n107    6       20       500      gini 0.1905192\n108    5       15       100      gini 0.1906484\n109    4       15       100      gini 0.1915095\n110    7       20       100      gini 0.1919401\n111    6       20       300      gini 0.1921553\n112    3       15       500      gini 0.1928012\n113    2       10       300      gini 0.1929303\n114    3       15       300      gini 0.1930164\n115    5       20       500      gini 0.1933178\n116    5       20       300      gini 0.1940067\n117    6       20       100      gini 0.1955136\n118    4       20       500      gini 0.1959442\n119    3       15       100      gini 0.1970206\n120    4       20       300      gini 0.1973650\n121    2       10       100      gini 0.1974942\n122    5       20       100      gini 0.1983983\n123    4       20       100      gini 0.1990011\n124    2       15       300      gini 0.2003789\n125    3       20       500      gini 0.2010678\n126    2       15       500      gini 0.2013692\n127    3       20       300      gini 0.2017567\n128    3       20       100      gini 0.2037802\n129    2       15       100      gini 0.2047275\n130    2       20       300      gini 0.2083441\n131    2       20       500      gini 0.2083441\n132    2       20       100      gini 0.2106260\n133    1        5       500      gini 0.2295703\n134    1        5       100      gini 0.2300870\n135    1        5       300      gini 0.2349092\n136    1       10       300      gini 0.2349522\n137    1       10       500      gini 0.2386980\n138    1       10       100      gini 0.2398174\n139    1       15       300      gini 0.2410230\n140    1       15       500      gini 0.2412813\n141    1       15       100      gini 0.2456730\n142    1       20       500      gini 0.2462757\n143    1       20       100      gini 0.2473952\n144    1       20       300      gini 0.2490743\n\n\n\n\n\n\nSubasi A., Cankur S. 2019. “Prediction of Default Payment of Credit Card Clients Using Data Mining Techniques.” Fifth International Engineering Conference on Developments in Civil & Computer Engineering Applications 2019 - (IEC2019) - Erbil - IRAQ. https://doi.org/10.1109/IEC47844.2019.8950597.\n\n\nYeh, I-Cheng. 2016. “Default of credit card clients.” UCI Machine Learning Repository."
  },
  {
    "objectID": "rf-simulations.html#helper-functions",
    "href": "rf-simulations.html#helper-functions",
    "title": "7  Simulations",
    "section": "7.1 Helper Functions",
    "text": "7.1 Helper Functions\nBefore running the simulations, we need some helper functions.\n\n7.1.1 Calibration/Test Splits\nWe define get_samples() which is used to create a partition of the data to create a calibration and a test set.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\nget_samples &lt;- function(seed,\n                        data) {\n  set.seed(seed)\n  n_obs &lt;- nrow(data)\n  \n  # Calibration/test sets----\n  calib_index &lt;- sample(1:nrow(data), size = .5 * nrow(data), replace = FALSE)\n  tb_calib &lt;- data |&gt; slice(calib_index)\n  tb_test &lt;- data |&gt; slice(-calib_index)\n  \n  list(\n    data = data,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs\n  )\n}\n\n\n\n7.1.2 Standard Metrics For Classification / Regression\nWe define the compute_gof() function wich computes multiple goodness-of-fit criteria.\n\n\nDisplay the codes used to define the standard metrics\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities. If `NULL` (default), True MSE is not \n#'   computed and the `NA` value is returned for this metric\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\n#' @returns tibble with MSE, accuracy, missclassification rate, sensititity \n#'  (TPR), specificity (TNR), FPR, and used threshold\ncompute_gof &lt;- function(true_prob = NULL,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  if (!is.null(true_prob)) {\n    mse &lt;- mean((true_prob - pred)^2)\n  } else {\n    mse = NA\n  }\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  # AUC\n  AUC &lt;- as.numeric(pROC::auc(obs, pred, levels = c(\"0\", \"1\"), direction = \"&gt;\"))\n  \n  # roc &lt;- pROC::roc(obs, pred, levels = c(\"0\", \"1\"), direction = \"&gt;\")\n  # AUC &lt;- as.numeric(pROC::auc(roc))\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR,\n    AUC = AUC\n  )\n}\n\n\nWe define functions to estimate calibration, as in Section 1.2 from Chapter 1 (brier_score(), get_summary_bins(), e_calib_error(), qmse_error(), local_ci_scores(), weighted_mse(), local_calib_score(), and a wrapper function, compute_metrics()).\n\n\nDisplay the codes used to define calibration metrics functions.\n## Brier Score----\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n## Expected Calibration Error (ECE)----\n\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n## Quantile-based Mean Squared Error----\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n## Weighted Mean Squared Error----\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\n#' @importFrom binom binom.confint\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n}\n\n## Local regression score\n#' Calibration score using Local Regression\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\nlocal_calib_score &lt;- function(obs, \n                              scores) {\n  \n  # Add a little noise to the scores, to avoir crashing R\n  scores &lt;- scores + rnorm(length(scores), 0, .001)\n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(\n      d = obs,\n      scores = scores\n    )\n  )\n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n  locfit_0_linspace[locfit_0_linspace &gt; 1] &lt;- 1\n  locfit_0_linspace[locfit_0_linspace &lt; 0] &lt;- 0\n  \n  # Squared difference between predicted value and the bissector, weighted by the density of values\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(linspace_raw)\n  )\n  # The weights\n  weights &lt;- dens$y[keep_linspace]\n  \n  weighted.mean((linspace - locfit_0_linspace)^2, weights)\n}\n\n## Wrapper----\n\n#' Computes the calibration metrics for a set of observed and predicted \n#' probabilities\n#'  - mse: Mean Squared Error based on true proba\n#'  - brier: Brier score\n#'  - ece: Expectec Calibration Error\n#'  - qmse: MSE on bins defined by the quantiles of the predicted scores\n#'  - wmse: MSE weighted by the density of the predicted scores\n#' \n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#' @param linspace vector of values at which to compute the WMSE\n#' @param k number of classes (bins) to create (default to `10`)\ncompute_metrics &lt;- function(obs, \n                            scores, \n                            true_probas = NULL,\n                            linspace = NULL,\n                            k = 10) {\n  \n  ## True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n  \n  \n  brier &lt;- brier_score(obs = obs, scores = scores)\n  if (length(unique(scores)) &gt; 1) {\n    ece &lt;- e_calib_error(obs = obs, scores = scores, k = k, threshold = .5)\n    qmse &lt;- qmse_error(obs = obs, scores = scores, k = k, threshold = .5)\n  } else {\n    ece &lt;- NA\n    qmse &lt;- NA\n  }\n  \n  locfit_score &lt;- local_calib_score(obs = obs, scores = scores)\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  expected_events &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs, \n      scores = scores,\n      tau = .x, nn = .15, prob = .95, method = \"probit\")\n  ) |&gt; \n    bind_rows()\n  wmse &lt;- weighted_mse(local_scores = expected_events, scores = scores)\n  tibble(\n    mse = mse, \n    locfit_score = locfit_score,\n    brier = brier, \n    ece = ece, \n    qmse = qmse, \n    wmse = wmse\n  )\n}\n\n\n\n\n7.1.3 Functions to Train Random Forests\nWe define two functions to train a random forest on the train set, apply_rf() (for a regressor) and apply_rf_vote() (for a classifier). Both functions return the estimated scores on the train set, the calibration set, and the test set.\n\n#' Apply Random Forest algorithm\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf &lt;- function(train_data, \n                     calib_data, \n                     test_data,\n                     mtry = max(floor(ncol(train_data)/3), 1),\n                     nodesize = 1,\n                     ntree = 500,\n                     splitrule = \"gini\") {\n  \n  rf &lt;- randomForest(\n    d ~ ., \n    data = train_data, \n    mtry = mtry, \n    nodesize = nodesize, \n    ntree = ntree,\n    splitrule =  splitrule\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"response\")\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"response\")\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"response\")\n  \n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n#' Apply Random Forest algorithm (classification task)\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf_vote &lt;- function(train_data, \n                          calib_data, \n                          test_data,\n                          mtry = 1,\n                          nodesize = 0.1 * nrow(train_data),\n                          ntree = 500,\n                          splitrule = \"gini\") {\n  rf &lt;- randomForest(\n    d ~ ., \n    data = train_data |&gt; mutate(d = factor(d)), \n    mtry = mtry, \n    nodesize = nodesize, \n    ntree = ntree,\n    splitrule =  splitrule,\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"vote\")[, \"1\"]\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"vote\")[, \"1\"]\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"vote\")[, \"1\"]\n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n\n7.1.4 Calibration Curve\nWe define the calib_curve_locfit_simul_rf() function to compute the calibration curve for a simulation.\n\n\nCode\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_locfit_simul_rf &lt;- function(simul,\n                                        linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  locfit_0_train &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_train$d, score = scores_train)\n  )\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_c_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n  score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  \n  # Make sure to have values in [0,1]\n  score_c_locfit_0_train[score_c_locfit_0_train &gt; 1] &lt;- 1\n  score_c_locfit_0_train[score_c_locfit_0_train &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n  score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n  score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n  \n  res_train &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_test,\n    sample = \"test\"\n  )\n  \n  res_train |&gt; \n    bind_rows(\n      res_calib\n    ) |&gt; \n    bind_rows(\n      res_test\n    ) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed\n    )\n}\n\n\n\n\n7.1.5 Recalibration\nWe define a core function, recalibrate() which recalibrates the scores using a recalibrator (we use, as in Chapter 2, the following recalibrator: Platt scaling, isotonic regression, beta calibration, and locfit).\n\n#' Recalibrates scores using a calibrator\n#' \n#' @param obs_train vector of observed events in the train set\n#' @param scores_train vector of predicted probabilities in the train set\n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression, `\"beta\"` for beta calibration, \n#'   `\"locfit\"` for local regression)\n#' @param params list of named parameters to use in the local regression \n#'   (`nn` for fraction of nearest neighbors to use, `deg` for degree)\n#' @param linspace vector of alues at which to compute the recalibrated scores\n#' @returns list of three elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set, and recalibrated scores on a segment \n#'   of values\nrecalibrate &lt;- function(obs_train,\n                        pred_train, \n                        obs_calib,\n                        pred_calib,\n                        obs_test,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\", \"beta\", \"locfit\"),\n                        params = NULL,\n                        linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  data_train &lt;- tibble(d = obs_train, scores = pred_train)\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  # Recalibrator trained on calibration data\n  if (method == \"platt\") {\n    # Recalibrator\n    lr &lt;- glm(\n      d ~ scores, family = binomial(link = 'logit'), data = data_calib\n    )\n    # Recalibrated scores on calib/test sets\n    score_c_train &lt;- predict(lr, newdata = data_train, type = \"response\")\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n    # Recalibrated scores on [0,1]\n    score_c_linspace &lt;- predict(\n      lr, newdata = tibble(scores = linspace), type = \"response\"\n    )\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_train &lt;- fit_iso(data_train$scores)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    score_c_linspace &lt;- fit_iso(linspace)\n  } else if (method == \"beta\") {\n    capture.output({\n      bc &lt;- try(beta_calibration(\n        p = data_calib$scores, \n        y = data_calib$d, \n        parameters = \"abm\" # 3 parameters a, b & m\n      ))\n    })\n    if (!inherits(bc, \"try-error\")) {\n      score_c_train &lt;- beta_predict(p = data_train$scores, bc)\n      score_c_calib &lt;- beta_predict(p = data_calib$scores, bc)\n      score_c_test &lt;- beta_predict(p = data_test$scores, bc)\n      score_c_linspace &lt;- beta_predict(p = linspace, bc)\n    } else {\n      score_c_train &lt;- score_c_calib &lt;- score_c_test &lt;- score_c_linspace &lt;- NA\n    }\n    \n  } else if (method == \"locfit\") {\n    noise_scores &lt;- data_calib$scores + rnorm(nrow(data_calib), 0, 0.01)\n    noise_data_calib &lt;- data_calib %&gt;% mutate(scores = noise_scores)\n    locfit_reg &lt;- locfit(\n      formula = d ~ lp(scores, nn = params$nn, deg = params$deg), \n      kern = \"rect\", maxk = 200, data = noise_data_calib\n    )\n    score_c_train &lt;- predict(locfit_reg, newdata = data_train)\n    score_c_train[score_c_train &lt; 0] &lt;- 0\n    score_c_train[score_c_train &gt; 1] &lt;- 1\n    score_c_calib &lt;- predict(locfit_reg, newdata = data_calib)\n    score_c_calib[score_c_calib &lt; 0] &lt;- 0\n    score_c_calib[score_c_calib &gt; 1] &lt;- 1\n    score_c_test &lt;- predict(locfit_reg, newdata = data_test)\n    score_c_test[score_c_test &lt; 0] &lt;- 0\n    score_c_test[score_c_test &gt; 1] &lt;- 1\n    score_c_linspace &lt;- predict(locfit_reg, newdata = linspace)\n    score_c_linspace[score_c_linspace &lt; 0] &lt;- 0\n    score_c_linspace[score_c_linspace &gt; 1] &lt;- 1\n  } else {\n    stop(str_c(\n      'Wrong method. Use one of the following:',\n      '\"platt\", \"isotonic\", \"beta\", \"locfit\"'\n    ))\n  }\n  \n  # Format results in tibbles:\n  # For train set\n  tb_score_c_train &lt;- tibble(\n    d = obs_train,\n    p_u = pred_train,\n    p_c = score_c_train\n  )\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  # For linear space\n  tb_score_c_linspace &lt;- tibble(\n    linspace = linspace,\n    p_c = score_c_linspace\n  )\n  \n  list(\n    tb_score_c_train = tb_score_c_train,\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test,\n    tb_score_c_linspace = tb_score_c_linspace\n  )\n  \n}\n\n\n\n7.1.6 Wrapper Function\nLastly, we define a wrapper function to perform a single replication of the simulations, for a type of problem (regression or classification).\n\nsimul_calib_rf &lt;- function(seed,\n                           type = c(\"regression\", \"classification\"),\n                           tuning = TRUE){\n  \n  set.seed(seed)\n  linspace &lt;- seq(0, 1, length.out = 101)\n  \n  # 1. Get and Split the dataset----\n  \n  data &lt;- get_samples(seed = seed, data = tb_rest)\n  tb_train &lt;- tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # 2. Train RF----\n  \n  if (type == \"regression\") {\n    # RF regressor parameters tuned\n    if (tuning == TRUE) {\n      # Use hyperparameters selected in `rf-grid-search.R`\n      nodesize &lt;- best_params_rf_reg$nodesize\n      mtry &lt;- best_params_rf_reg$mtry\n      ntree &lt;- best_params_rf_reg$num_trees\n      splitrule &lt;- best_params_rf_reg$splitrule\n    } else {\n      # Use default values from `randomForest()`\n      nodesize &lt;- 5\n      # nodesize &lt;- 0.1 * nrow(tb_train)\n      mtry &lt;- max(floor(ncol(tb_train)/3), 1)\n      ntree &lt;- 500\n      splitrule &lt;- \"gini\"\n    }\n    \n    # Regression\n    scores_reg &lt;- apply_rf(\n      train_data = tb_train,\n      calib_data = tb_calib, \n      test_data = tb_test,\n      mtry = mtry,\n      nodesize = nodesize,\n      ntree = ntree, \n      splitrule = splitrule\n    )\n    scores &lt;- scores_reg\n  } else {\n    if (tuning == TRUE) {\n      # Use hyperparameters selected in `rf-grid-search.R`\n      nodesize &lt;- best_params_rf_classif$nodesize\n      mtry &lt;- best_params_rf_classif$mtry\n      ntree &lt;- best_params_rf_classif$num_trees\n      splitrule &lt;- best_params_rf_classif$splitrule\n    } else {\n      # Use default values from `randomForest()`\n      nodesize &lt;- 0.1 * nrow(tb_train)\n      mtry &lt;- max(floor(ncol(tb_train)/3), 1)\n      ntree &lt;- 500\n      splitrule &lt;- \"gini\"\n    }\n    # Classification\n    scores_classif &lt;- apply_rf_vote(\n      train_data = tb_train, \n      calib_data = tb_calib, \n      test_data = tb_test,\n      mtry = mtry,\n      nodesize = nodesize,\n      ntree = ntree, \n      splitrule = splitrule\n    )\n    scores &lt;- scores_classif\n  }\n  \n  # 3. Calibration----\n  \n  ## 3.1. Metrics----\n  \n  # For regression\n  calibration_train &lt;- compute_metrics(\n    obs = tb_train$d, \n    scores = scores$scores_train,\n    k = 10\n  ) |&gt; \n    mutate(sample = \"train\")\n  \n  calibration_calib &lt;- compute_metrics(\n    obs = tb_calib$d, \n    scores = scores$scores_calib,\n    k = 5\n  ) |&gt; \n    mutate(sample = \"calibration\")\n  \n  calibration_test &lt;- compute_metrics(\n    obs = tb_test$d, \n    scores = scores$scores_test,\n    k = 5\n  ) |&gt; \n    mutate(sample = \"test\")\n  \n  calibration &lt;- \n    calibration_train |&gt; \n    bind_rows(calibration_calib) |&gt; \n    bind_rows(calibration_test) |&gt;\n    mutate(model = type)\n  \n  # Comparison with standard metrics\n  gof_train &lt;- compute_gof(\n    obs = tb_train$d, \n    pred = scores$scores_train\n  ) |&gt; mutate(sample = \"train\")\n  \n  gof_calib &lt;- compute_gof( \n    obs = tb_calib$d, \n    pred = scores$scores_calib\n  ) |&gt; mutate(sample = \"calibration\")\n  \n  gof_test &lt;- compute_gof(\n    obs = tb_test$d, \n    pred = scores$scores_test\n  ) |&gt; mutate(sample = \"test\")\n  \n  gof &lt;- \n    gof_train |&gt; \n    bind_rows(gof_calib) |&gt; \n    bind_rows(gof_test) |&gt;\n    mutate(model = type)\n  \n  summary_metrics &lt;- \n    gof |&gt; \n    left_join(calibration, by = c(\"mse\", \"sample\", \"model\")) |&gt; \n    relocate(sample, model, .before = \"mse\") |&gt;\n    mutate(seed = seed)\n  \n  # 5. Calibration curves with locfit ----\n  \n  scores_train &lt;- scores$scores_train\n  scores_calib &lt;- scores$scores_calib\n  scores_test &lt;- scores$scores_test\n  # Add a little noise to the scores, to avoir crashing R\n  scores_train &lt;- scores_train + rnorm(length(scores_train), 0, .001)\n  scores_calib &lt;- scores_calib + rnorm(length(scores_calib), 0, .001)\n  scores_test &lt;- scores_test + rnorm(length(scores_test), 0, .001)\n  \n  locfit_0_train &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_train$d, score = scores_train)\n  )\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n  score_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  # Make sure to have values in [0,1]\n  score_locfit_0_train[score_locfit_0_train &gt; 1] &lt;- 1\n  score_locfit_0_train[score_locfit_0_train &lt; 0] &lt;- 0\n  \n  score_locfit_0_calib[score_locfit_0_calib &gt; 1] &lt;- 1\n  score_locfit_0_calib[score_locfit_0_calib &lt; 0] &lt;- 0\n  \n  score_locfit_0_test[score_locfit_0_test &gt; 1] &lt;- 1\n  score_locfit_0_test[score_locfit_0_test &lt; 0] &lt;- 0\n  \n  res_train &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_test,\n    sample = \"test\"\n  )\n  \n  calibration_curves &lt;- \n    res_train |&gt; \n    bind_rows(\n      res_calib\n    ) |&gt; \n    bind_rows(\n      res_test\n    ) |&gt; \n    mutate(\n      seed = seed,\n      type = type,\n      method = \"No Calibration\"\n    )\n  \n  # 6. Recalibration----\n  \n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n  \n  scores_c &lt;- vector(mode = \"list\", length = length(methods))\n  names(scores_c) &lt;- method_names\n  res_c &lt;- tibble()\n  \n  for (i_method in 1:length(methods)) {\n    method &lt;- methods[i_method]\n    params_current &lt;- params[[i_method]]\n    \n    ## 6.1 Recalibrated Scores----\n    scores_c[[i_method]] &lt;- recalibrate(\n      obs_train = tb_train$d, \n      pred_train = scores$scores_train, \n      obs_calib = tb_calib$d, \n      pred_calib = scores$scores_calib, \n      obs_test = tb_test$d, \n      pred_test = scores$scores_test, \n      method = method,\n      params = params_current\n    )\n    ## 6.2 Recalibration Curves----\n    scores_c_train &lt;- scores_c[[i_method]]$tb_score_c_train$p_c\n    scores_c_calib &lt;- scores_c[[i_method]]$tb_score_c_calib$p_c\n    scores_c_test &lt;- scores_c[[i_method]]$tb_score_c_test$p_c\n    # Add a little noise to the scores, to avoir crashing R\n    scores_c_train &lt;- scores_c_train + rnorm(length(scores_c_train), 0, .001)\n    scores_c_calib &lt;- scores_c_calib + rnorm(length(scores_c_calib), 0, .001)\n    scores_c_test &lt;- scores_c_test + rnorm(length(scores_c_test), 0, .001)\n    \n    locfit_0_train &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_train$d, score = scores_c_train)\n    )\n    locfit_0_calib &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_calib$d, score = scores_c_calib)\n    )\n    locfit_0_test &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_test$d, score = scores_c_test)\n    )\n    \n    score_c_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n    score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n    score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n    \n    # Make sure to have values in [0,1]\n    score_c_locfit_0_train[score_c_locfit_0_train &gt; 1] &lt;- 1\n    score_c_locfit_0_train[score_c_locfit_0_train &lt; 0] &lt;- 0\n    \n    score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n    score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n    \n    score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n    score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n    \n    res_c_train &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_train,\n      sample = \"train\",\n      method = method_names[i_method]\n    )\n    res_c_calib &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_calib,\n      sample = \"calibration\",\n      method = method_names[i_method]\n    )\n    res_c_test &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_test,\n      sample = \"test\",\n      method = method_names[i_method]\n    )\n    \n    res_c &lt;- res_c  |&gt;\n      bind_rows(\n        res_c_train) |&gt;\n      bind_rows(\n        res_c_calib\n      ) |&gt; \n      bind_rows(\n        res_c_test\n      )\n  }\n  \n  res_c &lt;- res_c |&gt;\n    mutate(\n      seed = seed,\n      type = type\n    )\n  \n  recalibration_curves &lt;- calibration_curves |&gt; bind_rows(res_c)\n  \n  # 7. Recalibration Metrics----\n  \n  ## 7.1. Standard Metrics----\n  \n  gof_c_train &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_train$d, \n      pred = .x$tb_score_c_train$p_c\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"train\")\n  \n  gof_c_calib &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_calib$d, \n      pred = .x$tb_score_c_calib$p_c\n    ) \n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"calibration\")\n  \n  gof_c_test &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_test$d, \n      pred = .x$tb_score_c_test$p_c\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"test\")\n  \n  gof_c &lt;- \n    gof_c_train |&gt; \n    bind_rows(gof_c_calib) |&gt; \n    bind_rows(gof_c_test) |&gt;\n    mutate(model = type)\n  \n  ## 7.2. Calibration Metrics----\n  \n  calibration_c_train &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_train$d, \n      scores = .x$tb_score_c_train$p_c,\n      k = 10\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"train\")\n  \n  calibration_c_calib &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_calib$d, \n      scores = .x$tb_score_c_calib$p_c,\n      k = 5\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"calibration\")\n  \n  calibration_c_test &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_test$d, \n      scores = .x$tb_score_c_test$p_c,\n      k = 5\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"test\")\n  \n  \n  calibration_c &lt;- \n    calibration_c_train |&gt; \n    bind_rows(calibration_c_calib) |&gt; \n    bind_rows(calibration_c_test) |&gt;\n    mutate(model = type)\n  \n  summary_metrics_c &lt;- \n    gof_c |&gt; \n    left_join(calibration_c, by = c(\"mse\", \"sample\", \"model\", \"method\")) |&gt; \n    relocate(sample, model, method, .before = \"mse\") |&gt;\n    mutate(seed = seed)\n  \n  summary_metrics &lt;- summary_metrics |&gt; mutate(method = \"No Calibration\")\n  \n    list(\n      recalibration_curves = recalibration_curves,\n      summary_metrics = list_rbind(list(summary_metrics, summary_metrics_c)),\n      seed = seed,\n      scores = scores,\n      scores_c = scores_c,\n      type = type,\n      tuning = tuning\n    )\n}"
  },
  {
    "objectID": "rf-simulations.html#import-data",
    "href": "rf-simulations.html#import-data",
    "title": "7  Simulations",
    "section": "7.2 Import Data",
    "text": "7.2 Import Data\nLet us load the data obtained in Chapter 6.\n\ntb_train &lt;- read.csv(\n  \"data/data_credit_smote_train.csv\")\ntb_rest &lt;- read.csv(\n  \"data/data_credit_smote_rest.csv\")\n\nThe set of hyperparameters and the associated oob MSE/oob error rate:\n\nbest_params_rf_reg &lt;- read_csv(\n  \"output/best_params_rf_reg.csv\")\nbest_params_rf_classif &lt;- read_csv(\n  \"output/best_params_rf_classif.csv\")\n\nWe select the hyperparameters with:\n\nthe lowest MSE (for regression)\nthe highest accuracy (for classification).\n\nThe set of hyperparameters for the regression task:\n\nbest_params_rf_reg &lt;- \n  best_params_rf_reg |&gt; \n  arrange(mse_oob) |&gt; \n  slice(1)\nbest_params_rf_reg\n\n# A tibble: 1 × 5\n   mtry nodesize num_trees splitrule mse_oob\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1    12        5       500 gini        0.116\n\n\nThe set of hyperparameters for the classification task:\n\nbest_params_rf_classif &lt;- \n  best_params_rf_classif |&gt; \n  arrange(err_oob) |&gt; \n  slice(1)\nbest_params_rf_classif\n\n# A tibble: 1 × 5\n   mtry nodesize num_trees splitrule err_oob\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1    12        5       300 gini        0.157"
  },
  {
    "objectID": "rf-simulations.html#running-the-simulations",
    "href": "rf-simulations.html#running-the-simulations",
    "title": "7  Simulations",
    "section": "7.3 Running the Simulations",
    "text": "7.3 Running the Simulations\nWe consider 200 different splits of the data on which a recalibration technique will be applied.\n\nn_repl &lt;- 200\nseed &lt;- 1:n_repl\n\n\n7.3.1 Regression\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  metrics_rf_tuned_reg &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      resul &lt;- simul_calib_rf(\n        seed = .x,\n        type = \"regression\",\n        tuning = TRUE\n      )\n      p()\n      resul\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use.\n\nsave(\n  metrics_rf_tuned_reg, \n  file = \"/output/simul-rf/metrics_rf_tuned_reg.rda\")\n\n\n\n7.3.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  metrics_rf_tuned_class &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      resul &lt;- simul_calib_rf(\n        seed = .x,\n        type = \"classification\",\n        tuning = TRUE\n      )\n      p()\n      resul\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use.\n\nsave(\n  metrics_rf_tuned_class, \n  file = \"/output/simul-rf/metrics_rf_tuned_class.rda\"\n  )"
  },
  {
    "objectID": "rf-simulations.html#sec-calib-vs-gof-simul",
    "href": "rf-simulations.html#sec-calib-vs-gof-simul",
    "title": "7  Simulations",
    "section": "7.4 Calibration vs. Goodness-of-fit",
    "text": "7.4 Calibration vs. Goodness-of-fit\nWe now explore the relationship between calibration and goodness-of-fit.\nLet us consider different set of hyperparameters for the random forests. We will train a forest on each set and compute calibration and goodness-of-fit metrics on both the training set and on the remaining set (we no longer split the remaining data into a calibration set and a test set).\n\ngrid_params &lt;- \n  expand_grid(\n    num_trees = c(100, 300, 500),\n    mtry = seq(1, (ncol(tb_train)/2)),\n    nodesize = c(5, 10, 15, 20),\n    splitrule = \"gini\"\n  )\n\nLet us run the simulations.\n\n7.4.1 Regression\n\n# Grid search : regression\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  compare_cal_gof_reg &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        d ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      out_of_bag &lt;- map(.x = 1:nrow(tb_train), .f = ~which(rf[[\"inbag\"]][.x,] == 0))\n      rf_pred_all &lt;- predict(rf, tb_train,\n                             predict.all = TRUE,\n                             type = \"response\")$individual\n      rf_pred &lt;- unlist(map(.x = 1:nrow(tb_train), .f = ~mean(rf_pred_all[.x,out_of_bag[[.x]]])))\n      \n      oob_err &lt;- mse_function(pred = rf_pred, obs = tb_train |&gt; pull(d))\n      mse_oob &lt;- oob_err\n      \n      # Predict RF on train/rest dataset\n      scores_train &lt;- predict(rf, newdata = tb_train, type = \"response\")\n      scores_rest &lt;- predict(rf, newdata = tb_rest, type = \"response\")\n      \n      # Calibration metrics (Brier Score and LCS) on train/rest dataset\n      calib_metrics_train &lt;- compute_metrics(obs = tb_train$d, scores = scores_train)\n      #calib_metrics_train &lt;- calib_metrics_train |&gt; \n      #  select(locfit_score, brier)\n      calib_metrics_rest &lt;- compute_metrics(obs = tb_rest$d, scores = scores_rest)\n      #calib_metrics_rest &lt;- calib_metrics_rest |&gt; \n      #  select(locfit_score, brier)\n      \n      # GOF metrics on train/rest dataset\n      gof_metrics_train &lt;- compute_gof(obs = tb_train$d, pred = scores_train)\n      #gof_metrics_train &lt;- gof_metrics_train |&gt; \n      #  select(accuracy, AUC)\n      gof_metrics_rest &lt;- compute_gof(obs = tb_rest$d, pred = scores_rest)\n      #gof_metrics_rest &lt;- gof_metrics_rest |&gt; \n      #  select(accuracy, AUC)\n      \n      # Update progressbar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        mse_oob = mse_oob,\n        brier_train = calib_metrics_train$brier,\n        LCS_train = calib_metrics_train$locfit_score,\n        ece_train = calib_metrics_train$ece,\n        qmse_train = calib_metrics_train$qmse,\n        wmse_train = calib_metrics_train$wmse,\n        sensitivity_train = gof_metrics_train$sensitivity,\n        specificity_train = gof_metrics_train$specificity,\n        AUC_train = gof_metrics_train$AUC,\n        accuracy_train = gof_metrics_train$accuracy,\n        brier_rest = calib_metrics_rest$brier,\n        LCS_rest = calib_metrics_rest$locfit_score,\n        ece_rest = calib_metrics_rest$ece,\n        qmse_rest = calib_metrics_rest$qmse,\n        wmse_rest = calib_metrics_rest$wmse,\n        sensitivity_rest = gof_metrics_rest$sensitivity,\n        specificity_rest = gof_metrics_rest$specificity,\n        AUC_rest = gof_metrics_rest$AUC,\n        accuracy_rest = gof_metrics_rest$accuracy\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\ncompare_cal_gof_reg &lt;- list_rbind(compare_cal_gof_reg)\n\nWe save the results for later use.\n\nwrite.csv(\n  compare_cal_gof_reg, \n  file = \"/output/compare_cal_gof_reg.csv\", \n  row.names = FALSE\n)\n\n\n\n7.4.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  compare_cal_gof_class &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        as.factor(d) ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      err_oob &lt;- rf$err.rate[num_trees,1]\n      \n      # Predict RF on train/rest dataset\n      scores_train &lt;- predict(rf, newdata = tb_train, type = \"vote\")[, \"1\"]\n      scores_rest &lt;- predict(rf, newdata = tb_rest, type = \"vote\")[, \"1\"]\n      \n      # Calibration metrics (Brier Score and LCS) on train/rest dataset\n      calib_metrics_train &lt;- compute_metrics(obs = tb_train$d, scores = scores_train)\n      #calib_metrics_train &lt;- calib_metrics_train |&gt; \n      #  select(locfit_score, brier)\n      calib_metrics_rest &lt;- compute_metrics(obs = tb_rest$d, scores = scores_rest)\n      #calib_metrics_rest &lt;- calib_metrics_rest |&gt; \n      #  select(locfit_score, brier)\n      \n      # GOF metrics on train/rest dataset\n      gof_metrics_train &lt;- compute_gof(obs = tb_train$d, pred = scores_train)\n      #gof_metrics_train &lt;- gof_metrics_train |&gt; \n      #  select(accuracy, AUC)\n      gof_metrics_rest &lt;- compute_gof(obs = tb_rest$d, pred = scores_rest)\n      #gof_metrics_rest &lt;- gof_metrics_rest |&gt; \n      #  select(accuracy, AUC)\n      \n      # Update progressbar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        err_oob = err_oob,\n        brier_train = calib_metrics_train$brier,\n        LCS_train = calib_metrics_train$locfit_score,\n        ece_train = calib_metrics_train$ece,\n        qmse_train = calib_metrics_train$qmse,\n        wmse_train = calib_metrics_train$wmse,\n        sensitivity_train = gof_metrics_train$sensitivity,\n        specificity_train = gof_metrics_train$specificity,\n        AUC_train = gof_metrics_train$AUC,\n        accuracy_train = gof_metrics_train$accuracy,\n        brier_rest = calib_metrics_rest$brier,\n        LCS_rest = calib_metrics_rest$locfit_score,\n        ece_rest = calib_metrics_rest$ece,\n        qmse_rest = calib_metrics_rest$qmse,\n        wmse_rest = calib_metrics_rest$wmse,\n        sensitivity_rest = gof_metrics_rest$sensitivity,\n        specificity_rest = gof_metrics_rest$specificity,\n        AUC_rest = gof_metrics_rest$AUC,\n        accuracy_rest = gof_metrics_rest$accuracy\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ncompare_cal_gof_class &lt;- list_rbind(compare_cal_gof_class)\n\nWe save the results for later use.\n\nwrite.csv(\n  compare_cal_gof_class, \n  file = \"/output/compare_cal_gof_class.csv\", \n  row.names = FALSE\n)"
  },
  {
    "objectID": "rf-simul-metrics.html#load-results",
    "href": "rf-simul-metrics.html#load-results",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.1 Load Results",
    "text": "8.1 Load Results\nLet us load the results from Chapter 7.\nFor the regression forests:\n\nload(\"output/simul-rf/metrics_rf_tuned_reg.rda\")\n\nFor the classification forests:\n\nload(\"output/simul-rf/metrics_rf_tuned_class.rda\")"
  },
  {
    "objectID": "rf-simul-metrics.html#metrics",
    "href": "rf-simul-metrics.html#metrics",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.2 Metrics",
    "text": "8.2 Metrics\nWe extract the summary metrics from these.\n\nsummary_metrics_tuned_reg &lt;- \n  map(metrics_rf_tuned_reg, \"summary_metrics\") |&gt; \n  list_rbind()\nsummary_metrics_tuned_class &lt;- map(metrics_rf_tuned_class, \"summary_metrics\") |&gt; \n  list_rbind()\n\nLet us bind the results in a single tibble.\n\nmetrics_rf &lt;- summary_metrics_tuned_reg |&gt; \n  mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    summary_metrics_tuned_class |&gt; \n      mutate(model = \"Classification\")\n  ) |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    AUC = 1-AUC,\n    model = factor(\n      model, \n      levels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = c(\"No Calibration\", \"platt\", \"isotonic\", \"beta\", \n                 \"locfit_0\", \"locfit_1\", \"locfit_2\") |&gt; rev(),\n      labels = c(\"No Calibration\", \"Platt Scaling\", \"Isotonic\", \"Beta\", \n                 \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\") |&gt; rev()\n    ),\n    sample = factor(\n      sample,\n      levels = c(\"calibration\", \"test\") |&gt; rev(),\n      labels = c(\"Calibration\", \"Test\") |&gt; rev())\n  )\nmetrics_rf\n\n# A tibble: 5,600 × 17\n   sample  model mse   accuracy missclass_rate sensitivity specificity threshold\n   &lt;fct&gt;   &lt;fct&gt; &lt;lgl&gt;    &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 Calibr… Regr… NA       0.852          0.148       0.758       0.923       0.5\n 2 Test    Regr… NA       0.852          0.148       0.755       0.923       0.5\n 3 Calibr… Regr… NA       0.860          0.140       0.804       0.902       0.5\n 4 Calibr… Regr… NA       0.861          0.139       0.812       0.897       0.5\n 5 Calibr… Regr… NA       0.860          0.140       0.825       0.887       0.5\n 6 Calibr… Regr… NA       0.860          0.140       0.813       0.895       0.5\n 7 Calibr… Regr… NA       0.860          0.140       0.822       0.890       0.5\n 8 Calibr… Regr… NA       0.860          0.140       0.818       0.892       0.5\n 9 Test    Regr… NA       0.857          0.143       0.794       0.903       0.5\n10 Test    Regr… NA       0.858          0.142       0.803       0.899       0.5\n# ℹ 5,590 more rows\n# ℹ 9 more variables: FPR &lt;dbl&gt;, AUC &lt;dbl&gt;, locfit_score &lt;dbl&gt;, brier &lt;dbl&gt;,\n#   ece &lt;dbl&gt;, qmse &lt;dbl&gt;, wmse &lt;dbl&gt;, seed &lt;int&gt;, method &lt;fct&gt;"
  },
  {
    "objectID": "rf-simul-metrics.html#boxplots",
    "href": "rf-simul-metrics.html#boxplots",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.3 Boxplots",
    "text": "8.3 Boxplots\nLet us define two functions to display the goodness-of-fit metrics (boxplot_metrics_gof()) and the calibration metrics (boxplot_metrics_calib()) in boxplots.\n\nboxplot_metrics_gof &lt;- function(data_metrics){\n  \n  gof_metrics &lt;- c(\"accuracy\", \"sensitivity\", \"specificity\")\n  gof_metrics_lab &lt;- c(\"Accuracy\", \"Sensitivity\", \"Specificity\")\n  \n  data_metrics_plot &lt;- \n    data_metrics |&gt; \n    select(-mse) |&gt; \n    select(sample, model, method, seed, !!!syms(gof_metrics)) |&gt; \n    pivot_longer(\n      cols = !!gof_metrics, \n      names_to = \"metric\", values_to = \"value\"\n    )\n  \n  mat &lt;- matrix(\n    c(1:(6), rep(7,3)), ncol = 3, byrow = TRUE\n  )\n  layout(mat, heights = c(rep(3, 4),1))\n  \n  range_val &lt;- data_metrics_plot |&gt; \n    group_by(metric) |&gt; \n    summarise(\n      min_val = min(value),\n      max_val = max(value)\n    )\n  \n  # Goodness of Fit----\n  for (i_model in 1:length(levels(data_metrics_plot$model))) {\n    model &lt;- levels(data_metrics_plot$model)[i_model]\n    data_metrics_gof &lt;- data_metrics_plot |&gt; \n      filter(model == !!model)\n    \n    for (i_metric in 1:length(gof_metrics)) {\n      \n      metric &lt;- gof_metrics[i_metric]\n      metric_lab &lt;- gof_metrics_lab[i_metric]\n      val_min &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(min_val)\n      val_max &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(max_val)\n      title &lt;- \"\"\n      if (i_metric == 2) {\n        title &lt;- str_c(model, \"\\n\", metric_lab)\n      } else {\n        title &lt;- str_c(\"\\n\", metric_lab)\n      }\n      \n      nb_methods &lt;- data_metrics_gof$method |&gt; levels() |&gt; length()\n      \n      data_metrics_gof_current &lt;- \n        data_metrics_gof |&gt; \n        filter(metric == !!metric)\n      \n      par(mar = c(2.5, 1, 4.1, 1))\n      boxplot(\n        value ~ sample + method,\n        data = data_metrics_gof_current,\n        col = colours,\n        horizontal = TRUE,\n        las = 1, xlab = \"\", ylab = \"\",\n        main = title,\n        border = c(\"black\", adjustcolor(\"black\", alpha.f = .5)),\n        yaxt = \"n\",\n        ylim = c(val_min, val_max)\n      )\n      # Horizontal lines\n      for (i in seq(3, nb_methods * 2, by = 2) - .5) {\n        abline(h = i, lty = 1, col = \"gray\")\n      }\n    }\n    \n  }\n  \n  # Legend----\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = rev(levels(data_metrics_plot$method)),\n    fill = rev(colours_calib),\n    # lwd = 2,\n    xpd = TRUE, ncol = 4\n  )\n}\n\n\nboxplot_metrics_calib &lt;- function(data_metrics){\n  \n  calib_metrics &lt;- c(\"brier\", \"ece\", \"locfit_score\")\n  calib_metrics_lab &lt;- c(\"Brier Score\", \"ECE\", \"LCS\")\n  \n  data_metrics_plot &lt;- \n    data_metrics |&gt; \n    select(-mse) |&gt; \n    select(sample, model, method, seed, !!!syms(calib_metrics)) |&gt; \n    pivot_longer(\n      cols = !!calib_metrics, \n      names_to = \"metric\", values_to = \"value\"\n    )\n  \n  range_val &lt;- data_metrics_plot |&gt; \n    group_by(metric) |&gt; \n    summarise(\n      min_val = min(value),\n      max_val = max(value)\n    )\n  \n  mat &lt;- matrix(\n    c(1:(6), rep(7,3)), ncol = 3, byrow = TRUE\n  )\n  layout(mat, heights = c(rep(3, 4),1))\n  \n  # Calibration Metrics----\n  for (i_model in 1:length(levels(data_metrics_plot$model))) {\n    model &lt;- levels(data_metrics_plot$model)[i_model]\n    data_metrics_calib &lt;- data_metrics_plot |&gt; \n      filter(model == !!model)\n    \n    for (i_metric in 1:length(calib_metrics)) {\n      metric &lt;- calib_metrics[i_metric]\n      metric_lab &lt;- calib_metrics_lab[i_metric]\n      val_min &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(min_val)\n      val_max &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(max_val)\n      if (i_metric == 2) {\n        title &lt;- str_c(model, \"\\n\", metric_lab)\n      } else {\n        title &lt;- str_c(\"\\n\", metric_lab)\n      }\n      \n      nb_methods &lt;- data_metrics_calib$method |&gt; levels() |&gt; length()\n      \n      data_metrics_calib_current &lt;- \n        data_metrics_calib |&gt; \n        filter(metric == !!metric)\n      \n      par(mar = c(2.5, 1, 4.1, 1))\n      boxplot(\n        value ~ sample + method,\n        data = data_metrics_calib_current,\n        col = colours,\n        horizontal = TRUE,\n        las = 1, xlab = \"\", ylab = \"\",\n        main = title,\n        border = c(\"black\", adjustcolor(\"black\", alpha.f = .5)),\n        yaxt = \"n\",\n        ylim = c(val_min, val_max)\n      )\n      # Horizontal lines\n      for (i in seq(3, nb_methods * 2, by = 2) - .5) {\n        abline(h = i, lty = 1, col = \"gray\")\n      }\n    }\n    \n  }\n  \n  # Legend----\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = rev(levels(data_metrics_plot$method)),\n    fill = rev(colours_calib),\n    # lwd = 2,\n    xpd = TRUE, ncol = 4\n  )\n}\n\n\nboxplot_metrics_gof(metrics_rf)\n\n\n\nFigure 8.1: Goodness-of-fit Metrics Computed on 200 Replications, for the Regression Random Forest (top) and for the Classification Random Forest (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). A probability threshold of \\(\\tau=0.5\\) was used to compute the sensivity and the specificity.\n\n\n\n\n\n\nboxplot_metrics_calib(metrics_rf)\n\n\n\nFigure 8.2: Calibration Metrics Computed on 200 Replications, for the Regression Random Forest (top) and for the Classification Random Forest (bottom), on the Calibration (transparent colors) and on the Test Set (full colors)."
  },
  {
    "objectID": "rf-simul-metrics.html#calibration-vs.-performance",
    "href": "rf-simul-metrics.html#calibration-vs.-performance",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.4 Calibration vs. Performance",
    "text": "8.4 Calibration vs. Performance\nLet us load the results obtaine in Section 7.4.\n\ncompare_class &lt;- read.csv(\"output/compare_cal_gof_class.csv\")\ncompare_reg &lt;- read.csv(\"output/compare_cal_gof_reg.csv\")\n\n\ncompare_class &lt;- compare_class |&gt; arrange(err_oob) |&gt; \n  mutate(AUC_train = 1-AUC_train,\n         AUC_rest = 1-AUC_rest)\ncompare_reg &lt;- compare_reg |&gt; arrange(mse_oob)|&gt; \n  mutate(AUC_train = 1-AUC_train,\n         AUC_rest = 1-AUC_rest)\n\nSome cosmetics:\n\nsamples &lt;- c(\"train\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Test\")\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Test\" = \"#009E73\"\n)\n\nWe create a function to plot the calibration metrics vs goodness of fit metrics. Each point represent the result obtained in a simulation, computed either in the train set or in the test set.\n\nplot_compare &lt;- function(gof_metric, calib_metric) {\n  \n  gof_metrics &lt;- c(\"sensitivity\", \"specificity\", \"AUC\", \"accuracy\")\n  calib_metrics &lt;- c(\"brier\", \"LCS\", \"ece\", \"qmse\", \"wmse\")\n  types &lt;- c(\"regression\", \"classification\")\n  types_labs &lt;- c(\"Regression\", \"Classification\")\n  \n  mat &lt;- c(1,2,3,3) |&gt; matrix(ncol=2, byrow = TRUE)\n  layout(mat, heights = c(3,.5))\n  \n  for (i_type in 1:length(types)) {\n    type &lt;- types[i_type]\n    type_lab &lt;- types_labs[i_type]\n    \n    if (type == \"regression\"){\n      data &lt;- compare_reg\n    } else if(type == \"classification\") {\n      data &lt;- compare_class\n    } else {\n      stop(\"wrong type\")\n    }\n    \n    train_gof &lt;- paste(gof_metric, \"train\", sep = \"_\")\n    rest_gof &lt;- paste(gof_metric, \"rest\", sep = \"_\")\n    train_calib &lt;- paste(calib_metric, \"train\", sep = \"_\")\n    rest_calib &lt;- paste(calib_metric, \"rest\", sep = \"_\")\n    \n    # Plot boundaries\n    xmin &lt;- min(data[[train_gof]], data[[rest_gof]])\n    xmax &lt;- max(data[[train_gof]], data[[rest_gof]])\n    ymin &lt;- min(data[[train_calib]], data[[rest_calib]])\n    ymax &lt;- max(data[[train_calib]], data[[rest_calib]])\n    \n    \n    par(mar = c(4.1, 4.1, 2.1, 1))\n    \n    # From train to test\n    plot(\n      xmin, xmax,\n      xlim = c(xmin, xmax), ylim = c(ymin, ymax),\n      xlab = gof_metric, ylab = calib_metric,\n      main = type_lab,\n    )\n    segments(\n      x0 = data[[train_gof]], \n      y0 = data[[train_calib]], \n      x1 = data[[rest_gof]], \n      y1 = data[[rest_calib]],\n      col = adjustcolor(\"gray\", alpha.f = .3)\n    )\n    \n    # Train set\n    points(\n      x = data[[train_gof]], \n      y = data[[train_calib]],\n      pch = 19, cex = .8,\n      col = adjustcolor(colours_samples[[\"Train\"]], alpha.f = .3)\n    )\n    # Loess\n    lw_train &lt;- loess(data[[train_calib]] ~ data[[train_gof]])\n    ord &lt;- order(data[[train_gof]])\n    lines(data[[train_gof]][ord],lw_train$fitted[ord], col = colours_samples[[\"Train\"]],lwd = 3, lty = 3)\n    # Test set\n    points(\n      x = data[[rest_gof]], \n      y = data[[rest_calib]],\n      col = adjustcolor(colours_samples[[\"Test\"]], alpha.f = .3),\n      pch = 19, cex = .8\n    )\n    # Loess\n    lw_rest &lt;- loess(data[[rest_calib]] ~ data[[rest_gof]])\n    ord &lt;- order(data[[rest_gof]])\n    lines(data[[rest_gof]][ord],lw_rest$fitted[ord], col = colours_samples[[\"Test\"]],lwd = 3, lty = 3)\n    \n  }\n  # Legend\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = c(\"Train\", \"Test\"),\n    col = colours_samples,\n    pch = 19,\n    xpd = TRUE, ncol = 2\n  )\n  \n}\n\n\nLCS vs. AUCBrier vs. AUCECE vs. AUC\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"LCS\")\n\n\n\nFigure 8.3: LCS vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model.\n\n\n\n\n\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"brier\")\n\n\n\nFigure 8.4: Brier Score vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model.\n\n\n\n\n\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"ece\")\n\n\n\nFigure 8.5: Expected Calibration Error vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model."
  },
  {
    "objectID": "rf-simul-metrics.html#calibration-curves",
    "href": "rf-simul-metrics.html#calibration-curves",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.5 Calibration Curves",
    "text": "8.5 Calibration Curves\nLet us extract the calibration curves computed in Chapter 7.\n\ncurves_reg &lt;- \n  map(metrics_rf_tuned_reg, \"recalibration_curves\") |&gt; \n  list_rbind()\ncurves_class &lt;- \n  map(metrics_rf_tuned_class, \"recalibration_curves\") |&gt; \n  list_rbind()\n\nWe merge those in a single tibble:\n\ncalib_curves &lt;- \n  curves_reg |&gt; \n  bind_rows(curves_class) |&gt; \n  group_by(xlim, sample, type, method) |&gt; \n  summarise(\n    mean = mean(locfit_pred),\n    lower = quantile(locfit_pred, probs = .025),\n    upper = quantile(locfit_pred, probs = 0.975),\n    .groups = \"drop\"\n  )\n\nSome cosmetics:\n\nmethods &lt;- c(\n  \"No Calibration\",\n  \"platt\", \"isotonic\", \"beta\", \n  \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\nmethods_labs &lt;- c(\n  \"No Calibration\", \"Platt\", \"Isotonic\", \"Beta\", \n  \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\"\n)\n\nAnd we apply them:\n\ncalib_curves &lt;- \n  calib_curves |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"calibration\", \"test\"), \n      labels = c(\"Calibration\", \"Test\")\n    ),\n    type = factor(\n      type,\n      levels = c(\"regression\", \"classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = methods,\n      labels = methods_labs\n    )\n  )\n\nWe need to get the scores, to add boxplots on top of the curves.\n\n## Uncalibrated scores----\n\n### Train set----\nscores_no_calib_reg_train &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\nscores_no_calib_class_train &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\n\n### Calibration set----\nscores_no_calib_reg_calib &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_calib\")\nscores_no_calib_class_calib &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_calib\")\n\n### Test set----\nscores_no_calib_reg_test &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_test\")\nscores_no_calib_class_test &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_test\")\n\n## Recalibrated Scores----\nscores_no_calib_reg_train &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\n\nNow, we will partition the [0,1] segment into equal-sized bins. In each bin, we will compute the number of observations with scores falling into that bin, for each sample of each simulation (and each recalibration technique).\n\n8.5.0.1 Helper Functions\nFirst, the count_scores() function, which counts the number of observations in each bin, for a vector of scores.\n\n#' For a vector of scores, compute the number of obs in each bin\n#' The bins are defined on evenly separated values in [0,1]\n#' \ncount_scores &lt;- function(scores) {\n  breaks &lt;- seq(0, 1, by = .05)\n  \n  if (!is.null(scores)) {\n    n_bins &lt;- table(cut(scores, breaks = breaks))\n  } else {\n    n_bins &lt;- NA_integer_\n  }\n  tibble(\n    bins = names(table(cut(breaks, breaks = breaks))),\n    n_bins = as.vector(n_bins)\n  )\n}\n\nThen, another function, count_scores_simul(), applies count_scores() to all the non-recalibrated scores in the simulations.\n\n#' Applies the count_scores() function to a list of simulations\n#' \ncount_scores_simul &lt;- function(scores_simul) {\n  map(scores_simul, count_scores) |&gt; \n    list_rbind() |&gt; \n    group_by(bins) |&gt; \n    summarise(\n      n_bins = mean(n_bins)\n    )\n}\n\nLastly, the count_scores_simul_method() function does the same for the all the recalibrated scores.\n\n#' Extract the recalibrated scores from a list of simulations and computes\n#' the number of scores in bins (see count_scores())\n#' For each simulation, there are multiple correction techniques\n#' \ncount_scores_simul_method &lt;- function(scores_simul_methods) {\n  map(scores_simul_methods, \"scores_c\") |&gt; \n    map(\n      .f = function(simul){\n        map(\n          simul, \n          .f = function(sim_method) {\n            count_scores_simul(sim_method$tb_score_c_train) |&gt; \n              mutate(sample = \"train\") |&gt; \n              bind_rows(\n                count_scores_simul(sim_method$tb_score_c_calib) |&gt; \n                  mutate(sample = \"calibration\")\n              ) |&gt; \n              bind_rows(\n                count_scores_simul(sim_method$tb_score_c_test) |&gt; \n                  mutate(sample = \"test\")\n              )\n          }\n        ) |&gt; \n          list_rbind(names_to = \"method\")\n      },\n      .progress = TRUE\n    ) |&gt; \n    list_rbind() |&gt; \n    group_by(method, sample, bins) |&gt; \n    summarise(\n      n_bins = mean(n_bins),\n      .groups = \"drop\"\n    )\n}\n\nLet us get the those values!\n\n## Regression----\n### No Calibration----\nn_bins_no_calib_reg &lt;- \n  count_scores_simul(scores_no_calib_reg_train) |&gt; \n  mutate(sample = \"train\", method = \"No Calibration\", type = \"Regression\") |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_reg_calib) |&gt; \n      mutate(sample = \"calibration\", method = \"No Calibration\", type = \"Regression\")\n  ) |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_reg_test) |&gt; \n      mutate(sample = \"test\", method = \"No Calibration\", type = \"Regression\")\n  )\n### With Recalibration methods----\nn_bins_recalib_reg &lt;- count_scores_simul_method(\n  scores_simul_methods = metrics_rf_tuned_reg) |&gt; \n  mutate(type = \"Regression\")\n\n## Regression----\n### No Calibration----\nn_bins_no_calib_class &lt;- \n  count_scores_simul(scores_no_calib_class_train) |&gt; \n  mutate(sample = \"train\", method = \"No Calibration\", type = \"Classification\") |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_class_calib) |&gt; \n      mutate(sample = \"calibration\", method = \"No Calibration\", type = \"Classification\")\n  ) |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_class_test) |&gt; \n      mutate(sample = \"test\", method = \"No Calibration\", type = \"Classification\")\n  )\n### With Recalibration methods----\nn_bins_recalib_class &lt;- count_scores_simul_method(\n  scores_simul_methods = metrics_rf_tuned_class) |&gt; \n  mutate(type = \"Classification\")\n\n## Merge all simulations----\nn_bins &lt;- \n  n_bins_no_calib_reg |&gt; \n  bind_rows(n_bins_recalib_reg) |&gt; \n  bind_rows(n_bins_no_calib_class) |&gt; \n  bind_rows(n_bins_recalib_class)\n\nn_bins &lt;- \n  n_bins |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"calibration\", \"test\"), \n      labels = c(\"Calibration\", \"Test\")\n    ),\n    type = factor(\n      type,\n      levels = c(\"Regression\", \"Classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = !!methods,\n      labels = !!methods_labs\n    )\n  )\n\n\nload(\"output/n_bins.rda\")\n\n\n\n8.5.1 Visualization\nSome cosmetics:\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\ncolours_calib &lt;- c(\n  # \"#332288\", \n  \"#117733\",\n  \"#44AA99\", \"#88CCEE\",\n  \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\")\n\nThe plot_calib_locfit_simuls() allows us to visualize the calibration curves (on the train set and on the test set) for a type of forests (regression or classification), for uncalibrated scores and recalibrated ones.\n\nplot_calib_locfit_simuls &lt;- function(calib_curve, type) {\n  tb_calib_curve &lt;- calib_curves |&gt;filter(type == !!type)\n  tb_n_bins &lt;- n_bins |&gt; filter(type == !!type)\n  \n  nb_methods &lt;- tb_calib_curve$method |&gt; unique() |&gt; length()\n  \n  mat_init &lt;- c(1, 1, 2, 4, 3, 5) |&gt; matrix(ncol = 2, byrow = TRUE)\n  mat &lt;- mat_init\n  for (j in 2:(ceiling(nb_methods/2))) {\n    mat &lt;- rbind(mat, mat_init + (5*(j-1)))\n  }\n  mat &lt;- cbind(mat, mat + max(mat))\n  \n  layout(mat, height = rep(c(.35, 1, 3), nb_methods))\n  \n  y_lim &lt;- c(0, 1)\n  \n  samples &lt;- c(\"Calibration\", \"Test\")\n  \n  for (i_method in 1:length(methods)) {\n    method &lt;- methods_labs[i_method]\n    # Title for the models\n    par(mar = c(0, 0.5, 0, 0.5))\n    plot(\n      0:1, 0:1,\n      col = NULL,\n      type=\"n\",\n      xlim = c(0,2), ylim = 0:1,\n      xlab = \"\",\n      ylab = \"\",\n      main = \"\",\n      xaxt = \"n\",\n      yaxt = \"n\",\n      new = TRUE,\n      frame.plot = FALSE\n    )\n    # Get the center coordinates of the plot\n    plot_center &lt;- c(mean(par(\"usr\")[1:2]), mean(par(\"usr\")[3:4]))\n    # Get the width and height of the text\n    text_dim &lt;- strwidth(method, cex = 1.2, font = 2)\n    # Calculate the coordinates to center the text\n    text_x &lt;- plot_center[1] - text_dim / 2\n    text_y &lt;- plot_center[2]\n    \n    # Add text to the plot at the centered coordinates\n    text(\n      1, text_y,\n      method, col = colours_calib[i_method],\n      cex = 1, font = 2\n    )\n    for(i_sample in 1:length(samples)) {\n      sample &lt;- samples[i_sample]\n      tb_plot &lt;- tb_calib_curve |&gt; \n        filter(\n          sample == !!sample,\n          method == !!method,\n          type == !!type\n        )\n      n_bins_current &lt;- tb_n_bins |&gt; \n        filter(\n          sample == !!sample,\n          method == !!method,\n          type == !!type\n        )\n      n_bins_no_calib_current &lt;- tb_n_bins |&gt; \n        filter(\n          sample == !!sample,\n          method == \"No Calibration\",\n          type == !!type\n        )\n      # Barplots on top\n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(\n        c(n_bins_no_calib_current$n_bins,\n          n_bins_current$n_bins, na.rm = TRUE)\n      )\n      barplot(\n        n_bins_no_calib_current$n_bins,\n        col = adjustcolor(\"gray\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\"\n      )\n      barplot(\n        n_bins_current$n_bins,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      \n      # Calib curve\n      par(mar = c(4.1, 4.3, 0.5, 0.5), mgp = c(2, 1, 0))\n      plot(\n        tb_plot$xlim, tb_plot$mean,\n        type = \"l\", col = colours_samples[sample],\n        xlim = 0:1, ylim = 0:1,\n        xlab = latex2exp::TeX(\"$p^u$\"), \n        ylab = latex2exp::TeX(\"$E(D | p^u = p^c)$\"),\n        main = \"\"\n      )\n      polygon(\n        c(tb_plot$xlim, rev(tb_plot$xlim)),\n        c(tb_plot$lower, rev(tb_plot$upper)),\n        col = adjustcolor(col = colours_samples[sample], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n  \n}\n\n\nplot_calib_locfit_simuls(\n  calib_curve = calib_curves, type = \"Regression\"\n)\n\n\n\nFigure 8.6: Calibration Curves Obtained with Local Regression, for the Regression Random Forest, for the  Calibration Set and for the Test Set. The curves are the averages values obtained on 200 different splits of the calibration and test datasets, and the color bands are the 95% bootstrap confidence intervals. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\nplot_calib_locfit_simuls(\n  calib_curve = calib_curves, type = \"Classification\"\n)\n\n\n\nFigure 8.7: Calibration Curves Obtained with Local Regression, for the Classification Random Forest, for the  Calibration Set and for the Test Set. The curves are the averages values obtained on 200 different splits of the calibration and test datasets, and the color bands are the 95% bootstrap confidence intervals. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kull, Meelis, Telmo M. Silva Filho, and Peter Flach. 2017. “Beyond\nSigmoids: How to Obtain Well-Calibrated Probabilities from Binary\nClassifiers with Beta Calibration.” Electronic Journal of\nStatistics 11 (2). https://doi.org/10.1214/17-ejs1338si.\n\n\nPlatt, John et al. 1999. “Probabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods.”\nAdvances in Large Margin Classifiers 10 (3): 61–74.\n\n\nSubasi A., Cankur S. 2019. “Prediction of Default Payment of\nCredit Card Clients Using Data Mining Techniques.” Fifth\nInternational Engineering Conference on Developments in Civil &\nComputer Engineering Applications 2019 - (IEC2019) - Erbil - IRAQ.\nhttps://doi.org/10.1109/IEC47844.2019.8950597.\n\n\nYeh, I-Cheng. 2016. “Default of credit card\nclients.” UCI Machine Learning Repository.\n\n\nZadrozny, Bianca, and Charles Elkan. 2002. “Transforming\nClassifier Scores into Accurate Multiclass Probability\nEstimates.” In Proceedings of the Eighth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining.\nKDD02. ACM. https://doi.org/10.1145/775047.775151."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "",
    "text": "Introduction\nThis ebook is the online supplementary materials for the article titled “From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration.” In this ebook, we are interested in the calibration of a binary classifier for which the scores returned by the model should be interpreted as probabilities.\nThe codes are written in R. This notebook explains them. You can download the scripts on the following github repository:\nGithub Repository\nThis ebook is structured into three main parts"
  },
  {
    "objectID": "index.html#part-1-synthetic-data-and-calibration-metrics",
    "href": "index.html#part-1-synthetic-data-and-calibration-metrics",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 1: Synthetic Data and Calibration Metrics",
    "text": "Part 1: Synthetic Data and Calibration Metrics\nIn the first part, we provide an overview of the synthetic data utilized in our study and introduce various calibration metrics and visualization techniques. This foundational chapter lays the groundwork for understanding the subsequent analysis (Chapter 1). We then move to the presentation of the recalibration techniques and we apply them to our synthetic data (Chapter 2)"
  },
  {
    "objectID": "index.html#part-2-calibration-of-random-forests",
    "href": "index.html#part-2-calibration-of-random-forests",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 2: Calibration of Random Forests",
    "text": "Part 2: Calibration of Random Forests\nMoving forward, we look into the calibration of random forests. Initially, we estimate single random forest regressors and classifiers, detailing the calibration process step by step. We explore the effects of recalibration on the forest scores, both before and after adjustment (Chapter 3). Following this, we employ bootstrap simulations to further explore the calibration of random forests using synthetic data. We analyze the calibration metrics pre- (Chapter 4) and post-recalibration (Chapter 5)."
  },
  {
    "objectID": "index.html#part-3-real-world-data-analysis",
    "href": "index.html#part-3-real-world-data-analysis",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 3: Real-World Data Analysis",
    "text": "Part 3: Real-World Data Analysis\nIn the final part, we transition to real-world data on default prediction. We kick off with a comprehensive grid search to identify the optimal set of hyperparameters for both random forest regressors and classifiers (Chapter 6). Subsequently, we conduct bootstrap simulations to evaluate the calibration of random forests on real-world data, and examine the relationship between performance and calibration metrics. The simulations are run in Chapter 7 and the results are presented in Chapter 8."
  }
]