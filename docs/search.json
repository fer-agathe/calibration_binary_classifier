[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "",
    "text": "Introduction\nThis ebook is the online supplementary materials for the article titled “From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration.” In this ebook, we are interested in the calibration of a binary classifier for which the scores returned by the model should be interpreted as probabilities.\nThe codes are written in R. This notebook explains them. You can download the scripts on the following github repository:\nGithub Repository\nThis ebook is structured into three main parts"
  },
  {
    "objectID": "index.html#part-1-synthetic-data-and-calibration-metrics",
    "href": "index.html#part-1-synthetic-data-and-calibration-metrics",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 1: Synthetic Data and Calibration Metrics",
    "text": "Part 1: Synthetic Data and Calibration Metrics\nIn the first part, we provide an overview of the synthetic data utilized in our study and introduce various calibration metrics and visualization techniques. This foundational chapter lays the groundwork for understanding the subsequent analysis (Chapter 1). We then move to the presentation of the recalibration techniques and we apply them to our synthetic data (Chapter 2)"
  },
  {
    "objectID": "index.html#part-2-calibration-of-random-forests",
    "href": "index.html#part-2-calibration-of-random-forests",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 2: Calibration of Random Forests",
    "text": "Part 2: Calibration of Random Forests\nMoving forward, we look into the calibration of random forests. Initially, we estimate single random forest regressors and classifiers, detailing the calibration process step by step. We explore the effects of recalibration on the forest scores, both before and after adjustment (Chapter 3). Following this, we employ bootstrap simulations to further explore the calibration of random forests using synthetic data. We analyze the calibration metrics pre- (Chapter 4) and post-recalibration (Chapter 5)."
  },
  {
    "objectID": "index.html#part-3-real-world-data-analysis",
    "href": "index.html#part-3-real-world-data-analysis",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "section": "Part 3: Real-World Data Analysis",
    "text": "Part 3: Real-World Data Analysis\nIn the final part, we transition to real-world data on default prediction. We kick off with a comprehensive grid search to identify the optimal set of hyperparameters for both random forest regressors and classifiers (Chapter 6). Subsequently, we conduct bootstrap simulations to evaluate the calibration of random forests on real-world data, and examine the relationship between performance and calibration metrics. The simulations are run in Chapter 7 and the results are presented in Chapter 8."
  },
  {
    "objectID": "calibration.html#sec-dgp",
    "href": "calibration.html#sec-dgp",
    "title": "1  Calibration",
    "section": "1.1 Data Generating Process",
    "text": "1.1 Data Generating Process\nLet us consider a binary variable that is assumed to follow a Bernoulli distribution: \\(D_i\\sim B(p_i)\\), where \\(p_i\\) is the probability of observing \\(D_i = 1\\). We define this probability \\(p_i\\) according to the following function \\[p_i = \\frac{1}{1+\\exp(-\\eta_i)}, \\tag{1.1}\\]\nwhere \\(\\eta_i\\) is defined as follows:\n\\[\\eta_i = -0.1x_1 + 0.05x_2 + 0.2x_3 - 0.05x_4 + \\varepsilon_i, \\tag{1.2}\\] where \\(\\varepsilon_i \\sim \\mathcal{N}(0, 0.5^2)\\).\nLet us simulate a toy dataset, following this DGP:\n\nn &lt;- 2000\nx1 &lt;- runif(n)\nx2 &lt;- runif(n)\nx3 &lt;- runif(n)\nx4 &lt;- runif(n)\nepsilon_p &lt;- rnorm(n, mean = 0, sd = .5)\n\nThe latent score:\n\neta &lt;- -0.1*x1 + 0.05*x2 + 0.2*x3 - 0.05*x4  + epsilon_p\n\nThe true probabilities:\n\np &lt;- (1 / (1 + exp(-eta)))\n\nAnd the outcome binary variable:\n\nd &lt;- rbinom(n, size = 1, prob = p)\n\nThe distribution of the true probabilities are depicted in Figure 1.1\n\npar(mar = c(4.1, 4.3, 2.1, 0.5))\nhist(p, xlab = \"p\", main = \"\", xlim = c(0,1))\n\n\n\nFigure 1.1: Distribution of the true probabilities\n\n\n\n\n\n\n1.1.1 Poor Calibration\nAs previously mentioned, we would like to measure the calibration of a binary classifier. To do so, we will deliberately transform the probabilities. We consider two types of transformations:\n\none applied to the latent probability\nanother applied to the linear predictor.\n\nIn a more formal way, we will introduce two scaling parameters. The scaling parameter that modifies the latent probability changes Equation 1.1 which becomes: \\[p_i^{u} = \\left(\\frac{1}{1+\\exp(-\\eta_i^u)}\\right)^{\\alpha} \\tag{1.3}\\] The scaling parameter that modifies the linear predictor changes Equation 1.4 which becomes: \\[\\eta_i^u = \\gamma \\times \\left( (-0.1)x_1 + 0.05x_2 + 0.2x_3 - 0.05x_4 + \\varepsilon_i\\right), \\tag{1.4}\\]\nWe define a function to simulate data according to the GDP. This function allows to transform the probabilities (by default, it does not).\n\n#' Simulates data\n#'\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\n#' @param alpha scale parameter for the latent probability (if different \n#'   from 1, the probabilities are transformed and it may induce decalibration)\n#' @param gamma scale parameter for the latent score (if different from 1, \n#'   the probabilities are transformed and it may induce decalibration)\nsim_data &lt;- function(n_obs = 2000, \n                     seed, \n                     alpha = 1, \n                     gamma = 1,\n                     a = c(-0.1, 0.05, 0.2, -0.05)) {\n  set.seed(seed)\n\n  x1 &lt;- runif(n_obs)\n  x2 &lt;- runif(n_obs)\n  x3 &lt;- runif(n_obs)\n  x4 &lt;- runif(n_obs)\n  \n  a1 &lt;- a[1]\n  a2 &lt;- a[2]\n  a3 &lt;- a[3]\n  a4 &lt;- a[4]\n  \n  epsilon_p &lt;- rnorm(n_obs, mean = 0, sd = .5)\n  \n  # True latent score\n  eta &lt;- a1*x1 + a2*x2 + a3*x3 + a4*x4  + epsilon_p\n  # Transformed latent score\n  eta_u &lt;- gamma * eta\n  \n  # True probability\n  p &lt;- (1 / (1 + exp(-eta)))\n  # Transformed probability\n  p_u &lt;- ((1 / (1 + exp(-eta_u))))^alpha\n\n  # Observed event\n  d &lt;- rbinom(n_obs, size = 1, prob = p)\n\n  tibble(\n    # Event Probability\n    p = p,\n    p_u = p_u,\n    # Binary outcome variable\n    d = d,\n    # Variables\n    x1 = x1,\n    x2 = x2,\n    x3 = x3,\n    x4 = x4\n  )\n}\n\nWe can then use that function to observe how it impacts the latent probability distribution.\n\n1.1.1.1 Varying \\(\\alpha\\)\nLet us consider \\(\\alpha = \\{1/3, 2/3, 1, 3/2, 3\\}\\).\n\nalphas &lt;- c(1/3, 2/3, 1, 3/2, 3)\n\nWe generate data using these values:\n\ndata_alphas &lt;- \n  map(\n  .x = alphas,\n  .f = ~sim_data(\n    n_obs = 2000, seed = 1, alpha = .x, gamma = 1\n  )\n)\n\nLet us look at the transformed probabilities as a function as the true probabilities.\n\ntrue_prob_alphas &lt;- map(data_alphas, \"p\")\ntransformed_prob_alphas &lt;- map(data_alphas, \"p_u\")\n# Ordering by increasing values of the true proba\norder_true_alphas &lt;- map(true_prob_alphas, order)\ntrue_prob_alphas &lt;- map2(\n  .x = true_prob_alphas, .y = order_true_alphas, .f = ~.x[.y])\ntransformed_prob_alphas &lt;- map2(\n  .x = transformed_prob_alphas, .y = order_true_alphas, .f = ~.x[.y])\n\ncolours &lt;- RColorBrewer::brewer.pal(\n  length(true_prob_alphas)+1, name = \"Blues\"\n)\ncolours &lt;- colours[-1]\ncolours[alphas == 1] &lt;- \"orange\"\n\npar(mar = c(4.1, 4.3, 2.1, 0.5))\nplot(\n  x = true_prob_alphas[[1]],\n  y = transformed_prob_alphas[[1]], type = \"l\",\n  xlab = latex2exp::TeX(\"$p$\"),\n  ylab = latex2exp::TeX(\"$p^u$\"),\n  col = colours[1],\n  ylim = c(0, 1)\n)\nfor (i in 2:length(true_prob_alphas)) {\n  lines(\n    x = true_prob_alphas[[i]],\n    y = transformed_prob_alphas[[i]],\n    col = colours[i]\n  )\n}\nlegend(\n  \"bottomright\", col = colours, lty = 1,\n  legend = latex2exp::TeX(str_c(\"$\\\\alpha = \",round(alphas, 2), \"$\"))\n)\n\n\n\nFigure 1.2: Transformed probabilities as a function of true probabilities\n\n\n\n\n\nLet us also have a look at a histogram.\n\np_alphas &lt;- map(data_alphas, ~sort(.x$p_u))\ncolours &lt;- RColorBrewer::brewer.pal(\n  length(p_alphas)+1, name = \"Blues\"\n)\ncolours &lt;- colours[-1]\ncolours[alphas == 1] &lt;- \"orange\"\n\npar(mar = c(4.1, 4.3, 2.1, 0.5), mfrow = c(3,2))\nhist(\n  p_alphas[[1]], breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[1], alpha.f = .4),\n  xlab = \"p\", ylab = \"Freq.\",\n  xlim = c(0, 1),\n  main = latex2exp::TeX(str_c(\"$\\\\alpha = \", round(alphas[1],2), \"$\"))\n)\nfor (i in 2:length(p_alphas)) {\n  hist(\n    p_alphas[[i]], breaks = seq(0, 1, by = .05),\n    col = adjustcolor(colours[i], alpha.f = .4),\n    xlab = \"p\", ylab = \"Freq.\",\n    main = latex2exp::TeX(str_c(\"$\\\\alpha = \", round(alphas[i], 2), \"$\"))\n  )\n}\n\n\n\nFigure 1.3: Effect of varying \\(\\alpha\\) on the latent probabilities\n\n\n\n\n\n\n\n1.1.1.2 Varying \\(\\gamma\\)\nLet us consider \\(\\gamma = \\{c(1/3, 2/3, 1, 3/2, 3)\\}\\).\n\ngammas &lt;- c(c(1/3, 2/3, 1, 3/2, 3))\n\nWe generate data using these values:\n\ndata_gammas &lt;- \n  map(\n  .x = gammas,\n  .f = ~sim_data(\n    n_obs = 2000, seed = 1, alpha = 1, gamma = .x\n  )\n)\n\nLet us look at the transformed probabilities as a function as the true probabilities.\n\ntrue_prob_gammas &lt;- map(data_gammas, \"p\")\ntransformed_prob_gammas &lt;- map(data_gammas, \"p_u\")\n# Ordering by increasing values of the true proba\norder_true_gammas &lt;- map(true_prob_gammas, order)\ntrue_prob_gammas &lt;- map2(\n  .x = true_prob_gammas, .y = order_true_gammas, .f = ~.x[.y])\ntransformed_prob_gammas &lt;- map2(\n  .x = transformed_prob_gammas, .y = order_true_gammas, .f = ~.x[.y])\n\ncolours &lt;- RColorBrewer::brewer.pal(\n  length(true_prob_gammas)+1, name = \"Blues\"\n)\ncolours &lt;- colours[-1]\ncolours[gammas == 1] &lt;- \"orange\"\n\npar(mar = c(4.1, 4.3, 2.1, 0.5))\nplot(\n  x = true_prob_gammas[[1]],\n  y = transformed_prob_gammas[[1]], type = \"l\",\n  xlab = latex2exp::TeX(\"$p$\"),\n  ylab = latex2exp::TeX(\"$p^u$\"),\n  col = colours[1],\n  ylim = c(0, 1)\n)\nfor (i in 2:length(true_prob_gammas)) {\n  lines(\n    x = true_prob_gammas[[i]],\n    y = transformed_prob_gammas[[i]],\n    col = colours[i]\n  )\n}\nlegend(\n  \"bottomright\", col = colours, lty = 1,\n  legend = latex2exp::TeX(str_c(\"$\\\\gamma = \",round(gammas, 2), \"$\"))\n)\n\n\n\nFigure 1.4: Transformed probabilities as a function of true probabilities\n\n\n\n\n\nWe look at the distribution of the values with histograms as well.\n\np_gammas &lt;- map(data_gammas, ~sort(.x$p_u))\ncolours &lt;- RColorBrewer::brewer.pal(\n  length(p_gammas)+1, name = \"Blues\"\n)\ncolours &lt;- colours[-1]\ncolours[gammas == 1] &lt;- \"orange\"\n\npar(mar = c(4.1, 4.3, 2.1, 0.5), mfrow = c(3,2))\nhist(\n  p_gammas[[1]], breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[1], alpha.f = .4),\n  xlab = \"p\", ylab = \"Freq.\",\n  xlim = c(0, 1),\n  main = latex2exp::TeX(str_c(\"$\\\\gamma = \", round(gammas[1],2), \"$\"))\n)\nfor (i in 2:length(p_gammas)) {\n  hist(\n    p_gammas[[i]], breaks = seq(0, 1, by = .05),\n    col = adjustcolor(colours[i], alpha.f = .4),\n    xlab = \"p\", ylab = \"Freq.\",\n    main = latex2exp::TeX(str_c(\"$\\\\gamma = \", round(gammas[i], 2), \"$\"))\n  )\n}\n\n\n\nFigure 1.5: Effect of varying \\(\\gamma\\) on the latent probabilities"
  },
  {
    "objectID": "calibration.html#sec-measuring-calibration",
    "href": "calibration.html#sec-measuring-calibration",
    "title": "1  Calibration",
    "section": "1.2 Measuring Calibration",
    "text": "1.2 Measuring Calibration\nA model is calibrated if:\n\\[\\mathbb{P}(D = d \\mid s(x) = p) = p,\\]\nfor all values \\(p \\in[0,1]\\) and values \\(d = \\{0,1\\}\\), where \\(s(x)\\) is the estimated score that is output by a model.\n\n\n\n\n\n\nWarning\n\n\n\nIn our case, when we vary \\(\\alpha\\) or \\(\\gamma\\), \\(s(x)\\) corresponds to \\(p^u\\), the transformed probabilities.\n\n\nTo have an idea of the calibration of a model, we can use two ways:\n\na metric-based technique\na visualization-based technique.\n\n\n1.2.1 Metrics\nLet us define functions to compute the following metrics:\n\nMSE based on true probabilities\nBrier Score\nExpected Calibration Error\nMSE on quantile-based bins (QMSE)\nWeighted MSE on the calibration curve defined on a continuum of values.\n\n\n\n\n\n\n\nNote\n\n\n\nEach metric is defined below.\n\n\n\n1.2.1.1 MSE Based on true probabilities\nAs we observe the true probabilities in our simulations, we can compare the transformed probabilities \\(p^u\\) to the true probabilities \\(p\\). The MSE writes: \\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(p_i - p_i^u\\right)^2. \\tag{1.5}\\]\n\n\n1.2.1.2 Brier Score\nThe target variable \\(D\\) is binary and takes values in \\(\\{0,1\\}\\). In that case, the Brier Score writes: \\[\\text{BS} = \\frac{1}{n}\\sum_{i=1}^{n} (s(x_i) - d_i)^{2}, \\tag{1.6}\\] where \\(s(x_i)\\) is the predicted score (we use here either the true value \\(p\\) or its “uncalibrated” version \\(p^u\\)) for observation \\(i\\) and \\(d_i\\) is the outcome variable.\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n\n\n1.2.1.3 Expected Calibration Error\nTo measure the calibration of a model, one of the metrics, Expected Calibration Error (or ECE), consists in splitting the predicted scores \\(s(x)\\) into bins and computing in each bins two metrics: the accuracy (average of the empirical probabilities in the sample, or fractions of correctly predicted classes) and the confidence (average of the predicted scores in the bin). The Expected Calibration Error is then computed as the average over the bins\n\\[\\text{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{n} \\mid \\text{acc}(b) - \\text{conf}(b) \\mid, \\tag{1.7}\\]\nwhere \\(n_b\\) is the number of observations in bin \\(b\\), \\(n\\) is the number of observations, \\(\\text{acc}(b)\\) is the accuracy of bin \\(b\\), and \\(\\text{conf}(b)\\) is the the confidence in bin \\(b\\).\nThe accuracy of a bin is defined as the average of the correctly predicted classes in the bin: \\[\\text{acc}(b) = \\frac{1}{n_b} \\sum_{i \\in b} \\mathrm{1}_{\\hat{d}_i = d_i}, \\tag{1.8}\\] where \\(\\hat{d}_i\\) is the predicted class (0 or 1) for observation \\(i\\): \\[\\hat{d}_i = \\begin{cases}\n1, & s(x) \\geq \\tau\\\\\n0, & s(x) &lt; \\tau\\\\\n\\end{cases},\\]\nwhere \\(\\tau\\) is the classification threshold, with \\(\\tau = .5\\) by default and where \\(s(x)\\) is the propensity score.\nThe confidence of a bin is defined as the average of the predicted scores in the bin: \\[\\text{conf}(b) = \\frac{1}{n_b} \\sum_{i \\in b} s(x) \\tag{1.9}\\]\nWe define a function to compute the bins. We will consider here the deciles to create the bins (and therefore set k=10 in our function).\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\nLet us consider, to illustrate how this function works, the case where the probabilities are transformed using \\(\\alpha=\\) 0.3333333. We can consider that these values would be the estimated score returned by an uncalibrated model. We would then obtain the following summary statistics for each bin:\n\nget_summary_bins(\n  obs = data_alphas[[1]]$d, \n  scores = data_alphas[[1]]$p_u,\n  k = 10, threshold = .5\n)\n\n# A tibble: 10 × 6\n   score_class    nb mean_obs mean_score sum_obs accuracy\n         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1           1   200    0.315      0.673      63    0.315\n 2           2   200    0.35       0.726      70    0.35 \n 3           3   200    0.39       0.753      78    0.39 \n 4           4   200    0.445      0.773      89    0.445\n 5           5   200    0.585      0.790     117    0.585\n 6           6   200    0.495      0.808      99    0.495\n 7           7   200    0.535      0.823     107    0.535\n 8           8   200    0.63       0.840     126    0.63 \n 9           9   200    0.68       0.862     136    0.68 \n10          10   200    0.675      0.893     135    0.675\n\n\nWe then define the e_calib_error() function, which computes the ECE using the bins defined thanks to the get_summary_bins() function.\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\nFor example, the ECE when \\(s(x)\\) is obtained using transformed probabilites with \\(\\alpha=\\) 0.3333333 equals:\n\ne_calib_error(\n  obs = data_alphas[[1]]$d, \n  scores = data_alphas[[1]]$p_u, \n  k = 10, threshold = .5\n)\n\n[1] 0.2842172\n\n\nWhile when we use the true probabilites instead of the transformed ones, the ECE equals:\n\ne_calib_error(\n  obs = data_alphas[[which(alphas == 1)]]$d,\n  scores = data_alphas[[which(alphas == 1)]]$p,\n  k = 10, threshold = .5\n)\n\n[1] 0.1101167\n\n\nNote that due to the random noise introduced in the DGP, the ECE is not equal to 0.\n\n\n1.2.1.4 QMSE\nLet us compute the MSE on based on the quantile-defined bins (as explained in Section 1.2.1.3). To do so, we first calculate the quantiles of the predicted scores: \\(p_\\tau\\). This allows us to define bins \\(b=1,\\ldots, B\\) based on the quantiles.\nFor each bin, we compute the average of the observed event: \\[\\bar{d}_b = \\frac{1}{n_b} \\sum_{i\\in b} d_i,\\] where \\(n_b\\) is the number of observation in bin \\(b\\).\nThen, we can compute the confidence in each bin, i.e, the average of the predicted score \\(s(x)\\): \\[\\text{conf}(b) = \\frac{1}{n_b} \\sum_{i\\in b} s(x),\\]\nwhere \\(s(x)\\) are the estimated probabilities.\nThe Quantile-based MSE is then defined as:\n\\[\\text{QMSE} = \\frac{1}{n}\\sum_{b=1}^{B} n_b \\left[\\bar{d}_b - \\text{conf}(b)\\right]^2.\\]\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n\n\n1.2.1.5 WMSE\nIn a similar fashion to the QMSE, we define the Weighted MSE (WMSE). We define a metric to measure calibration based on how far from the perfect calibration is the model. More specifically, for each of the values at which we computed \\(E(D \\mid s(x) = p)\\), with \\(p \\in [0,1]\\) we compute the squared difference between \\(E(D \\mid s(x) = p)\\) and \\(D\\). We then aggregate the results using a weighted mean, where the weights are the estimated density of the propensity scores \\(s(x)\\) at the corresponding values at which \\(E(D \\mid s(x) = p)\\) was estimated.\nLet us define a function, local_ci_scores(), that identifies the nearest neighbors of a certain predicted score and then calculates the mean scores in that neighborhood accompanied with its confidence interval. This functions requires the binom.confint() function from {binom}.\n\nlibrary(binom)\n\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n\nlocal_ci_scores(\n  obs = data_alphas[[which(alphas == 1)]]$d,\n  scores = data_alphas[[which(alphas == 1)]]$p_u,\n  tau = .1, nn = .15, prob = .5, method = \"probit\"\n)\n\n# A tibble: 1 × 4\n   xlim  mean lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.1 0.327 0.309 0.345\n\n\nLater on, we can compute the mean observed events in the neighborhood of multiple values between 0 and 1:\n\nlinspace &lt;- seq(0, 1, length.out = 100)\n\n\nmap(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = data_alphas[[which(alphas == 1)]]$d,\n    scores = data_alphas[[which(alphas == 1)]]$p_u,\n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n\n# A tibble: 100 × 4\n     xlim  mean lower upper\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0      0.327 0.309 0.345\n 2 0.0101 0.327 0.309 0.345\n 3 0.0202 0.327 0.309 0.345\n 4 0.0303 0.327 0.309 0.345\n 5 0.0404 0.327 0.309 0.345\n 6 0.0505 0.327 0.309 0.345\n 7 0.0606 0.327 0.309 0.345\n 8 0.0707 0.327 0.309 0.345\n 9 0.0808 0.327 0.309 0.345\n10 0.0909 0.327 0.309 0.345\n# ℹ 90 more rows\n\n\nWe define the weighted_mse() function that will then rely results obtained with the local_ci_scores() and compute the WMSE.\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n}\n\n\n\n1.2.1.6 Local Calibration Score\n\nlibrary(locfit)\n\nlocfit 1.5-9.8   2023-06-11\n\n\n\nAttaching package: 'locfit'\n\n\nThe following object is masked from 'package:purrr':\n\n    none\n\n\n\n#' Calibration score using Local Regression\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\nlocal_calib_score &lt;- function(obs, \n                              scores) {\n  \n  # Add a little noise to the scores, to avoir crashing R\n  scores &lt;- scores + rnorm(length(scores), 0, .001)\n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(\n      d = obs,\n      scores = scores\n    )\n  )\n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n  locfit_0_linspace[locfit_0_linspace &gt; 1] &lt;- 1\n  locfit_0_linspace[locfit_0_linspace &lt; 0] &lt;- 0\n  \n  # Squared difference between predicted value and the bissector, weighted by the density of values\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(linspace_raw)\n  )\n  # The weights\n  weights &lt;- dens$y[keep_linspace]\n  \n  weighted.mean((linspace - locfit_0_linspace)^2, weights)\n}"
  },
  {
    "objectID": "calibration.html#settings-for-the-simulations",
    "href": "calibration.html#settings-for-the-simulations",
    "title": "1  Calibration",
    "section": "1.3 Settings for the Simulations",
    "text": "1.3 Settings for the Simulations\nWe will now run some simulations on uncalibrated models. To do so, we will consider more or less distorted probabilities \\(p^u\\) (see Section 1.1.1). For each simulation, we first split the dataset into two parts: a calibration and a test set. While this step is not necessary here, it will be of crucial importance in Chapter 2. On the test set, we will compute standard metrics (such as MSE, accuracy, and so on, in Section 1.5) and then we will consider the previously defined calibration metrics (Section 1.4).\nFor each value of \\(\\alpha\\) or \\(\\gamma\\) that transform the initial probabilities, we replicate the simulations 200 times.\nLet us define a function, get_samples() which generates data from the DGP described in Section 1.1.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\n#' @param alpha scale parameter for the latent probability (if different \n#'   from 1, the probabilities are transformed and it may induce decalibration)\n#' @param gamma scale parameter for the latent score (if different from 1, \n#'   the probabilities are transformed and it may induce decalibration)\nget_samples &lt;- function(seed,\n                        n_obs = 2000,\n                        alpha = 1,\n                        gamma = 1) {\n  set.seed(seed)\n  data_all &lt;- sim_data(\n    n_obs = n_obs, seed = seed, alpha = alpha, gamma = gamma\n  )\n  \n  # Calibration/test sets----\n  data &lt;- data_all |&gt; select(d, x1:x4)\n  probas &lt;- data_all |&gt; select(p)\n\n  calib_index &lt;- sample(1:nrow(data), size = .6 * nrow(data), replace = FALSE)\n  tb_calib &lt;- data |&gt; slice(calib_index)\n  tb_test &lt;- data |&gt; slice(-calib_index)\n  probas_calib &lt;- probas |&gt; slice(calib_index)\n  probas_test &lt;- probas |&gt; slice(-calib_index)\n\n  list(\n    data_all = data_all,\n    data = data,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    probas_calib = probas_calib,\n    probas_test = probas_test,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs,\n    alpha = alpha,\n    gamma = gamma\n  )\n}\n\nLet us consider 200 replications for each value of \\(\\alpha = \\left\\{\\frac{1}{3}, \\frac{2}{3}, 1, \\frac{3}{2}, 3\\right\\}\\) and then \\(\\gamma = \\left\\{\\frac{1}{3}, \\frac{2}{3}, 1, \\frac{3}{2}, 3\\right\\}\\).\n\nn_repl &lt;- 200 # number of replications\nn_obs &lt;- 2000 # number of observations to draw\ngrid_alpha &lt;- expand_grid(alpha = c(1/3, 2/3, 1, 3/2, 3), seed = 1:n_repl)\ngrid_gamma &lt;- expand_grid(gamma = c(1/3, 2/3, 1, 3/2, 3), seed = 1:n_repl)"
  },
  {
    "objectID": "calibration.html#sec-calib-standard-metrics-simul",
    "href": "calibration.html#sec-calib-standard-metrics-simul",
    "title": "1  Calibration",
    "section": "1.4 Standard Metrics on Simulations",
    "text": "1.4 Standard Metrics on Simulations\nBefore turning to calibration, let us have a look at some standard metrics computed on data drawn from the PGD. We will consider 200 datasets for each value for \\(\\alpha\\) or \\(\\gamma\\), and compute various standard metrics in each replication.\nIn our predictive modeling framework, denoted as \\(h\\), we consider a binary variable \\(D\\) with observed values represented by \\(d\\), and corresponding observed features denoted as \\(\\boldsymbol X\\) with a realization \\(\\boldsymbol x\\). The predictive model \\(h\\) takes these inputs and generates a score \\(s(\\boldsymbol x)\\). This score serves as an estimation of the probability that the binary variable \\(D\\) equals 1. Effectively, \\(h\\) maps the observed features to a numerical score, reflecting the likelihood of observing \\(D=1\\). The relationship can be succinctly expressed as \\(\\mathbb{P}(D=1 | \\boldsymbol x) \\approx s(\\boldsymbol x)\\).\nBased on the drawn probabilities (that we are able to get here only because we know the data generating process) \\(p\\) and the “predicted” scores \\(s(\\boldsymbol x)\\), we can compute the Mean Squared Error (MSE): \\[\nMSE(h) = \\frac{1}{n} \\sum_{i=1}{n}\\left(s(\\boldsymbol x) - p\\right)^2\n\\] By defining a probability threshold \\(\\tau\\), we can transform the score into a binary variable \\(\\hat{D}\\) such that: \\[\n\\hat{D} = \\begin{cases}\n1, & \\text{if } s(\\boldsymbol x) \\geq \\tau,\\\\\n0, & \\text{if } s(\\boldsymbol x) &lt; \\tau,\\\\\n\\end{cases}\n\\] We will consider different values of \\(\\tau\\) for each replication of \\(h\\).\nFor specific values of \\(\\tau\\), we can compute multiple goodness of fit metrics, based on the confusion table shown in Table 1.1.\n\n\nTable 1.1: Confusion matrix.\n\n\n\n\n\n\n\n\nPredicted Positive (P)\nPredicted Negative (N)\n\n\n\n\nActual Positive (A)\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative (B)\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\n\nThe cells in this table give the following metrics:\n\nTrue Positive (TP): Instances where the model correctly predicts the positive class.\nFalse Negative (FN): Instances where the model predicts the negative class, but the true class is positive.\nFalse Positive (FP): Instances where the model predicts the positive class, but the true class is negative.\nTrue Negative (TN): Instances where the model correctly predicts the negative class.\n\nBased on these metrics, we compute the following metrics:\n\nAccuracy: Accuracy is a measure of the overall correctness of the model. It is calculated as the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. The formula is given by: \\[ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}}\\]\nMisclassification Rate: The misclassification rate, also known as the error rate, represents the proportion of incorrectly classified instances among all instances. It is calculated as the ratio of the sum of false positives and false negatives to the total number of instances. The formula is given by: \\[\\text{Misclassification Rate} = \\frac{\\text{False Positives} + \\text{False Negatives}}{\\text{Total Instances}}\\]\nSensitivity (True Positive Rate or Recall): Sensitivity measures the ability of the model to correctly identify instances of the positive class. It is the ratio of true positives to the total number of actual positive instances. The formula is given by: \\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\\]\nSpecificity (True Negative Rate): Specificity measures the ability of the model to correctly identify instances of the negative class. It is the ratio of true negatives to the total number of actual negative instances. The formula is given by: \\[ \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}\\]\n\nLet us define function compute_gof() that will compute those metrics for some true probabilities \\(p\\), observed values \\(d\\), predicted scores \\(s(\\boldsymbol x)\\), and a specific probability threshold \\(\\tau\\).\n\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\ncompute_gof &lt;- function(true_prob,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  mse &lt;- mean((true_prob - pred)^2)\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR\n  )\n}\n\nWe encompass these functions into a second one that will:\n\ndraw data from the DGP\ntransform the true probabilities accordingly with either \\(\\alpha\\) or \\(\\gamma\\), to obtain \\(p^u\\) which will be considered as the score return by some model \\(s(\\boldsymbol x) := p^u\\).\ncompute the goodness of fit metrics using those values.\n\nRecall that the R objects grid_alpha and grid_gamma contain the value of \\(\\alpha\\) and \\(\\gamma\\), respectively, to use to transform the probabilities. Each value is repeated 200 times to replicate the generation of the data from the same PGD, using the same type of probability transformation at each replication.\n\n#' Computes goodness of fit metrics for a replication\n#'\n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\ncompute_gof_simul &lt;- function(i,\n                              grid,\n                              n_obs,\n                              type = c(\"alpha\", \"gamma\")) {\n  current_seed &lt;- grid$seed[i]\n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  \n  # Get the calib/test datasets with true probabilities\n  data_all_calib &lt;- current_data$data_all |&gt;\n    slice(current_data$calib_index)\n  \n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  # Calibration set\n  true_prob_calib &lt;- data_all_calib$p_u\n  obs_calib &lt;- data_all_calib$d\n  pred_calib &lt;- data_all_calib$p\n  # Test set\n  true_prob_test &lt;- data_all_test$p_u\n  obs_test &lt;- data_all_test$d\n  pred_test &lt;- data_all_test$p\n  \n  metrics_simul_calib &lt;- map(\n    .x = seq(0, 1, by = .01),\n    .f = ~compute_gof(\n      true_prob = true_prob_calib,\n      obs = obs_calib,\n      pred = pred_calib, \n      threshold = .x\n    )\n  ) |&gt; \n    list_rbind()\n  \n  metrics_simul_test &lt;- map(\n    .x = seq(0, 1, by = .01),\n    .f = ~compute_gof(\n      true_prob = true_prob_test,\n      obs = obs_test,\n      pred = pred_test, \n      threshold = .x\n    )\n  ) |&gt; \n    list_rbind()\n  \n  roc_calib &lt;- pROC::roc(obs_calib, pred_calib)\n  auc_calib &lt;- as.numeric(pROC::auc(roc_calib))\n  roc_test &lt;- pROC::roc(obs_test, pred_test)\n  auc_test &lt;- as.numeric(pROC::auc(roc_test))\n  \n  metrics_simul_calib |&gt; \n    mutate(\n      auc = auc_calib,\n      seed = current_seed,\n      scale_parameter = transform_scale,\n      type = type,\n      sample = \"calibration\"\n    ) |&gt; \n    bind_rows(\n      metrics_simul_test |&gt; \n        mutate(\n          auc = auc_test,\n          seed = current_seed,\n          scale_parameter = transform_scale,\n          type = type,\n          sample = \"test\"\n        )\n    )\n}\n\nLet us apply this function to the different simulations. We begin with probabilities transformed according to the variation of the parameter \\(\\alpha\\).\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n  metrics_alpha &lt;- furrr::future_map(\n    .x = 1:nrow(grid_alpha),\n    .f = ~{\n      p()\n      compute_gof_simul(\n        i = .x, \n        grid = grid_alpha, \n        n_obs = n_obs, \n        type = \"alpha\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nmetrics_alpha &lt;- list_rbind(metrics_alpha)\n\nWe do the same for \\(\\gamma\\):\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n  metrics_gamma &lt;- furrr::future_map(\n    .x = 1:nrow(grid_gamma),\n    .f = ~{\n      p()\n      compute_gof_simul(\n        i = .x, \n        grid = grid_gamma, \n        n_obs = n_obs, \n        type = \"gamma\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nmetrics_gamma &lt;- list_rbind(metrics_gamma)\n\nWe define function boxplot_simuls_metrics() to plot the results of the simulations. This function will produce a panel of boxplots. Each row of the panel will correspond to a metric whereas each column will correspond to a value for either \\(\\alpha\\) or \\(\\gamma\\). On each figure, the x-axis will correspond to the value used for the probability threshold \\(\\tau\\), and the y-axis will correspond to the values of the metric.\n\n#' Boxplots for the simulations to visualize the distribution of some \n#' traditional metrics as a function of the probability threshold.\n#' And, ROC curves\n#' The resulting figure is a panel of graphs, with vayring values for the \n#' transformation applied to the probabilities (in columns) and different \n#' metrics (in rows).\n#' \n#' @param tb_metrics tibble with computed metrics for the simulations\n#' @param type type of transformation: `\"alpha\"` or `\"gamma\"`\n#' @param metrics names of the metrics computed\nboxplot_simuls_metrics &lt;- function(tb_metrics,\n                                   type = c(\"alpha\", \"gamma\"),\n                                   metrics) {\n  scale_parameters &lt;- unique(tb_metrics$scale_parameter)\n  \n  par(mfrow = c(length(metrics), length(scale_parameters)))\n  for (i_metric in 1:length(metrics)) {\n    metric &lt;- metrics[i_metric]\n    for (i_scale_parameter in 1:length(scale_parameters)) {\n      scale_parameter &lt;- scale_parameters[i_scale_parameter]\n      \n      tb_metrics_current &lt;- tb_metrics |&gt; \n        filter(scale_parameter == !!scale_parameter)\n      \n      if (metric == \"roc\") {\n        seeds &lt;- unique(tb_metrics_current$seed)\n        if (i_metric == 1) {\n          # first row\n          title &lt;- latex2exp::TeX(\n            str_c(\"$\\\\\", type, \" = \", round(scale_parameter, 2), \"$\")\n          )\n          size_top &lt;- 2.1\n        } else if (i_metric == length(metrics)) {\n          # Last row\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        } else {\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        }\n        \n        if (i_scale_parameter == 1) {\n          # first column\n          y_lab &lt;- str_c(metric, \"\\n True Positive Rate\") \n          size_left &lt;- 5.1\n        } else {\n          y_lab &lt;- \"\"\n          size_left &lt;- 4.1\n        }\n        \n        par(mar = c(4.5, size_left, size_top, 2.1))\n        plot(\n          0:1, 0:1,\n          type = \"l\", col = NULL,\n          xlim = 0:1, ylim = 0:1,\n          xlab = \"False Positive Rate\", \n          ylab = y_lab,\n          main = \"\"\n        )\n        for (i_seed in 1:length(seeds)) {\n          tb_metrics_current_seed &lt;- \n            tb_metrics_current |&gt; \n            filter(seed == seeds[i_seed])\n          lines(\n            x = tb_metrics_current_seed$FPR, \n            y = tb_metrics_current_seed$sensitivity,\n            lwd = 2, col = adjustcolor(\"black\", alpha.f = .04)\n          )\n        }\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n      } else {\n        # not ROC\n        tb_metrics_current &lt;- \n          tb_metrics_current |&gt; \n          filter(threshold %in% seq(0, 1, by = .1))\n        form &lt;- str_c(metric, \"~threshold\")\n        if (i_metric == 1) {\n          # first row\n          title &lt;- latex2exp::TeX(\n            str_c(\"$\\\\\", type, \" = \", round(scale_parameter, 2), \"$\")\n          )\n          size_top &lt;- 2.1\n        } else if (i_metric == length(metrics)) {\n          # Last row\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        } else {\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        }\n        \n        if (i_scale_parameter == 1) {\n          # first column\n          y_lab &lt;- metric\n        } else {\n          y_lab &lt;- \"\"\n        }\n        \n        par(mar = c(4.5, 4.1, size_top, 2.1))\n        boxplot(\n          formula(form), data = tb_metrics_current,\n          xlab = \"Threshold\", ylab = y_lab,\n          main = title\n        )\n      }\n    }\n  }\n}\n\nWe aim to create a set of boxplots to visually assess the influence of probability transformations using \\(\\alpha\\) or \\(\\gamma\\) on standard metrics. Whenever \\(\\alpha \\neq 1\\) or \\(\\gamma \\neq 1\\), the resulting scores \\(p^u\\) represent values akin to those obtained from an uncalibrated model. Our focus is on investigating whether these transformations have any discernible impact on the standard metrics. The key question is to understand if adjusting the probabilities through these transformations introduces noticeable changes in the model’s performance as evaluated by the standard metrics. The results are shown in Figure 1.6 for vayring values of \\(\\alpha\\), and in Figure 1.7 for vayring values of \\(\\gamma\\).\n\nmetrics &lt;- c(\"mse\", \"accuracy\", \"sensitivity\", \"specificity\", \"roc\", \"auc\")\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(\n  tb_metrics = metrics_alpha |&gt; filter(sample == \"test\"), \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 1.6: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = metrics_gamma |&gt; filter(sample == \"test\"),\n                       type = \"gamma\", metrics = metrics)\n\n\n\n\nFigure 1.7: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\).\n\n\n\n\n\n\n\n\nLet us also visualize some standard metrics with a fix threshold of \\(\\tau=.5\\).\n\nplot_boxplot_metric_2 &lt;- function(tb_metrics, type, metrics, metrics_labs) {\n  \n  df_plot &lt;- tb_metrics |&gt; filter(threshold == 0.5)\n  colours &lt;- RColorBrewer::brewer.pal(\n    5+1, name = \"Blues\"\n  )\n  colours &lt;- colours[-1]\n  colours[3] &lt;- \"orange\"\n  if (length(unique(df_plot$scale_parameter)) == 3) {\n    colours &lt;- colours[c(1, 3, 5)]\n  }\n  \n  \n  \n  for (i_metric in 1:length(metrics)) {\n    y_lab &lt;- str_c(\"$\", type, \"=\", sort(round(unique(df_plot$scale_parameter), 2)), \"$\")\n    metric &lt;- metrics[i_metric]\n    metric_lab &lt;- metrics_labs[i_metric]\n    boxplot(\n      formula(str_c(metric, \"~ scale_parameter\")),\n      data = df_plot,\n      las = 1,\n      xlab = \"\", ylab = \"\",\n      main = metric_lab,\n      col = rev(colours),\n      horizontal = TRUE,\n      yaxt = \"n\"\n    )\n    axis(\n      side = 2, at = 1:length(y_lab), \n      labels = latex2exp::TeX(y_lab), las = 2\n    )\n  }\n}\n\nWe focus on the folllwing metrics:\n\nmetrics &lt;- c(\"accuracy\", \"sensitivity\", \"specificity\", \"auc\")\nmetrics_labs &lt;- c(\"Accuracy\", \"Sensitivity\", \"Specificity\", \"AUC\")\n\n\n\nDisplay the R codes used to produce the Figure.\npar(mar = c(4.1, 4.1, 2.1, 2.1), mfrow = c(2,4))\nplot_boxplot_metric_2(\n  tb_metrics = metrics_alpha |&gt; filter(scale_parameter %in% c(1/3, 1, 3)) |&gt; \n    filter(sample == \"test\"), \n  type = \"alpha\", \n  metrics = metrics, metrics_labs = metrics_labs\n)\nplot_boxplot_metric_2(\n  tb_metrics = metrics_gamma |&gt; filter(scale_parameter %in% c(1/3, 1, 3)) |&gt; \n    filter(sample == \"test\"), \n  type = \"gamma\", \n  metrics = metrics, metrics_labs = metrics_labs\n)\n\n\n\n\nFigure 1.8: Standard Goodness of Fit Metrics on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom. The probability threshold is set to \\(\\tau=0.5\\)"
  },
  {
    "objectID": "calibration.html#sec-calib-metrics-simul",
    "href": "calibration.html#sec-calib-metrics-simul",
    "title": "1  Calibration",
    "section": "1.5 Calibration Metrics on Simulations",
    "text": "1.5 Calibration Metrics on Simulations\nWe define the f_simul() function to run a single simulation.\n\n#' Performs one replication for a simulation\n#' \n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\n#' @param linspace values at which to compute the mean observed event when computing the WMSE\nf_simul &lt;- function(i, \n                    grid, \n                    n_obs, \n                    type = c(\"alpha\", \"gamma\"),\n                    linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 100)\n  \n  current_seed &lt;- grid$seed[i]\n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  # Get the calib/test datasets with true probabilities\n  data_all_calib &lt;- current_data$data_all |&gt;\n    slice(current_data$calib_index)\n  \n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n\n  # Transformed probabilities\n  p_u_calib &lt;- data_all_calib$p_u\n  p_u_test &lt;- data_all_test$p_u\n  # Observed events\n  d_calib &lt;- data_all_calib$d\n  d_test &lt;- data_all_test$d\n  \n  # Mean observed events\n  expected_events_calib &lt;- map(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = data_all_calib$d,\n    scores = data_all_calib$p_u, \n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n  \n  expected_events_test &lt;- map(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = data_all_test$d,\n    scores = data_all_test$p_u, \n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n  \n  # Compute Metrics\n  ## Calibration set\n  mse_calib &lt;- mean((data_all_calib$p - data_all_calib$p_u)^2)\n  brier_calib &lt;- brier_score(obs = d_calib, score = p_u_calib)\n  ece_calib &lt;- e_calib_error(\n    obs = d_calib, scores = p_u_calib, k = 10, threshold = .5\n  )\n  qmse_calib &lt;- qmse_error(\n    obs = d_calib, score = p_u_calib, k = 10, threshold = .5\n  )\n  wmse_calib &lt;- weighted_mse(\n    local_scores = expected_events_calib, scores = p_u_calib\n  )\n  lcs_calib &lt;- local_calib_score(obs = d_calib, scores = p_u_calib)\n  \n  ## Test Set\n  mse_test &lt;- mean((data_all_test$p - data_all_test$p_u)^2)\n  brier_test &lt;- brier_score(obs = d_test, score = p_u_test)\n  ece_test &lt;- e_calib_error(\n    obs = d_test, scores = p_u_test, k = 10, threshold = .5\n  )\n  qmse_test &lt;- qmse_error(\n    obs = d_test, score = p_u_test, k = 10, threshold = .5\n  )\n  wmse_test &lt;- weighted_mse(\n    local_scores = expected_events_test, scores = p_u_test\n  )\n  lcs_test &lt;- local_calib_score(obs = d_test, scores = p_u_test)\n  \n  tibble(\n    seed = grid$seed[i],\n    scale_parameter = transform_scale,\n    type = type,\n    sample = \"calibration\",\n    mse = mse_test,\n    brier = brier_test,\n    ece = ece_test,\n    qmse = qmse_test,\n    wmse = wmse_test,\n    lcs = lcs_test\n  ) |&gt; \n    bind_rows(\n      tibble(\n        seed = grid$seed[i],\n        scale_parameter = transform_scale,\n        type = type,\n        sample = \"test\",\n        mse = mse_test,\n        brier = brier_test,\n        ece = ece_test,\n        qmse = qmse_test,\n        wmse = wmse_test,\n        lcs = lcs_test\n      )\n    )\n}\n\nWe run the replications of the simulations for each value of \\(\\alpha\\):\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n  simul_alpha &lt;- furrr::future_map(\n    .x = 1:nrow(grid_alpha),\n    .f = ~{\n      p()\n      f_simul(\n        i = .x, \n        grid = grid_alpha, \n        n_obs = n_obs, \n        type = \"alpha\", \n        linspace = NULL\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nsimul_alpha &lt;- list_rbind(simul_alpha)\n\nAnd we do the same for each value of \\(\\gamma\\):\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n  simul_gamma &lt;- furrr::future_map(\n    .x = 1:nrow(grid_gamma),\n    .f = ~{\n      p()\n      f_simul(\n        i = .x, \n        grid = grid_gamma, \n        n_obs = n_obs, \n        type = \"gamma\", \n        linspace = NULL\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nsimul_gamma &lt;- list_rbind(simul_gamma)\n\n\n1.5.1 Results\n\ncalib_metrics &lt;- \n  simul_alpha |&gt;\n  bind_rows(simul_gamma) |&gt; \n  pivot_longer(\n    cols = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\", \"lcs\"),\n    names_to = \"metric\", values_to = \"value\"\n  ) |&gt; \n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\", \"lcs\"),\n      labels = c(\"MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\", \"LCS\")\n    )\n  )\n\n\nload(\"calib_metrics.rda\")\n\n\n#' @param current_metric name of the metric to display (MSE, Brier Score, \n#'   ECE, QMSE, or WMSE)\n#' @param calib_metrics tibble with the metrics computed on the test set\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\nplot_boxplot_metric &lt;- function(current_metric,\n                                calib_metrics, \n                                type = c(\"alpha\", \"gamma\"),\n                                sample = \"test\") {\n  \n  data_plot &lt;- calib_metrics |&gt;\n    filter(sample == !!sample) |&gt; \n    filter(metric == current_metric, type == !!type) |&gt;\n    mutate(\n      label = str_c(\"$\\\\\", type, \"$=\", scale_parameter, \"$\")\n    )\n  \n  labels_y &lt;- rep(unique(data_plot$label))\n  par(mar = c(4.1, 4.1, 2.1, 2.1))\n  boxplot(\n    value ~ scale_parameter,\n    data = data_plot,\n    xlab = latex2exp::TeX(str_c(\"$\\\\\", type, \"$\")),\n    ylab = current_metric\n  )\n}\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nMSEBrier ScoreECEQMSEWMSELCS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSEBrier ScoreECEQMSEWMSELCS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us also display these results in a different way.\n\n#' @param current_metric name of the metric to display (MSE, Brier Score, \n#'   ECE, QMSE, or WMSE)\n#' @param calib_metrics tibble with the metrics computed on the test set\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\nplot_boxplot_metric_2 &lt;- function(current_metric,\n                                  calib_metrics, \n                                  type = c(\"alpha\", \"gamma\"),\n                                  sample = \"test\") {\n  \n  data_plot &lt;- calib_metrics |&gt;\n    filter(metric == current_metric, type == !!type) |&gt;\n    filter(sample == !!sample) |&gt; \n    mutate(\n      label = str_c(\"$\\\\\", type, \"$=\", round(scale_parameter, 2), \"$\")\n    ) |&gt; \n    mutate(scale_parameter = round(scale_parameter, 2))\n  \n  \n  if (as.character(current_metric) == \"QMSE\") {\n    title &lt;- \"QBS\"\n  } else if (as.character(current_metric) == \"MSE\") {\n    title &lt;- \"True MSE\"\n  } else {\n    title &lt;- current_metric\n  }\n  \n  labels_y &lt;- rep(unique(data_plot$label))\n  colours &lt;- RColorBrewer::brewer.pal(\n    5+1, name = \"Blues\"\n  )\n  colours &lt;- colours[-1]\n  colours[3] &lt;- \"orange\"\n  if (length(labels_y) == 3) {\n    colours &lt;- colours[c(1, 3, 5)]\n  }\n  boxplot(\n    value ~ scale_parameter,\n    data = data_plot,\n    ylab = \"\",\n    xlab = \"\",\n    las = 1,\n    main = title,\n    col = colours,\n    horizontal = TRUE,\n    yaxt = \"n\"\n  )\n  axis(\n    side = 2, at = 1:length(labels_y), \n    labels = latex2exp::TeX(labels_y), las = 2\n  )\n}\n\n\n\nDisplay the R codes used to produce the Figure.\ntb_lab &lt;- expand_grid(\n  metric = c(\"MSE\", \"Brier Score\", \"ECE\", \"LCS\"),\n  type = c(\"alpha\", \"gamma\")\n) |&gt; \n  mutate(\n    metric = factor(metric, levels = c(\"MSE\", \"Brier Score\", \"ECE\", \"LCS\")),\n    type = factor(type, levels = c(\"alpha\", \"gamma\"))\n  ) |&gt; \n  arrange(type, metric) |&gt; \n  mutate(\n    metric = as.character(metric)\n  )\npar(mfrow = c(2,4), mar = c(4.1, 2.5, 2.1, 2.1))\nfor (i in 1:nrow(tb_lab)) {\n  plot_boxplot_metric_2(\n    current_metric = tb_lab$metric[i], \n    calib_metrics = calib_metrics,\n    type = tb_lab$type[i]\n  )\n}\n\n\n\n\nFigure 1.9: Calibration Metrics on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom)."
  },
  {
    "objectID": "calibration.html#sec-calib-visualization",
    "href": "calibration.html#sec-calib-visualization",
    "title": "1  Calibration",
    "section": "1.6 Visualizations",
    "text": "1.6 Visualizations\nLet us visualize the calibration for the different decalibrated models. We can either use a quantile-based approach, or a method based on moving averages.\n\n1.6.1 Quantile-Based Bins\nWe visualize the calibration curve similarly to what is done with the calibration_curve() method from sci-kit learn.\nThe x-axis of the calibration plot reports the mean predicted probabilities computed on different bins, where the bins are defined using the deciles of the predicted scores (using the get_summary_bins() function previously defined). On the y-axis, the corresponding fraction of positive events (\\(D=1\\)) are reported.\nAs we simulated data on multiple samples, we can show the calibration curve for all the replications. We can also focus on a single replication and use the confidence interval computed in the get_summary_bins() function.\nWe define a function, calibration_curve_quant_simul(), to get the summary for each bin, for a single replication.\n\n#' Get the calibration curve for one simulation\n#' using the quantile-based approach\n#' \n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\n#' @param linspace values at which to compute the mean observed event when computing the WMSE\ncalibration_curve_quant_simul &lt;- function(i, \n                                          grid, \n                                          n_obs, \n                                          type = c(\"alpha\", \"gamma\"),\n                                          linspace = NULL) {\n\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 100)\n  \n  current_seed &lt;- grid$seed[i]\n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  # Get the test dataset with true probabilities\n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  summary_bins &lt;- get_summary_bins(\n    obs = data_all_test$d,\n    scores = data_all_test$p_u, \n    k = 10, threshold = .5)\n  summary_bins |&gt; \n    select(score_class, mean_score, mean_obs) |&gt; \n    mutate(\n      seed = grid$seed[i],\n      scale_parameter = transform_scale,\n      type = type\n    )\n}\n\n\n1.6.1.1 All the Simulations\nLet us retrieve the calibration curve values for each value of \\(\\alpha\\), anb then for \\(\\gamma\\), for all replications.\nFor \\(\\alpha\\):\n\ntb_calibration_curve_quant_alphas &lt;- map(\n  .x = 1:nrow(grid_alpha),\n  .f = ~calibration_curve_quant_simul(\n    i = .x, \n    grid = grid_alpha, \n    n_obs = n_obs, \n    type = \"alpha\"\n  )\n) |&gt; \n  list_rbind()\n\nFor \\(\\gamma\\):\n\ntb_calibration_curve_quant_gammas &lt;- map(\n  .x = 1:nrow(grid_gamma),\n  .f = ~calibration_curve_quant_simul(\n    i = .x, \n    grid = grid_gamma, \n    n_obs = n_obs, \n    type = \"gamma\"\n  )\n) |&gt; \n  list_rbind()\n\nWe bind those two tibbles into one:\n\ntb_calibration_curve_quant &lt;- \n  tb_calibration_curve_quant_alphas |&gt; \n  bind_rows(tb_calibration_curve_quant_gammas)\n\nNext, let us define a plot function to display the calibration curves for each value of one of the scaling parameter (\\(\\alpha\\) or \\(\\gamma\\)), for all the replications.\nIf we want to add barplots showing the distributions of the true probabilities on top of the graphs, we need to define bins and compute the average number of observation in each bin over the 200 replications.\n\nget_count &lt;- function(seed, alpha = 1, gamma = 1) {\n  breaks &lt;- seq(0, 1, by = .05)\n  current_data &lt;- get_samples(seed = seed, n_obs = 2000, alpha = alpha, gamma = gamma)\n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n \n  nb_0_bins &lt;- table(cut(data_all_test |&gt; filter(d == 0) |&gt; pull(p_u), breaks = breaks))\n  nb_1_bins &lt;- table(cut(data_all_test |&gt; filter(d == 1) |&gt; pull(p_u), breaks = breaks))\n  tibble(\n    bins = names(nb_0_bins),\n    nb_0_bins = as.vector(nb_0_bins),\n    nb_1_bins = as.vector(nb_1_bins),\n    seed = seed,\n    alpha = alpha,\n    gamma = gamma\n  )\n}\n\nWe define a grid with all the combinations of values for \\(\\alpha\\) and \\(\\gamma\\) used in the simulations.\n\ngrid &lt;- expand_grid(seed = 1:n_repl, alpha = alphas) |&gt; \n  mutate(gamma = 1) |&gt; \n  bind_rows(\n    expand_grid(seed = 1:n_repl, gamma = gammas) |&gt; \n      mutate(alpha = 1)\n  )\n\nThen, we can apply the get_count() function to get the average number of observation in each bin.\n\ncounts_samples &lt;- map(\n  .x = 1:nrow(grid), \n  ~get_count(\n    seed = grid$seed[.x], \n    alpha = grid$alpha[.x],\n    gamma = grid$gamma[.x]\n  ),\n  .progress = TRUE\n) |&gt; \n  list_rbind() |&gt; \n  group_by(bins, alpha, gamma) |&gt; \n  summarise(\n    nb_0_bins = mean(nb_0_bins),\n    nb_1_bins = mean(nb_1_bins),\n  )\n\nLet us save for use in Chapter 2.\n\nsave(counts_samples, file = \"simul-calib-counts_samples.rda\")\n\nFor example, in the 200 simulations run with \\(\\alpha=1\\) and \\(\\gamma=3\\):\n\ncounts_samples |&gt; \n  filter(alpha == 1, gamma == 3)\n\n# A tibble: 20 × 5\n# Groups:   bins, alpha [20]\n   bins       alpha gamma nb_0_bins nb_1_bins\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (0,0.05]       1     3     12.6       3.94\n 2 (0.05,0.1]     1     3     22.8       9.54\n 3 (0.1,0.15]     1     3     23.4      12.4 \n 4 (0.15,0.2]     1     3     24.7      14.2 \n 5 (0.2,0.25]     1     3     23.6      15.6 \n 6 (0.25,0.3]     1     3     23.4      17.0 \n 7 (0.3,0.35]     1     3     21.7      17.7 \n 8 (0.35,0.4]     1     3     22.2      19.1 \n 9 (0.4,0.45]     1     3     22.2      19.6 \n10 (0.45,0.5]     1     3     21.5      20.6 \n11 (0.5,0.55]     1     3     20.6      22.0 \n12 (0.55,0.6]     1     3     20.5      22.6 \n13 (0.6,0.65]     1     3     20.1      24.0 \n14 (0.65,0.7]     1     3     19.4      24.9 \n15 (0.7,0.75]     1     3     18.8      26.5 \n16 (0.75,0.8]     1     3     18.7      28.0 \n17 (0.8,0.85]     1     3     17.7      29.6 \n18 (0.85,0.9]     1     3     16.4      31.1 \n19 (0.9,0.95]     1     3     13.4      31.4 \n20 (0.95,1]       1     3      6.20     20.1 \n\n\nLet us now define the function that will plot the calibration curves.\n\n#' Plot the calibration curve for a set of simulations, using quantile-based\n#' bins to obtain the estimations\n#'\n#' @param tb_calibration_curve\n#' @param counts_samples tibble with the average number of observation with\n#'   true probability in each bin defined along the [0,1] segment.\n#' @param type deformation probability type (either `alpha` or `gamma`)\nplot_calibration_quant_simul &lt;- function(tb_calibration_curve,\n                                         counts_samples,\n                                         type = c(\"alpha\", \"gamma\")) {\n  tb_calibration_curve_curr &lt;-  tb_calibration_curve |&gt; \n    filter(type == !!type)\n  \n  scale_params &lt;- unique(tb_calibration_curve_curr$scale_parameter)\n  seeds &lt;- unique(tb_calibration_curve_curr$seed)\n  \n  for (scale_param in scale_params) {\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    \n    # Histogram\n    ## Calculate the heights for stacking\n    if (type == \"alpha\") {\n      counts_samples_current &lt;- \n        counts_samples |&gt; \n        filter(gamma == 1, alpha == !!scale_param)\n    } else {\n      counts_samples_current &lt;- \n        counts_samples |&gt; \n        filter(alpha == 1, gamma == !!scale_param)\n    }\n    heights &lt;- rbind(\n      counts_samples_current$nb_0_bins, \n      counts_samples_current$nb_1_bins\n    )\n    col_bars &lt;- c(\"#CC79A7\", \"#E69F00\")\n    par(mar = c(0.5, 4.5, 3.0, 0.5))\n    barplot(\n      heights, \n      col = col_bars, \n      border = \"white\", \n      space = 0,\n      xlab = \"\", ylab = \"\", main = latex2exp::TeX(title),\n      axes = FALSE,\n    )\n    par(mar = c(4.1, 4.5, 0.5, 0.5))\n    colour &lt;- ifelse(scale_param == 1, wongOrange, wongBlue)\n    plot(\n      0:1, 0:1,\n      type = \"l\", col = NULL,\n      xlim = 0:1, ylim = 0:1,\n      xlab = latex2exp::TeX(\"$p^u$\"), \n      ylab = latex2exp::TeX(\"$\\\\hat{E}(D | p^u = p^c)$\"),\n      main = \"\"\n    )\n    for (i_simul in seeds) {\n      tb_current &lt;- tb_calibration_curve_curr |&gt; \n        filter(\n          scale_parameter == scale_param,\n          seed == i_simul\n        )\n      lines(\n        tb_current$mean_score, tb_current$mean_obs,\n        lwd = 1, cex = .1, \n        col = adjustcolor(colour, alpha.f = 0.1), t = \"b\",\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\nLastly, let us visualize the calibration curves, using the plot_calibration_quant_simul() function.\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3))\nplot_calibration_quant_simul(\n  tb_calibration_curve = tb_calibration_curve_quant, \n  type = \"alpha\",\n  counts_samples = counts_samples\n)\n\n\n\nFigure 1.10: Calibration curve for the 200 replications for varying values of \\(\\alpha\\)\n\n\n\n\n\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3))\nplot_calibration_quant_simul(\n  tb_calibration_curve = tb_calibration_curve_quant, \n  type = \"gamma\", counts_samples = counts_samples\n)\n\n\n\nFigure 1.11: Calibration curve for the 200 replications for varying values of \\(\\gamma\\)\n\n\n\n\n\n\n\n\n\n\n\n1.6.2 Calibration Curve with Local Regression\nWe would like to introduce another way of showing the calibration curve, not based on bins. Instead, to have a smoother curve, we will fit a local regression model with degree 0. Using a degree 0 allows us to calculate the local mean of the observed events in the neighborhood of a predicted probability. We will regress the observed events on the predicted probabilities. Then, in a second step, we will use the trained model to make predictions on a line space with values in \\([0,1]\\).\nThe {locfit} library is required:\n\nlibrary(locfit)\n\nLet us define a function, calibration_curve_locfit_simul() that will perform the local regression for a single replication of our simulations:\n\n#' Get the calibration curve for one simulation\n#' using the local regression-based approach\n#' \n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the\n#'   deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\ncalibration_curve_locfit_simul &lt;- function(i,\n                                           grid,\n                                           n_obs,\n                                           type = c(\"alpha\", \"gamma\")) {\n  \n  \n  current_seed &lt;- grid$seed[i]\n  \n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  # Get the test dataset with true probabilities\n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(p_u, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, data = data_all_test\n  )\n  \n  scores &lt;- data_all_test$p_u\n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  score_c_locfit_0 &lt;- predict(locfit_0, newdata = linspace)\n  score_c_locfit_0[score_c_locfit_0 &gt; 1] &lt;- 1\n  score_c_locfit_0[score_c_locfit_0 &lt; 0] &lt;- 0\n  \n  tibble(\n    seed = grid$seed[i],\n    scale_parameter = transform_scale,\n    type = type,\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0\n  )\n}\n\nThe curves for all the values of \\(\\alpha\\):\n\ntb_calibration_curve_locfit_alphas &lt;- map(\n  .x = 1:nrow(grid_alpha),\n  .f = ~calibration_curve_locfit_simul(\n    i = .x, \n    grid = grid_alpha, \n    n_obs = n_obs, \n    type = \"alpha\"\n  )\n) |&gt; \n  list_rbind()\n\nAnd for \\(\\gamma\\):\n\ntb_calibration_curve_locfit_gammas &lt;- map(\n  .x = 1:nrow(grid_gamma),\n  .f = ~calibration_curve_locfit_simul(\n    i = .x, \n    grid = grid_gamma, \n    n_obs = n_obs, \n    type = \"gamma\"\n  )\n) |&gt; \n  list_rbind()\n\nWe bind those two tibbles into one:\n\ntb_calibration_curve_locfit &lt;- \n  tb_calibration_curve_locfit_alphas |&gt; \n  bind_rows(tb_calibration_curve_locfit_gammas)\n\nLet us define a function to plot the calibration curves for all simulation and values of one of the scaling parameter (\\(\\alpha\\) or \\(\\gamma\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#' Plot the calibration curve for a set of simulations, using local regression\n#' to obtain the estimations\n#'\n#' @param tb_calibration_curve\n#' @param type deformation probability type (either `alpha` or `gamma`)\n#' @param counts_samples tibble with the average number of observation with\n#'   true probability in each bin defined along the [0,1] segment.\nplot_calibration_locfit_simuls &lt;- function(tb_calibration_curve,\n                                           type = c(\"alpha\", \"gamma\"),\n                                           counts_samples = counts_samples) {\n  \n  tb_calibration_curve_curr &lt;-  tb_calibration_curve |&gt; \n    filter(type == !!type)\n  \n  scale_params &lt;- unique(tb_calibration_curve_curr$scale_parameter)\n  \n  \n  df_plot &lt;- tb_calibration_curve_curr |&gt; \n    group_by(type, scale_parameter, xlim) |&gt; \n    summarise(\n      mean = mean(locfit_pred),\n      lower = quantile(locfit_pred, probs = .025),\n      upper = quantile(locfit_pred, probs = .975),\n      .groups = \"drop\"\n    )\n  \n  for (scale_param in scale_params) {\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    # Histogram\n    ## Calculate the heights for stacking\n    if (type == \"alpha\") {\n      counts_samples_current &lt;- \n        counts_samples |&gt; \n        filter(gamma == 1, alpha == !!scale_param)\n    } else {\n      counts_samples_current &lt;- \n        counts_samples |&gt; \n        filter(alpha == 1, gamma == !!scale_param)\n    }\n    heights &lt;- rbind(\n      counts_samples_current$nb_0_bins, \n      counts_samples_current$nb_1_bins\n    )\n    col_bars &lt;- c(\"#CC79A7\", \"#E69F00\")\n    par(mar = c(0.5, 4.5, 3.0, 0.5))\n    barplot(\n      heights, \n      col = col_bars, \n      border = \"white\", \n      space = 0,\n      xlab = \"\", ylab = \"\", main = latex2exp::TeX(title),\n      axes = FALSE,\n    )\n    \n    par(mar = c(4.1, 4.5, 0.5, 0.5))\n    df_plot_current &lt;- df_plot |&gt; filter(scale_parameter == scale_param)\n    colour &lt;- ifelse(scale_param == 1, wongOrange, wongBlue)\n    plot(\n      df_plot_current$xlim, df_plot_current$mean,\n      type = \"l\", col = colour,\n      xlim = 0:1, ylim = 0:1,\n      xlab = latex2exp::TeX(\"$p^u$\"), \n      ylab = latex2exp::TeX(\"$\\\\hat{E}(D | p^u = p^c)$\"),\n      main = \"\"\n    )\n    polygon(\n      c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n      c(df_plot_current$lower, rev(df_plot_current$upper)),\n      col = adjustcolor(col = colour, alpha.f = .4),\n      border = NA\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\nLastly, we can plot the graphs:\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3, 1,3))\nplot_calibration_locfit_simuls(\n  tb_calibration_curve = tb_calibration_curve_locfit,\n  type = \"alpha\", counts_samples = counts_samples\n)\n\n\n\nFigure 1.12: Calibration curve for different values of \\(\\gamma\\), using local regression\n\n\n\n\n\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3, 1,3))\nplot_calibration_locfit_simuls(\n  tb_calibration_curve = tb_calibration_curve_locfit,\n  type = \"gamma\", counts_samples = counts_samples\n)\n\n\n\nFigure 1.13: Calibration curve for different values of \\(\\gamma\\), using local regression\n\n\n\n\n\n\n\n\n\n1.6.2.1 Looking at a Single Replication\nWe would like to plot the calibration curve for a single replication obtained with local regression, also showing the distribution of the observed events.\nWe define a function that simulates data, estimates the calibration curve with local regression, and then returns the simulated data and the calibration curve.\n\n#' Simulates data and estimate the calibration curve with local regression\n#' \n#' @param seed desired seed\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\n#' @param type deformation probability type (either `alpha` or `gamma`)\n#' @param transform_scale scale value (either `alpha` or `gamma`)\n#' @param ci_level level for confidence intervals. \n#'   For 95% confidence interval: .95 (if `NULL`, confidence  intervals are \n#'   not computed)\n#' @param nn fraction of nearest neighbors\n#' @returns list with two elements:\n#'   - `data_all_test`: simulated data\n#'   - `tb_calibration_curve_locfit`: calibration curve \ncalibration_curve_locfit_param &lt;- function(seed,\n                                           n_obs,\n                                           transform_scale,\n                                           type = c(\"alpha\", \"gamma\"),\n                                           ci_level = NULL,\n                                           nn = 10) {\n  \n  \n  if (type == \"alpha\") {\n    current_data &lt;- get_samples(\n      seed = seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    current_data &lt;- get_samples(\n      seed = seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  # Get the test dataset with true probabilities\n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(p_u, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, data = data_all_test\n  )\n  \n  scores &lt;- data_all_test$p_u\n  \n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  score_c_locfit_0 &lt;- predict(locfit_0, newdata = linspace)\n  score_c_locfit_0[score_c_locfit_0 &lt; 0] &lt;- 0\n  score_c_locfit_0[score_c_locfit_0 &gt; 1] &lt;- 1\n  \n \n  tb_calibration_curve_locfit &lt;- tibble(\n    seed = seed,\n    scale_parameter = transform_scale,\n    type = type,\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0\n  )\n  \n   # Confidence intervals\n  if (!is.null(ci_level)) {\n    tb_ci &lt;- map(\n      .x = linspace,\n      .f = ~local_ci_scores(\n        obs = data_all_test$d, \n        scores = data_all_test$p_u,\n        tau = .x, \n        nn = nn, \n        prob = ci_level, \n        method = \"probit\"\n      )\n    ) |&gt; \n      bind_rows()\n    tb_calibration_curve_locfit &lt;- \n      tb_calibration_curve_locfit |&gt; \n      left_join(\n        tb_ci, by = \"xlim\"\n      )\n  }\n  \n  list(\n    data_all_test = data_all_test,\n    tb_calibration_curve_locfit = tb_calibration_curve_locfit\n  )\n  \n}\n\nLet us simulate data for varying values of \\(\\alpha\\) and estimate the calibration curve with local regression of degree 0:\n\nres_calibration_curve_locfit_alphas &lt;- map(\n  .x = alphas,\n  .f = ~calibration_curve_locfit_param(\n    seed = 1, \n    n_obs = n_obs,\n    transform_scale = .x,\n    type = \"alpha\",\n    ci_level = .95, \n    nn = .15\n  )\n)\n\nWe do the same for varying values of \\(\\gamma\\):\n\nres_calibration_curve_locfit_gammas &lt;- map(\n  .x = gammas,\n  .f = ~calibration_curve_locfit_param(\n    seed = 1, \n    n_obs = n_obs,\n    transform_scale = .x,\n    type = \"gamma\",\n    ci_level = .95, \n    nn = .15\n  )\n)\n\nThen, we define a function to plot the calibration curves for varying values of one of the scaling parameter (\\(\\alpha\\) or \\(\\gamma\\)):\n\n#' Plot the calibration curve for a set of simulations, using local regression\n#' to obtain the estimations\n#'\n#' @param tb_calibration_curve calibration data from \n#'   `calibration_curve_locfit_param()`\n#' @param type deformation probability type (either `alpha` or `gamma`)\n#' @param mfrow mfrow for plot\nplot_calibration_locfit_simul &lt;- function(tb_calibration_curve,\n                                          type = c(\"alpha\", \"gamma\"),\n                                          mfrow = NULL) {\n  if (!is.null(mfrow)) par(mfrow = mfrow)\n  scale_params &lt;- map(tb_calibration_curve, \"tb_calibration_curve_locfit\") |&gt; \n    map_dbl(~unique(.x$scale_parameter))\n\n  for (i in 1:length(tb_calibration_curve)) {\n    scale_param &lt;- scale_params[i]\n    df_plot_current &lt;- tb_calibration_curve[[i]]$tb_calibration_curve_locfit\n    df_data_current &lt;- tb_calibration_curve[[i]]$data_all_test\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    colour &lt;- ifelse(scale_param == 1, wongOrange, wongBlue)\n    \n    # Histogram with values\n    par(mar = c(0.5, 4.3, 3.0, 0.5))\n    # Event == 0\n    hist(\n      df_data_current |&gt; filter(d == 0) |&gt; pull(p_u),\n      breaks = seq(0, 1, by = .05),\n      col = adjustcolor(\"#CC79A7\", alpha.f = .4), border = \"white\",\n      axes = FALSE,\n      xlab = \"\", ylab = \"\",\n      main = latex2exp::TeX(title)\n    )\n    # Event == 1\n    hist(\n      df_data_current |&gt; filter(d == 1) |&gt; pull(p_u),\n      breaks = seq(0, 1, by = .05),\n      col = adjustcolor(\"#E69F00\", alpha.f = .4), border = \"white\",\n      add = TRUE,\n      axes = FALSE,\n      xlab = \"\", ylab = \"\"\n    )\n    par(mar = c(4.1, 4.3, 0.5, 0.5))\n    plot(\n      df_plot_current$xlim, df_plot_current$locfit_pred,\n      type = \"l\", col = colour,\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n      main = \"\"\n    )\n    polygon(\n      c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n      c(df_plot_current$lower, rev(df_plot_current$upper)),\n      col = adjustcolor(col = colour, alpha.f = .4),\n      border = NA\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\nWe can now plot the curves:\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3))\nplot_calibration_locfit_simul(\n  tb_calibration_curve = res_calibration_curve_locfit_alphas, \n  type = \"alpha\"\n)\n\n\n\nFigure 1.14: Calibration curve for different values of \\(\\alpha\\), using local regression (for a single replication)\n\n\n\n\n\n\n\n\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3))\nplot_calibration_locfit_simul(\n  tb_calibration_curve = res_calibration_curve_locfit_gammas, \n  type = \"gamma\"\n)\n\n\n\nFigure 1.15: Calibration curve for different values of \\(\\alpha\\), , using local regression (for a single replication\n\n\n\n\n\n\n\n\n\n\n\n1.6.3 Calibration Curve with Moving Average and Confidence Intervals\nInstead of running 200 replications, we could also use the computed confidence interval returned by our local_ci_scores() function.\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\nFirst, we compute the confidence intervals for each value of \\(\\alpha\\).\n\ncalib_curve_alpha_ci &lt;- map(\n  .x = alphas,\n  .f = function(alpha) {\n    linspace_raw &lt;- seq(0, 1, length.out = 100)\n    scores &lt;- data_alphas[[which(alphas == alpha)]]$p_u\n    keep_linspace &lt;- which(\n      linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores)\n    )\n    linspace &lt;- linspace_raw[keep_linspace]\n    map(\n      .x = linspace,\n      .f = ~local_ci_scores(\n        obs = data_alphas[[which(alphas == alpha)]]$d,\n        scores = data_alphas[[which(alphas == alpha)]]$p_u,\n        tau = .x, \n        nn = .15, prob = .5, method = \"probit\")\n    ) |&gt; \n      bind_rows() |&gt; \n      mutate(alpha = alpha)\n  }\n)\n\nThen, we can plot the calibration curve:\n\n\nDisplay the R codes used to create the Figure.\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3, 1,3))\n\nfor (i in 1:length(calib_curve_alpha_ci)) {\n  calib_curve_alpha_ci_curr &lt;- calib_curve_alpha_ci[[i]]\n  alpha &lt;- unique(calib_curve_alpha_ci_curr$alpha)\n  title &lt;- str_c(\"$\\\\alpha = $\", round(alpha, 2))\n  \n  # Histogram\n  ## Calculate the heights for stacking\n  counts_samples_current &lt;- \n    counts_samples |&gt; \n    filter(gamma == 1, alpha == !!alpha)\n  heights &lt;- rbind(\n    counts_samples_current$nb_0_bins, \n    counts_samples_current$nb_1_bins\n  )\n  col_bars &lt;- c(\"#CC79A7\", \"#E69F00\")\n  par(mar = c(0.5, 4.3, 3.0, 0.5))\n  barplot(\n    heights, \n    col = col_bars, \n    border = \"white\", \n    space = 0,\n    xlab = \"\", ylab = \"\", main = latex2exp::TeX(title),\n    axes = FALSE,\n  )\n  \n  par(mar = c(4.1, 4.3, 0.5, 0.5))\n  \n  plot(\n    calib_curve_alpha_ci_curr$xlim, calib_curve_alpha_ci_curr$mean, \n    type = \"l\", main = \"\",\n    xlim = c(0, 1), ylim = c(0, 1),\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\"\n  )\n  col_ic &lt;- ifelse(alpha == 1, wongOrange, wongBlue)\n  polygon(\n    c(calib_curve_alpha_ci_curr$xlim, rev(calib_curve_alpha_ci_curr$xlim)),\n    c(calib_curve_alpha_ci_curr$lower, rev(calib_curve_alpha_ci_curr$upper)),\n    col = adjustcolor(col = col_ic, alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 1.16: Calibration curve for different values of \\(\\alpha\\)\n\n\n\n\n\n\n\nFirst, we compute the confidence intervals for each value of \\(\\gamma\\).\n\ncalib_curve_gamma_ci &lt;- map(\n  .x = gammas,\n  .f = function(gamma) {\n    map(\n      .x = seq(0, 1, length.out = 100),\n      .f = ~local_ci_scores(\n        obs = data_gammas[[which(gammas == gamma)]]$d,\n        scores = data_gammas[[which(gammas == gamma)]]$p_u,\n        tau = .x, \n        nn = .15, prob = .5, method = \"probit\")\n    ) |&gt; \n      bind_rows() |&gt; \n      mutate(gamma = gamma)\n  }\n)\n\nThen, we can plot the calibration curve:\n\n\nDisplay the R codes used to create the Figure.\nmat &lt;- matrix(1:10, nrow = 2)\nlayout(mat, heights = c(1,3, 1,3))\n\nfor (i in 1:length(calib_curve_gamma_ci)) {\n  calib_curve_gamma_ci_curr &lt;- calib_curve_gamma_ci[[i]]\n  gamma &lt;- unique(calib_curve_gamma_ci_curr$gamma)\n  title &lt;- str_c(\"$\\\\gamma = $\", round(gamma, 2))\n  \n  # Histogram\n  ## Calculate the heights for stacking\n  counts_samples_current &lt;- \n    counts_samples |&gt; \n    filter(alpha == 1, gamma == !!gamma)\n  heights &lt;- rbind(\n    counts_samples_current$nb_0_bins, \n    counts_samples_current$nb_1_bins\n  )\n  col_bars &lt;- c(\"#CC79A7\", \"#E69F00\")\n  par(mar = c(0.5, 4.3, 3.0, 0.5))\n  barplot(\n    heights, \n    col = col_bars, \n    border = \"white\", \n    space = 0,\n    xlab = \"\", ylab = \"\", main = latex2exp::TeX(title),\n    axes = FALSE,\n  )\n  \n  par(mar = c(4.1, 4.3, 0.5, 0.5))\n  \n  plot(\n    calib_curve_gamma_ci_curr$xlim, calib_curve_gamma_ci_curr$mean, \n    type = \"l\", main = \"\",\n    xlim = c(0, 1), ylim = c(0, 1),\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\"\n  )\n  col_ic &lt;- ifelse(gamma == 1, wongOrange, wongBlue)\n  polygon(\n    c(calib_curve_gamma_ci_curr$xlim, rev(calib_curve_gamma_ci_curr$xlim)),\n    c(calib_curve_gamma_ci_curr$lower, rev(calib_curve_gamma_ci_curr$upper)),\n    col = adjustcolor(col = col_ic, alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 1.17: Calibration curve for different values of \\(\\gamma\\)"
  },
  {
    "objectID": "recalibration.html#data-generating-process",
    "href": "recalibration.html#data-generating-process",
    "title": "2  Recalibration",
    "section": "2.1 Data Generating Process",
    "text": "2.1 Data Generating Process\nWe use the same DGP as that presented in Section 1.1 in Chapter 1. Let us redefine here the function which simulates data.\n\n#' Simulates data\n#'\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\n#' @param alpha scale parameter for the latent probability (if different \n#'   from 1, the probabilities are transformed and it may induce decalibration)\n#' @param gamma scale parameter for the latent score (if different from 1, \n#'   the probabilities are transformed and it may induce decalibration)\nsim_data &lt;- function(n_obs = 2000, \n                     seed, \n                     alpha = 1, \n                     gamma = 1) {\n  set.seed(seed)\n\n  x1 &lt;- runif(n_obs)\n  x2 &lt;- runif(n_obs)\n  x3 &lt;- runif(n_obs)\n  x4 &lt;- runif(n_obs)\n  epsilon_p &lt;- rnorm(n_obs, mean = 0, sd = .5)\n  \n  # True latent score\n  eta &lt;- -0.1*x1 + 0.05*x2 + 0.2*x3 - 0.05*x4  + epsilon_p\n  # Transformed latent score\n  eta_u &lt;- gamma * eta\n  \n  # True probability\n  p &lt;- (1 / (1 + exp(-eta)))\n  # Transformed probability\n  p_u &lt;- ((1 / (1 + exp(-eta_u))))^alpha\n\n  # Observed event\n  d &lt;- rbinom(n_obs, size = 1, prob = p)\n\n  tibble(\n    # Event Probability\n    p = p,\n    p_u = p_u,\n    # Binary outcome variable\n    d = d,\n    # Variables\n    x1 = x1,\n    x2 = x2,\n    x3 = x3,\n    x4 = x4\n  )\n}"
  },
  {
    "objectID": "recalibration.html#sec-recalibration-methods",
    "href": "recalibration.html#sec-recalibration-methods",
    "title": "2  Recalibration",
    "section": "2.2 Recalibration Methods",
    "text": "2.2 Recalibration Methods\nTo compare different calibration metrics, we will split our dataset into the following sets:\n\na calibration set: to train the recalibrator\na test set: on which we will compute the calibration metrics.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the general case where the scores are obtained using a classifier, the dataset needs to be split into three parts instead of two:\n\na train set: to train the classifier\na calibration set: to train the recalibrator\na test set: on which we will compute the calibration metrics.\n\n\n\nWe define (as in the previous chapter 1) a function to create the splits.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\n#' @param alpha scale parameter for the latent probability (if different \n#'   from 1, the probabilities are transformed and it may induce decalibration)\n#' @param gamma scale parameter for the latent score (if different from 1, \n#'   the probabilities are transformed and it may induce decalibration)\nget_samples &lt;- function(seed,\n                        n_obs = 2000,\n                        alpha = 1,\n                        gamma = 1) {\n  set.seed(seed)\n  data_all &lt;- sim_data(\n    n_obs = n_obs, seed = seed, alpha = alpha, gamma = gamma\n  )\n  \n  # Calibration/test sets----\n  data &lt;- data_all |&gt; select(d, x1:x4)\n  probas &lt;- data_all |&gt; select(p)\n\n  calib_index &lt;- sample(1:nrow(data), size = .6 * nrow(data), replace = FALSE)\n  tb_calib &lt;- data |&gt; slice(calib_index)\n  tb_test &lt;- data |&gt; slice(-calib_index)\n  probas_calib &lt;- probas |&gt; slice(calib_index)\n  probas_test &lt;- probas |&gt; slice(-calib_index)\n\n  list(\n    data_all = data_all,\n    data = data,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    probas_calib = probas_calib,\n    probas_test = probas_test,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs,\n    alpha = alpha,\n    gamma = gamma\n  )\n}\n\nWe simulate a single toy dataset to begin with. Simulations made on replications will be done later.\nLet us consider a case where the probabilities are distorted using \\(\\alpha=.25\\).\n\nn_obs &lt;- 2000\ntoy_data &lt;- get_samples(seed = 1, n_obs = 2000, alpha = .25, gamma = 1)\ntoy_data$data_all\n\n# A tibble: 2,000 × 7\n       p   p_u     d     x1     x2     x3     x4\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.366 0.778     0 0.266  0.872  0.188  0.770 \n 2 0.613 0.885     1 0.372  0.967  0.505  0.690 \n 3 0.561 0.865     1 0.573  0.867  0.0273 0.650 \n 4 0.343 0.765     1 0.908  0.438  0.496  0.0747\n 5 0.293 0.736     0 0.202  0.192  0.947  0.903 \n 6 0.569 0.869     0 0.898  0.0823 0.381  0.133 \n 7 0.345 0.766     0 0.945  0.583  0.698  0.211 \n 8 0.705 0.916     0 0.661  0.0704 0.689  0.155 \n 9 0.726 0.923     1 0.629  0.528  0.478  0.0545\n10 0.673 0.906     1 0.0618 0.472  0.273  0.715 \n# ℹ 1,990 more rows\n\n\nWe extract the calib/test datasets with true probabilities:\n\ndata_all_calib &lt;- toy_data$data_all |&gt;\n    slice(toy_data$calib_index)\ndata_all_calib\n\n# A tibble: 1,200 × 7\n       p   p_u     d     x1      x2    x3     x4\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.670 0.905     0 0.262  0.155   0.818 0.0906\n 2 0.650 0.898     1 0.975  0.683   0.697 0.367 \n 3 0.413 0.802     0 0.229  0.687   0.554 0.734 \n 4 0.750 0.930     1 0.0438 0.0907  0.816 0.0173\n 5 0.304 0.742     0 0.0275 0.591   0.239 0.872 \n 6 0.652 0.899     0 0.753  0.121   0.953 0.704 \n 7 0.309 0.746     0 0.0747 0.922   0.557 0.408 \n 8 0.355 0.772     0 0.914  0.493   0.205 0.175 \n 9 0.555 0.863     0 0.513  0.00726 0.963 0.333 \n10 0.425 0.807     0 0.386  0.802   0.313 0.571 \n# ℹ 1,190 more rows\n\n\n\ndata_all_test &lt;- toy_data$data_all |&gt;\n    slice(-toy_data$calib_index)\ndata_all_test\n\n# A tibble: 800 × 7\n       p   p_u     d    x1     x2      x3      x4\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 0.613 0.885     1 0.372 0.967  0.505   0.690  \n 2 0.498 0.840     1 0.498 0.396  0.566   0.973  \n 3 0.402 0.796     1 0.718 0.106  0.0169  0.970  \n 4 0.302 0.741     1 0.126 0.0102 0.554   0.507  \n 5 0.684 0.909     0 0.382 0.0704 0.869   0.478  \n 6 0.457 0.822     1 0.340 0.413  0.822   0.914  \n 7 0.566 0.867     0 0.600 0.0802 0.983   0.377  \n 8 0.760 0.934     1 0.494 0.277  0.266   0.231  \n 9 0.444 0.816     1 0.827 0.0911 0.191   0.00274\n10 0.637 0.894     1 0.668 0.277  0.00375 0.653  \n# ℹ 790 more rows\n\n\n\n2.2.1 Platt Scaling\nPlatt scaling (Platt et al. 1999) consists of applying logistic regression to \\((d,s(x))\\) where \\(d\\) denotes the binary outcome and \\(s(x)\\) is the vector of predicted scores.\n\n# Logistic regression\nlr &lt;- glm(d ~ p_u, family = binomial(link = 'logit'), data = data_all_calib)\n\nThe predicted values in the calibration set and in the test set:\n\nscore_c_platt_calib &lt;- predict(lr, newdata = data_all_calib, type = \"response\")\nscore_c_platt_test &lt;- predict(lr, newdata = data_all_test, type = \"response\")\n\nLet us create a vector of values to estimate the calibration curve.\n\nlinspace &lt;- seq(0, 1, length.out = 100)\n\nWe can then use the fitted logistic regression to make predictions on this vector of values:\n\nscore_c_platt_linspace &lt;- predict(\n  lr, \n  newdata = tibble(p_u = linspace), \n  type = \"response\"\n)\n\nLet us put these values in a tibble:\n\ntb_scores_c_platt &lt;- tibble(\n  linspace = linspace,\n  p_c = score_c_platt_linspace #recalibrated score\n)\ntb_scores_c_platt\n\n# A tibble: 100 × 2\n   linspace       p_c\n      &lt;dbl&gt;     &lt;dbl&gt;\n 1   0      0.0000729\n 2   0.0101 0.0000817\n 3   0.0202 0.0000917\n 4   0.0303 0.000103 \n 5   0.0404 0.000115 \n 6   0.0505 0.000129 \n 7   0.0606 0.000145 \n 8   0.0707 0.000163 \n 9   0.0808 0.000183 \n10   0.0909 0.000205 \n# ℹ 90 more rows\n\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.1.\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0,1)\n)\nlines(\n  x = tb_scores_c_platt$linspace, y = tb_scores_c_platt$p_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.1: Recalibration Using Platt Scaling\n\n\n\n\n\n\n\n2.2.2 Isotonic Regression\nIsotonic regression is a non parametric approach using the pool-adjacent-violators (PAV) algorithm, introduced by Zadrozny and Elkan (2002). In a nutshell, it assumes that the predicted scores of the initial model (random forest in this notebook) reproduces well the ranks of the observations. Under this assumption, the mapping \\(g(\\cdot)\\) from the scores \\(s(x)\\) into the probabilities \\(g(p)\\) is non-decreasing. It is then possible to use isotonic regression to learn the mapping. The PAV algorithm works as follows:\n\nAt a given iteration: consider the ranked examples \\(x_{i-1}\\) and \\(x_{i}\\).\n\nIf the current values of the function to be learned is such that \\(g(x_{i-1}) \\leq g(x_{i})\\), nothing changes.\nOtherwise, \\(x_1\\) and \\(x_2\\) are called pair-adjacent violators. The values of \\(g(x_{i-1})\\) and \\(g(x_{i})\\) are replaced by their mean \\((g(x_{i-1}) + g(x_{i})) / 2\\). If this move creates earlier violations (\\(g(x_{i-1})\\) might be lower than \\(g(x_{i-2})\\)), a new value is set for \\(g(x_{i-2})\\), \\(g(x_{i-1})\\), and \\(g(x_{i})\\), as the average in the group.\n\n\nLet us compute the isototic least squares regression on the scores \\(p_u\\):\n\niso &lt;- isoreg(x = data_all_calib$p_u, y = data_all_calib$d)\n\nTransforming the fit into a function:\n\nfit_iso &lt;- as.stepfun(iso)\n\nThe predicted values on the calibration set and on the test set:\n\nscore_c_isotonic_calib &lt;- fit_iso(data_all_calib$p_u)\nscore_c_isotonic_test &lt;- fit_iso(data_all_test$p_u)\n\nThen, we can use this function to get estimated probabilities at some specific values (linspace):\n\nscore_c_isotonic_linspace &lt;- fit_iso(linspace)\n\nLet us recreate the tibble with the recalibrated scores:\n\ntb_scores_c_isotonic &lt;- tibble(\n  linspace = linspace,\n  p_c = score_c_isotonic_linspace\n)\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.2.\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = tb_scores_c_isotonic$linspace, y = tb_scores_c_isotonic$p_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.2: Recalibration Using Isotonic Regression\n\n\n\n\n\n\n\n2.2.3 Beta Calibration\nInstead of fitting a logistic regression on the predicted values, as we know that the distribution of the values are bounded to \\([0,1]\\), it is possible to use beta calibration Kull, Silva Filho, and Flach (2017). With this method, instead of assuming that the scores obtained by the classifier are normally distributed (as is the underlying assumption when using Platt scaling), the scores are assumed to follow a Beta distribution. We estimate : \\[\\mu(s;a,b,c) = \\frac{1}{1 + \\frac{1}{e^c \\frac{s^a}{(1-s)^b}}}\\]\n\nlibrary(betacal)\n# Beta calibration using the paper package\nbc &lt;- beta_calibration(\n  p = data_all_calib$p_u, \n  y = data_all_calib$d, \n  parameters = \"abm\" # 3 parameters a, b & m\n)\n\n[1] -126.7104\n[1] 42.94288\n\n\nThe predicted values on the calibration set and on the test set:\n\nscore_c_beta_calib &lt;- beta_predict(p = data_all_calib$p_u, bc)\nscore_c_beta_test &lt;- beta_predict(p = data_all_test$p_u, bc)\n\nWe can then use the beta calibration model to make predictions at the desired values (linspace).\n\nscore_c_beta_linspace &lt;- beta_predict(linspace, bc)\n\nLet us recreate the tibble with the recalibrated scores:\n\ntb_scores_c_beta &lt;- tibble(\n  linspace = linspace,\n  p_c = score_c_beta_linspace\n)\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.3\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = tb_scores_c_beta$linspace, y = tb_scores_c_beta$p_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.3: Recalibration Using Beta Calibration\n\n\n\n\n\n\n\n2.2.4 Local Regression\nLocal regression fits polynomials locally to each bin defined by nn argument of the locfit() function.\n\nlibrary(locfit)\n\nlocfit 1.5-9.8   2023-06-11\n\n\n\nAttaching package: 'locfit'\n\n\nThe following object is masked from 'package:purrr':\n\n    none\n\n\nWe consider three versions here, with different degrees for the polynomials (0, 1, or 2). We set the number of nearest neighbors to use to nn = 0.15, that is, 15%.\n\nDeg 0Deg 1Deg 2\n\n\n\n# Deg 0\nlocfit_0 &lt;- locfit(\n  formula = d ~ lp(p_u, nn = 0.15, deg = 0), \n  kern = \"rect\", maxk = 200, data = data_all_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_0_calib &lt;- predict(locfit_0, newdata = data_all_calib)\nscore_c_locfit_0_test &lt;- predict(locfit_0, newdata = data_all_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n\n\n\n\n# Deg 1\nlocfit_1 &lt;- locfit(\n  formula = d ~ lp(p_u, nn = 0.15, deg = 1), \n  kern = \"rect\", maxk = 200, data = data_all_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_1_calib &lt;- predict(locfit_1, newdata = data_all_calib)\nscore_c_locfit_1_test &lt;- predict(locfit_1, newdata = data_all_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_1_linspace &lt;- predict(locfit_1, newdata = linspace)\n\n\n\n\n# Deg 2\nlocfit_2 &lt;- locfit(\n  formula = d ~ lp(p_u, nn = 0.15, deg = 2), \n  kern = \"rect\", maxk = 200, data = data_all_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_2_calib &lt;- predict(locfit_2, newdata = data_all_calib)\nscore_c_locfit_2_test &lt;- predict(locfit_2, newdata = data_all_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_2_linspace &lt;- predict(locfit_2, newdata = linspace)\n\n\n\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.4, Figure 5.5, and Figure 5.4\n\nDeg 0Deg 1Deg 2\n\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_0_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.4: Recalibration Using Local Regression\n\n\n\n\n\n\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_1_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.5: Recalibration Using Local Regression\n\n\n\n\n\n\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_all_calib$p_u, data_all_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_2_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 2.6: Recalibration Using Local Regression"
  },
  {
    "objectID": "recalibration.html#simulations-setup",
    "href": "recalibration.html#simulations-setup",
    "title": "2  Recalibration",
    "section": "2.3 Simulations: setup",
    "text": "2.3 Simulations: setup\nLet us now consider multiple scenarios in which the scores are distorded by varying either \\(\\alpha\\) or \\(\\gamma\\) (see Section 1.1 in Chapter 1).\nFor each value of \\(\\alpha\\) and \\(\\gamma\\), we will generate 200 replications of the whole process consisting in the following steps:\n\ngenerate data from the PGD and transform true probabilities \\(p\\) accordingly with either \\(\\alpha\\) or \\(\\gamma\\) to obtain \\(p^u\\)\nsplit the data into two sets: calibration and test set\napply each recalibration method on the calibration set 4.compute calibration metrics on both sets (for comparison) using either:\n\n\nthe recalibrated scores \\(p^c := g(s(x))\\)\nthe estimated scores \\(p^u := s(x)\\)\nthe true probabilities \\(p\\).\n\n\n2.3.1 Helper Functions\nWe (re)define a helper function to compute standard metrics (see Section 1.4 in Chapter 1):\n\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\ncompute_gof &lt;- function(true_prob,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  mse &lt;- mean((true_prob - pred)^2)\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR\n  )\n}\n\nWe (re)define a few helper functions to compute calibration metrics (see Section 1.2 in Chapter 1):\n\nbrier_score() to compute Brier Score (see Section 1.2.1.2 in Chapter 1).\n\n\n\nDisplay the functions used to compute Brier Score\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n\n\ne_calib_error() to compute the Expected Calibration Error (see Section 1.2.1.3 in Chapter 1). This function relies on get_summary_bins() which computes summary statistics for binomial observed data and predicted scores returned by a model.\n\n\n\nDisplay the functions used to compute the ECE\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n\n\nqmse_error() to compute Quantile-based MSE (see Section 1.2.1.4 in Chapter 1). This function also relies on get_summary_bins().\n\n\n\nDisplay the functions used to compute the QMSE\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n\n\nwmse_error() to compute Weighted MSE (see Section 1.2.1.5 in Chapter 1). This function relies on local_ci_scores() which identifies the nearest neighbors of a certain predicted score and then calculates the mean scores in that neighborhood accompanied with its confidence interval.\n\n\n\nDisplay the functions used to compute the WMSE\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @param prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n}\n\n\n\nlocal_calib_score() to compute Local Calibration Score (see Section 1.2.1.6 in Chapter 1).\n\n\n\nDisplay the functions used to compute the LCS\n#' Calibration score using Local Regression\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\nlocal_calib_score &lt;- function(obs, \n                              scores) {\n  \n  # Add a little noise to the scores, to avoir crashing R\n  scores &lt;- scores + rnorm(length(scores), 0, .001)\n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(\n      d = obs,\n      scores = scores\n    )\n  )\n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n  locfit_0_linspace[locfit_0_linspace &gt; 1] &lt;- 1\n  locfit_0_linspace[locfit_0_linspace &lt; 0] &lt;- 0\n  \n  # Squared difference between predicted value and the bissector, weighted by the density of values\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(linspace_raw)\n  )\n  # The weights\n  weights &lt;- dens$y[keep_linspace]\n  \n  weighted.mean((linspace - locfit_0_linspace)^2, weights)\n}\n\n\nThen, we define the recalibrate() function which recalibrate a model using the observed events \\(d\\), the predicted associated probabilities \\(p^u\\) and a given recalibration technique (as presented above in Section 2.2).\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' #' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression, `\"beta\"` for beta calibration, \n#'   `\"locfit\"` for local regression)\n#' @param iso_params list of named parameters to use in the local regression \n#'   (`nn` for fraction of nearest neighbors to use, `deg` for degree)\n#' @param linspace vector of alues at which to compute the recalibrated scores\n#' @returns list of three elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set, and recalibrated scores on a segment \n#'   of values\nrecalibrate &lt;- function(obs_calib,\n                        scores_calib,\n                        obs_test,\n                        scores_test,\n                        method = c(\"platt\", \"isotonic\", \"beta\", \"locfit\"),\n                        iso_params = NULL,\n                        linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 100)\n  \n  data_calib &lt;- tibble(d = obs_calib, p_u = scores_calib)\n  data_test &lt;- tibble(d = obs_test, p_u = scores_test)\n  \n  if (method == \"platt\") {\n    lr &lt;- glm(d ~ p_u, family = binomial(link = 'logit'), data = data_calib)\n    # Recalibrated scores on calibration and test set\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n    # Recalibrated values along a segment\n    score_c_linspace &lt;- predict(\n      lr, \n      newdata = tibble(p_u = linspace), \n      type = \"response\"\n    )\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$p_u, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    # Recalibrated scores on calibration and test set\n    score_c_calib &lt;- fit_iso(data_calib$p_u)\n    score_c_test &lt;- fit_iso(data_test$p_u)\n    # Recalibrated values along a segment\n    score_c_linspace &lt;- fit_iso(linspace)\n  } else if (method == \"beta\") {\n    capture.output({\n      bc &lt;- beta_calibration(\n        p = data_calib$p_u, \n        y = data_calib$d, \n        parameters = \"abm\" # 3 parameters a, b & m\n      )\n    })\n    # Recalibrated scores on calibration and test set\n    score_c_calib &lt;- beta_predict(p = data_calib$p_u, bc)\n    score_c_test &lt;- beta_predict(p = data_test$p_u, bc)\n    # Recalibrated values along a segment\n    score_c_linspace &lt;- beta_predict(linspace, bc)\n  } else if (method == \"locfit\") {\n    # Deg 0\n    locfit_reg &lt;- locfit(\n      formula = d ~ lp(p_u, nn = iso_params$nn, deg = iso_params$deg), \n      kern = \"rect\", maxk = 200, data = data_calib\n    )\n    # Recalibrated scores on calibration and test set\n    score_c_calib &lt;- predict(locfit_reg, newdata = data_calib)\n    score_c_calib[score_c_calib &lt; 0] &lt;- 0\n    score_c_calib[score_c_calib &gt; 1] &lt;- 1\n    \n    score_c_test &lt;- predict(locfit_reg, newdata = data_test)\n    score_c_test[score_c_test &lt; 0] &lt;- 0\n    score_c_test[score_c_test &gt; 1] &lt;- 1\n    \n    # Recalibrated values along a segment\n    score_c_linspace &lt;- predict(locfit_reg, newdata = linspace)\n    score_c_linspace[score_c_linspace &lt; 0] &lt;- 0\n    score_c_linspace[score_c_linspace &gt; 1] &lt;- 1\n  } else {\n    stop(str_c(\n      'Wrong method. Use one of the following:',\n      '\"platt\", \"isotonic\", \"beta\", \"locfit\"'\n    ))\n  }\n  \n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = scores_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = scores_test,\n    p_c = score_c_test\n  )\n  # For linear space\n  tb_score_c_linspace &lt;- tibble(\n    linspace = linspace,\n    p_c = score_c_linspace\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test,\n    tb_score_c_linspace = tb_score_c_linspace\n  )\n}\n\nLet us define a function that computes the different calibration metrics for a single replication of the simulations.\n\n#' Computes the calibration metrics for a set of observed and predicted \n#' probabilities\n#' \n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#' @param linspace vector of values at which to compute the WMSE\ncompute_metrics &lt;- function(obs, \n                            scores, \n                            true_probas,\n                            linspace) {\n  mse &lt;- mean((true_probas - scores)^2)\n  brier &lt;- brier_score(obs = obs, scores = scores)\n  if (length(unique(scores)) &gt; 1) {\n    ece &lt;- e_calib_error(obs = obs, scores = scores, k = 10, threshold = .5)\n    qmse &lt;- qmse_error(obs = obs, scores = scores, k = 10, threshold = .5)\n  } else {\n    ece &lt;- NA\n    qmse &lt;- NA\n  }\n  \n  expected_events &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs, \n      scores = scores,\n      tau = .x, nn = .15, prob = .95, method = \"probit\")\n  ) |&gt; \n    bind_rows()\n  wmse &lt;- weighted_mse(local_scores = expected_events, scores = scores)\n  lcs &lt;- local_calib_score(obs = obs, scores = scores)\n  \n  tibble(\n    mse = mse, brier = brier, ece = ece, qmse = qmse, wmse = wmse, lcs = lcs\n  )\n  \n}\n\nLastly, we define the f_simul() function to perform one simulation.\n\n#' Performs one replication for a simulation\n#' \n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\n#' @param linspace values at which to compute the mean observed event when computing the WMSE\nf_simul &lt;- function(i, \n                    grid, \n                    n_obs, \n                    type = c(\"alpha\", \"gamma\"),\n                    linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 100)\n  \n  ## 1. Generate Data----\n  current_seed &lt;- grid$seed[i]\n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  ## 2. Calibration/Test sets----\n  # Datasets with true probabilities\n  data_all_calib &lt;- current_data$data_all |&gt;\n    slice(current_data$calib_index)\n  \n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  ## 3. Recalibration----\n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n  res_recalibration &lt;- map2(\n    .x = methods,\n    .y = params,\n    .f = ~recalibrate(\n      obs_calib = data_all_calib$d, \n      scores_calib = data_all_calib$p_u, \n      obs_test = data_all_test$d, \n      scores_test = data_all_test$p_u,\n      method = .x,\n      iso_params = .y,\n      linspace = linspace\n    )\n  )\n  names(res_recalibration) &lt;- method_names\n  \n  ## 4. Calibration metrics----\n  \n  ### Using True Probabilities\n  #### Calibration Set\n  calib_metrics_true_calib &lt;- compute_metrics(\n    obs = data_all_calib$d, \n    scores = data_all_calib$p, \n    true_probas = data_all_calib$p,\n    linspace = linspace) |&gt; \n    mutate(method = \"True Prob.\", sample = \"Calibration\")\n  #### Test Set\n  calib_metrics_true_test &lt;- compute_metrics(\n    obs = data_all_test$d, \n    scores = data_all_test$p, \n    true_probas = data_all_test$p,\n    linspace = linspace) |&gt; \n    mutate(method = \"True Prob.\", sample = \"Test\")\n  \n  ### Without Recalibration\n  #### Calibration Set\n  calib_metrics_without_calib &lt;- compute_metrics(\n    obs = data_all_calib$d, \n    scores = data_all_calib$p_u, \n    true_probas = data_all_calib$p,\n    linspace = linspace) |&gt; \n    mutate(method = \"No Calibration\", sample = \"Calibration\")\n  #### Test Set\n  calib_metrics_without_test &lt;- compute_metrics(\n    obs = data_all_test$d, \n    scores = data_all_test$p_u, \n    true_probas = data_all_test$p,\n    linspace = linspace) |&gt; \n    mutate(method = \"No Calibration\", sample = \"Test\")\n  \n  calib_metrics &lt;- \n    calib_metrics_true_calib |&gt; \n    bind_rows(calib_metrics_true_test) |&gt; \n    bind_rows(calib_metrics_without_calib) |&gt; \n    bind_rows(calib_metrics_without_test)\n  \n  ### With Recalibration: loop on methods\n  for (method in method_names) {\n    res_recalibration_current &lt;- res_recalibration[[method]]\n    #### Calibration Set\n    calib_metrics_without_calib &lt;- compute_metrics(\n      obs = data_all_calib$d, \n      scores = res_recalibration_current$tb_score_c_calib$p_c, \n      true_probas = data_all_calib$p,\n      linspace = linspace) |&gt; \n      mutate(method = method, sample = \"Calibration\")\n    #### Test Set\n    calib_metrics_without_test &lt;- compute_metrics(\n      obs = data_all_test$d, \n      scores = res_recalibration_current$tb_score_c_test$p_c, \n      true_probas = data_all_test$p,\n      linspace = linspace) |&gt; \n      mutate(method = method, sample = \"Test\")\n    \n    calib_metrics &lt;- \n      calib_metrics |&gt; \n      bind_rows(calib_metrics_without_calib) |&gt; \n      bind_rows(calib_metrics_without_test)\n  }\n  \n  calib_metrics &lt;- \n    calib_metrics |&gt; \n    mutate(\n      seed = current_seed,\n      transform_scale = transform_scale,\n      type = type\n    )\n  \n  list(\n    res_recalibration = res_recalibration,\n    linspace = linspace,\n    calib_metrics = calib_metrics,\n    data_all_calib = data_all_calib,\n    data_all_test = data_all_test,\n    seed = current_seed\n  )\n}"
  },
  {
    "objectID": "recalibration.html#running-the-simulations",
    "href": "recalibration.html#running-the-simulations",
    "title": "2  Recalibration",
    "section": "2.4 Running the Simulations",
    "text": "2.4 Running the Simulations\nLet us now run the simulations. We consider the following values for \\(\\alpha\\) and \\(\\gamma\\):\n\nalphas &lt;- gammas &lt;- c(1/3, 2/3, 1, 3/2, 3)\n\nFor each value of \\(\\alpha\\), and then for each value of \\(\\gamma\\), let us make 200 replication samples from the same DGP.\n\nn_repl &lt;- 200 # number of replications\nn_obs &lt;- 2000 # number of observations to draw\ngrid_alpha &lt;- expand_grid(alpha = alphas, seed = 1:n_repl)\ngrid_gamma &lt;- expand_grid(gamma = gammas, seed = 1:n_repl)\n\nWe perform the simulations for the varying values of \\(\\alpha\\)\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n  simul_recalib_alpha &lt;- furrr::future_map(\n    .x = 1:nrow(grid_alpha),\n    .f = ~{\n      p()\n      f_simul(\n        i = .x, \n        grid = grid_alpha, \n        n_obs = n_obs, \n        type = \"alpha\", \n        linspace = NULL)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nAnd we do the same for varying values of \\(\\gamma\\):\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n  simul_recalib_gamma &lt;- furrr::future_map(\n    .x = 1:nrow(grid_gamma),\n    .f = ~{\n      p()\n      f_simul(\n        i = .x, \n        grid = grid_gamma, \n        n_obs = n_obs, \n        type = \"gamma\", \n        linspace = NULL)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})"
  },
  {
    "objectID": "recalibration.html#standard-metrics-on-simulations",
    "href": "recalibration.html#standard-metrics-on-simulations",
    "title": "2  Recalibration",
    "section": "2.5 Standard Metrics on Simulations",
    "text": "2.5 Standard Metrics on Simulations\nWe (re)define the function compute_gof_simul() to apply compute_gof(), defined above, to compute the different standard performance metrics on recalibrated probabilities (see Section 1.4 in Chapter 1), to which, initially, we have applied transformations:\n\n#' Computes goodness of fit metrics for a replication\n#'\n#' @param i row number of the grid to use for the simulation\n#' @param grid grid tibble with the seed number (column `seed`) and the deformations value (either `alpha` or `gamma`)\n#' @param n_obs desired number of observation\n#' @param type deformation probability type (either `alpha` or `gamma`); the \n#' name should match with the `grid` tibble\ncompute_gof_simul &lt;- function(i,\n                              grid,\n                              n_obs,\n                              type = c(\"alpha\", \"gamma\")) {\n  current_seed &lt;- grid$seed[i]\n  if (type == \"alpha\") {\n    transform_scale &lt;- grid$alpha[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = transform_scale, gamma = 1\n    )\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid$gamma[i]\n    current_data &lt;- get_samples(\n      seed = current_seed, n_obs = n_obs, alpha = 1, gamma = transform_scale\n    )\n  } else {\n    stop(\"Transform type should be either alpha or gamma.\")\n  }\n  \n  \n  # Get the calib/test datasets with true probabilities\n  data_all_calib &lt;- current_data$data_all |&gt;\n    slice(current_data$calib_index)\n  \n  data_all_test &lt;- current_data$data_all |&gt;\n    slice(-current_data$calib_index)\n  \n  # Calibration set\n  true_prob_calib &lt;- data_all_calib$p_u\n  obs_calib &lt;- data_all_calib$d\n  pred_calib &lt;- data_all_calib$p\n  \n  # Test set\n  true_prob_test &lt;- data_all_test$p_u\n  obs_test &lt;- data_all_test$d\n  pred_test &lt;- data_all_test$p\n  \n  # Recalibration\n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n  res_recalibration &lt;- map2(\n    .x = methods,\n    .y = params,\n    .f = ~recalibrate(\n      obs_calib = data_all_calib$d, \n      scores_calib = data_all_calib$p_u, \n      obs_test = data_all_test$d, \n      scores_test = data_all_test$p_u,\n      method = .x,\n      iso_params = .y,\n      linspace = NULL\n    )\n  )\n  names(res_recalibration) &lt;- method_names\n  \n  # Initialisation\n  gof_metrics_simul_calib &lt;- tibble()\n  gof_metrics_simul_test &lt;- tibble()\n  \n  # Calculate standard metrics\n  ## With Recalibration: loop on methods\n  for (method in method_names) {\n    res_recalibration_current &lt;- res_recalibration[[method]]\n    ### Computation of metrics on the calibration set\n    metrics_simul_calib &lt;- map(\n    .x = seq(0, 1, by = .01), # we vary the probability threshold\n    .f = ~compute_gof(\n      true_prob = true_prob_calib,\n      obs = obs_calib,\n      #### the predictions are now recalibrated:\n      pred = res_recalibration_current$tb_score_c_calib$p_c,\n      threshold = .x\n      )\n    ) |&gt;\n      list_rbind()\n    \n    ### Computation of metricson the test set\n    metrics_simul_test &lt;- map(\n    .x = seq(0, 1, by = .01), # we vary the probability threshold\n    .f = ~compute_gof(\n      true_prob = true_prob_test,\n      obs = obs_test,\n      #### the predictions are now recalibrated:\n      pred = res_recalibration_current$tb_score_c_test$p_c,\n      threshold = .x\n      )\n    ) |&gt;\n      list_rbind()\n    \n    roc_calib &lt;- pROC::roc(\n      obs_calib, \n      res_recalibration_current$tb_score_c_calib$p_c\n    )\n    auc_calib &lt;- as.numeric(pROC::auc(roc_calib))\n    \n    roc_test &lt;- pROC::roc(\n      obs_test, \n      res_recalibration_current$tb_score_c_test$p_c\n    )\n    auc_test &lt;- as.numeric(pROC::auc(roc_test))\n    \n    metrics_simul_calib &lt;- metrics_simul_calib |&gt;\n      mutate(\n        auc = auc_calib,\n        seed = current_seed,\n        scale_parameter = transform_scale,\n        type = type,\n        method = method,\n        sample = \"calibration\"\n      )\n    \n    metrics_simul_test &lt;- metrics_simul_test |&gt;\n      mutate(\n        auc = auc_test,\n        seed = current_seed,\n        scale_parameter = transform_scale,\n        type = type,\n        method = method,\n        sample = \"test\"\n      )\n    \n    gof_metrics_simul_calib &lt;- gof_metrics_simul_calib |&gt;\n      bind_rows(metrics_simul_calib)\n    gof_metrics_simul_test &lt;- gof_metrics_simul_test |&gt;\n      bind_rows(metrics_simul_test)\n  }\n  \n  gof_metrics_simul_calib |&gt; \n    bind_rows(gof_metrics_simul_test)\n}\n\nLet us apply the function compute_gof_simul to the different simulations. We begin with the recalibrated probabilities initially transformed according to the variation of the parameter \\(\\alpha\\).\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n  recalib_metrics_alpha &lt;- furrr::future_map(\n    .x = 1:nrow(grid_alpha),\n    .f = ~{\n      p()\n      compute_gof_simul(\n        i = .x, \n        grid = grid_alpha, \n        n_obs = n_obs, \n        type = \"alpha\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nrecalib_metrics_alpha &lt;- list_rbind(recalib_metrics_alpha)\n\nWe do the same for \\(\\gamma\\):\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n  recalib_metrics_gamma &lt;- furrr::future_map(\n    .x = 1:nrow(grid_gamma),\n    .f = ~{\n      p()\n      compute_gof_simul(\n        i = .x, \n        grid = grid_gamma, \n        n_obs = n_obs, \n        type = \"gamma\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nrecalib_metrics_gamma &lt;- list_rbind(recalib_metrics_gamma)\n\nWe (re)define function boxplot_simuls_metrics() from Section 1.4 (Chapter 1) to plot the standard metrics results on the recalibrated simulations. This function will produce a panel of boxplots. Each row of the panel will correspond to a metric whereas each column will correspond to a value for either \\(\\alpha\\) or \\(\\gamma\\). We also have one column for each recalibration method used. On each figure, the x-axis will correspond to the value used for the probability threshold \\(\\tau\\), and the y-axis will correspond to the values of the metric.\n\n#' Boxplots for the simulations to visualize the distribution of some \n#' traditional metrics as a function of the probability threshold.\n#' And, ROC curves\n#' The resulting figure is a panel of graphs, with vayring values for the \n#' transformation applied to the probabilities (in columns) and different \n#' metrics (in rows).\n#' \n#' @param tb_metrics tibble with computed metrics for the simulations\n#' @param type type of transformation: `\"alpha\"` or `\"gamma\"`\n#' @param metrics names of the metrics computed\nboxplot_simuls_metrics &lt;- function(tb_metrics,\n                                   type = c(\"alpha\", \"gamma\"),\n                                   metrics) {\n  scale_parameters &lt;- unique(tb_metrics$scale_parameter)\n  \n  par(mfrow = c(length(metrics), length(scale_parameters)))\n  for (i_metric in 1:length(metrics)) {\n    metric &lt;- metrics[i_metric]\n    for (i_scale_parameter in 1:length(scale_parameters)) {\n      scale_parameter &lt;- scale_parameters[i_scale_parameter]\n      \n      tb_metrics_current &lt;- tb_metrics |&gt; \n        filter(scale_parameter == !!scale_parameter)\n      \n      if (metric == \"roc\") {\n        seeds &lt;- unique(tb_metrics_current$seed)\n        if (i_metric == 1) {\n          # first row\n          title &lt;- latex2exp::TeX(\n            str_c(\"$\\\\\", type, \" = \", round(scale_parameter, 2), \"$\")\n          )\n          size_top &lt;- 2.1\n        } else if (i_metric == length(metrics)) {\n          # Last row\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        } else {\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        }\n        \n        if (i_scale_parameter == 1) {\n          # first column\n          y_lab &lt;- str_c(metric, \"\\n True Positive Rate\") \n          size_left &lt;- 5.1\n        } else {\n          y_lab &lt;- \"\"\n          size_left &lt;- 4.1\n        }\n        \n        par(mar = c(4.5, size_left, size_top, 2.1))\n        plot(\n          0:1, 0:1,\n          type = \"l\", col = NULL,\n          xlim = 0:1, ylim = 0:1,\n          xlab = \"False Positive Rate\", \n          ylab = y_lab,\n          main = \"\"\n        )\n        for (i_seed in 1:length(seeds)) {\n          tb_metrics_current_seed &lt;- \n            tb_metrics_current |&gt; \n            filter(seed == seeds[i_seed])\n          lines(\n            x = tb_metrics_current_seed$FPR, \n            y = tb_metrics_current_seed$sensitivity,\n            lwd = 2, col = adjustcolor(\"black\", alpha.f = .04)\n          )\n        }\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n      } else {\n        # not ROC\n        tb_metrics_current &lt;- \n          tb_metrics_current |&gt; \n          filter(threshold %in% seq(0, 1, by = .1))\n        form &lt;- str_c(metric, \"~threshold\")\n        if (i_metric == 1) {\n          # first row\n          title &lt;- latex2exp::TeX(\n            str_c(\"$\\\\\", type, \" = \", round(scale_parameter, 2), \"$\")\n          )\n          size_top &lt;- 2.1\n        } else if (i_metric == length(metrics)) {\n          # Last row\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        } else {\n          title &lt;- \"\"\n          size_top &lt;- 1.1\n        }\n        \n        if (i_scale_parameter == 1) {\n          # first column\n          y_lab &lt;- metric\n        } else {\n          y_lab &lt;- \"\"\n        }\n        \n        par(mar = c(4.5, 4.1, size_top, 2.1))\n        boxplot(\n          formula(form), data = tb_metrics_current,\n          xlab = \"Threshold\", ylab = y_lab,\n          main = title\n        )\n      }\n    }\n  }\n}\n\nWe aim to create a set of boxplots to visually assess the influence of probability transformations using \\(\\alpha\\) or \\(\\gamma\\) on standard metrics. Whenever \\(\\alpha \\neq 1\\) or \\(\\gamma \\neq 1\\), the resulting scores \\(p^c\\) represent values akin to those obtained from an initially uncalibrated model, with recalibration method applied. We want to verify that the recalibration methods applied to the uncalibrated probabilities do not degrade performance, as assessed by standard metrics. The results are shown in Figure 1.6 for vayring values of \\(\\alpha\\), and in Figure 1.7 for vayring values of \\(\\gamma\\).\n\n\n\n\n\n\nNote\n\n\n\nWhen using monotone transformation methods such as isotonic regression, the AUC cannot be degraded as it is insensitive to the application of an increasing function to the predicted scores by a model. Isotonic regression assumes that the initial model, without recalibration, has an AUC of 1. Therefore, if the initial model requires decreasing transformations in the recalibration step, isotonic regression will not be effective.\n\n\n\nmetrics &lt;- c(\"mse\", \"accuracy\", \"sensitivity\", \"specificity\", \"roc\", \"auc\")\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"platt\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.7: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using Platt-scaling.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"isotonic\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.8: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using Isotonic regression.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"beta\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.9: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using Beta calibration.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"locfit_0\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.10: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using local regression (with deg=0).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"locfit_1\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.11: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using local regression (with deg=1).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_alpha |&gt; filter(method == \"locfit_2\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"alpha\", metrics = metrics\n)\n\n\n\n\nFigure 2.12: Calibration transformations made by varying \\(\\alpha\\) and impact on standard metrics. The model is calibrated when \\(\\alpha=1\\). The scores have been recalibrated using local regression (with deg=2).\n\n\n\n\n\n\n\n\n\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"platt\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.13: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). The scores have been recalibrated using Platt-scaling.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"isotonic\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.14: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). The scores have been recalibrated using Isotonic regression.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"beta\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.15: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). The scores have been recalibrated using Beta calibration.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"locfit_0\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.16: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). he scores have been recalibrated using local regression (with deg=0).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"locfit_1\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.17: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). he scores have been recalibrated using local regression (with deg=1).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\ncurrent_recalib_metrics &lt;- recalib_metrics_gamma |&gt; filter(method == \"locfit_2\")\nboxplot_simuls_metrics(\n  tb_metrics = current_recalib_metrics, \n  type = \"gamma\", metrics = metrics\n)\n\n\n\n\nFigure 2.18: Calibration transformations made by varying \\(\\gamma\\) and impact on standard metrics. The model is calibrated when \\(\\beta=1\\). he scores have been recalibrated using local regression (with deg=0).\n\n\n\n\n\n\n\n\n\n\n\nWe can focus on the transformations that have degraded performance. For that purpose, we load the standard metrics computed on the uncalibrated probabilities:"
  },
  {
    "objectID": "recalibration.html#calibration-maps-single-replication",
    "href": "recalibration.html#calibration-maps-single-replication",
    "title": "2  Recalibration",
    "section": "2.6 Calibration Maps (single replication)",
    "text": "2.6 Calibration Maps (single replication)\nWe can visualize the calibration curve as in Section 1.6 (Chapter 1).\n\n2.6.1 Quantile-Based Bins\nWe first visualize calibration in a fashion similar to what is done with the calibration_curve() method from sci-kit learn.\nThe x-axis of the calibration plot reports the mean predicted probabilities computed on different bins, where the bins are defined using the deciles of the predicted scores. On the y-axis, the corresponding fraction of positive events (\\(d=1\\)) are reported.\nWe can visualize the calibration curve as in Section 1.6 (Chapter 1).\nWe can accompany the predictions made for each bin with a confidence interval, using the binom.confint() function from {binom}.\n\nlibrary(binom)\n\n\n#' Confidence interval for binomial data, using quantile-defined bins\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of bins to create (quantiles, default to `10`)\n#' @param prob confidence interval level\n#' @param method Which method to use to construct the interval. Any combination \n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\", \n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with the following columns, where each row corresponds to\n#'   a bin:\n#' - `mean`: estimation of $E(d | s(x) = p)$ where $p$ is the average score in bin b\n#' - `lower`: lower bound of the confidence interval\n#' - `upper`: upper bound of the confidence interval\n#' - `prediction`: average of `s(x)` in bin b\n#' - `score_class`: decile level of bin b\n#' - `nb`: number of observation in bin b\nci_scores_bins &lt;- function(obs,\n                           scores,\n                           k,\n                           prob = .95, \n                           method = \"probit\" ) {\n  \n  summary_bins_calib &lt;- get_summary_bins(obs = obs, scores = scores, k = k)\n  \n  new_k &lt;- nrow(summary_bins_calib)\n  prob_ic &lt;- tibble(\n    mean = rep(NA, new_k),\n    lower = rep(NA, new_k),\n    upper = rep(NA, new_k),\n    prediction = summary_bins_calib |&gt; pull(\"mean_score\"),\n    score_class = summary_bins_calib$score_class,\n    nb = summary_bins_calib$nb\n  )\n  for (i in 1:new_k) {\n    prob_ic[i, 1:3] &lt;- binom.confint(\n      x = summary_bins_calib$sum_obs[i],\n      n = summary_bins_calib$nb[i], \n      conf.level = prob,\n      methods = method\n    )[, c(\"mean\", \"lower\", \"upper\")]\n  }\n  \n  prob_ic\n}\n\nLet us define here a function to compute the confidence intervals for a single replication of our simulations.\n\n#' Compute confidence intervals for the calibration values of a single \n#' replication, for a given method\n#' \n#' @param simul single simulation obtained with `f_simul()`\n#' @param method recalibration method\nconf_int_qbins_simul &lt;- function(simul, method) {\n  \n  obs_calib &lt;- simul$data_all_calib$d\n  obs_test &lt;- simul$data_all_test$d\n  \n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_c\n    scores_test &lt;- tb_score_c_test$p_c\n  }\n  \n  e_scores_bins_calib &lt;- ci_scores_bins(\n    obs = obs_calib, \n    scores = scores_calib,\n    k = 10, prob = .95, method = \"probit\"\n  )\n  e_scores_bins_test &lt;- ci_scores_bins(\n    obs = obs_test, \n    scores = scores_test,\n    k = 10, prob = .95, method = \"probit\"\n  )\n  \n  e_scores_bins &lt;- e_scores_bins_calib |&gt; \n    mutate(sample = \"Calibration\") |&gt; \n    bind_rows(\n      e_scores_bins_test |&gt; mutate(sample = \"Test\")\n    ) |&gt; \n    mutate(\n      seed = simul$seed, \n      method = method\n    )\n  \n  list(\n    e_scores_bins = e_scores_bins,\n    obs_calib = obs_calib,\n    obs_test = obs_test,\n    scores_calib = scores_calib,\n    scores_test = scores_test,\n    seed = simul$seed,\n    method = method\n  )\n}\n\nWe define a function, get_data_plot_quant_simul() to extract a desired simulation from our results (either from simul_recalib_alpha or from simul_recalib_gamma). The function get_data_plot_quant_simul() returns a list with two elements:\n\nci_res: the confidence interval for the calibration curve for the simulation\nn_bins_scores: the counts of observation in each bin defined over [0,1] for the scores (uncalibrated or calibrated, for both the calibration set and the test set).\n\n\n#' @param i index of the simulation to use (in `simul_recalib_alpha` or \n#'   `simul_recalib_gamma`)\n#' @param type type of transformed probabilities (made on `alpha` or `gamma`)\n#' @param method name of the recalibration method to focus on\nget_data_plot_quant_simul &lt;- function(i, type, method) {\n  if (type == \"alpha\") {\n    simul &lt;- simul_recalib_alpha[[i]]\n    transform_scale &lt;- grid_alpha$alpha[i]\n  } else if (type == \"gamma\") {\n    simul &lt;- simul_recalib_gamma[[i]]\n    transform_scale &lt;- grid_gamma$gamma[i]\n  } else {\n    stop(\"Wrong value for argument `type`.\")\n  }\n  \n  # Counting number of obs in bins defined over [0,1]\n  breaks &lt;- seq(0, 1, by = .05)\n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_u\n    scores_test &lt;- tb_score_c_test$p_u\n    scores_c_calib &lt;- tb_score_c_calib$p_c\n    scores_c_test &lt;- tb_score_c_test$p_c\n  }\n  \n  n_bins_calib &lt;- table(cut(scores_calib, breaks = breaks))\n  n_bins_test &lt;- table(cut(scores_test, breaks = breaks))\n  if (!is.null(scores_c_calib)) {\n    n_bins_c_calib &lt;- table(cut(scores_c_calib, breaks = breaks))\n  } else {\n    n_bins_c_calib &lt;- NA_integer_\n  }\n  if (!is.null(scores_c_test)) {\n    n_bins_c_test &lt;- table(cut(scores_c_test, breaks = breaks))\n  } else {\n    n_bins_c_test &lt;- NA_integer_\n  }\n\n  n_bins_scores &lt;- tibble(\n    bins = names(table(cut(breaks, breaks = breaks))),\n    n_bins_calib = as.vector(n_bins_calib),\n    n_bins_test = as.vector(n_bins_test),\n    n_bins_c_calib = as.vector(n_bins_c_calib),\n    n_bins_c_test = as.vector(n_bins_c_test),\n    method = method,\n    seed = simul$seed,\n    type = type\n  )\n  \n  # Confidence intervals\n  ci_res &lt;- conf_int_qbins_simul(simul = simul, method = method)\n  list(ci_res = ci_res, n_bins_scores = n_bins_scores)\n}\n\nNow, we can define a function that will plot the calibration maps computed on the calibration set and those computed on the test set. This function will plot a panel of calibration maps, each row corresponding to a specific value of the scale used to transform the probabilities (\\(\\alpha\\) or \\(\\gamma\\)). On top of each graph, we plot the histogram of uncalibrated scores and of calibrated scores.\n\nplot_conf_int_qbins_simul &lt;- function(method, type) {\n  current_grid &lt;- grid |&gt; \n    filter(method == !!method, type == !!type)\n  \n  if (type == \"alpha\") {\n    transform_scale &lt;- grid_alpha |&gt; slice(current_grid$i) |&gt; pull(alpha)\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid_gamma |&gt; slice(current_grid$i) |&gt; pull(gamma)\n  } else {\n    stop(\"Error argument `type`. Wrong value: either \\\"alpha\\\" or \\\"gamma\\\"\")\n  }\n  \n  data_plots &lt;-  map(\n    .x = current_grid$i,\n    .f = ~get_data_plot_quant_simul(i = .x, type = type, method = method)\n  )\n  data_plots_ci &lt;- map(data_plots, pluck(\"ci_res\"))\n  data_plots_n_bins_scores &lt;- map(data_plots, pluck(\"n_bins_scores\"))\n  \n  \n  nb &lt;- length(data_plots)\n  mat &lt;- mat_init &lt;- matrix(c(1:4), ncol = 2)\n  for (j in 1:(nb-1)) {\n    mat &lt;- rbind(mat, mat_init + j * 4)\n  }\n  layout(mat, heights = rep(c(1, 3), nb))\n  \n  y_lim &lt;- c(0, 1)\n  \n  \n  # i_recalib &lt;- 1\n  for (i_recalib in 1:nb) {\n    data_plot_ci &lt;- data_plots_ci[[i_recalib]]\n    data_plot_n_bins_scores &lt;- data_plots_n_bins_scores[[i_recalib]]\n    \n    obs_calib &lt;- data_plot_ci$obs_calib\n    scores_calib &lt;- data_plot_ci$scores_calib\n    obs_test &lt;- data_plot_ci$obs_test\n    scores_test &lt;- data_plot_ci$scores_test\n    ci &lt;- data_plot_ci$e_scores_bins\n    \n    title &lt;- str_c(\"$\\\\\", type, \"=\", round(transform_scale[i_recalib], 2), \"$\")\n    \n    for (sample in c(\"Calibration\", \"Test\")) {\n      # Histogram with values\n      df_plot &lt;- ci |&gt; filter(sample == !!sample)\n      \n      if (sample == \"Calibration\"){\n        obs_current &lt;- obs_calib\n        scores_current &lt;- scores_calib\n        colour &lt;- \"#D55E00\"\n        n_bins_current &lt;- data_plot_n_bins_scores$n_bins_calib\n        n_bins_c_current &lt;- data_plot_n_bins_scores$n_bins_c_calib\n      } else {\n        obs_current &lt;- obs_test\n        scores_current &lt;- scores_test\n        colour &lt;- \"#009E73\"\n        n_bins_current &lt;- data_plot_n_bins_scores$n_bins_test\n        n_bins_c_current &lt;- data_plot_n_bins_scores$n_bins_c_test\n      }\n      \n      # heights &lt;- rbind(n_bins_current, n_bins_c_current)\n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n      barplot(\n        n_bins_current,\n        col = adjustcolor(\"#000000\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", \n        main = latex2exp::TeX(title)\n      )\n      barplot(\n        n_bins_c_current,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      \n      par(mar = c(4.1, 4.3, 0.5, 0.5))\n      plot(\n        df_plot$prediction, df_plot$mean,\n        pch = 19, ylim = y_lim, xlim = 0:1,\n        xlab = latex2exp::TeX(\"Predicted score $p$\"), \n        ylab = latex2exp::TeX(\"$E(d | s(x) = p)$\"),\n        col = colour\n      )\n      arrows(\n        x0 = df_plot$prediction, y0 = df_plot$lower, \n        x1 = df_plot$prediction, y1 = df_plot$upper,\n        angle = 90,length = .05, code = 3,\n        col = colour\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n    \n  }\n}\n\n\n\nDisplay the code used to plot the calibration curves.\nmethods &lt;- c(\n  \"True Prob.\", \"No Calibration\", \"platt\", \"isotonic\", \"beta\", \n  \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\n\ni_alpha &lt;- grid_alpha |&gt; mutate(i = row_number()) |&gt; group_by(alpha) |&gt; slice(1) |&gt; \n  pull(i)\ni_gamma &lt;- grid_gamma |&gt; mutate(i = row_number()) |&gt; group_by(gamma) |&gt; slice(1) |&gt; \n  pull(i)\n\ntab &lt;- tibble(i = i_alpha, type = \"alpha\") |&gt; \n  bind_rows(\n    tibble(i = i_gamma, type = \"gamma\")\n  ) |&gt; \n  mutate(id = str_c(i, \"_\", type))\n\ngrid &lt;- expand_grid(id = tab$id, method = methods) |&gt; \n  separate(id, into = c(\"i\", \"type\"), sep = \"_\") |&gt; \n  mutate(i = as.numeric(i))\n\nmethod_names &lt;- tribble(\n  ~method, ~method_lab,\n  \"True Prob.\", \"True Prob.\",\n  \"No Calibration\", \"No Calibration\", \n  \"platt\", \"Platt Scaling\",\n  \"isotonic\", \"Isotonic Reg.\",\n  \"beta\", \"Beta Calib.\",\n  \"locfit_0\", \"Local Reg. (deg = 0)\", \n  \"locfit_1\", \"Local Reg. (deg = 1)\",\n  \"locfit_2\", \"Local Reg (deg = 2)\"\n)\n\n\nIn the Figures below, for the tabs True Pob. and No Calibration, the plots show the calibration curves obtained using the true probabilities and the uncalibrated scores instead of recalibrated scores. We do this for comparison purposes.\n\nFor \\(\\alpha\\)For \\(\\gamma\\)\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained using quantile-defined bins on the recalibrated scores for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Calibration Curve with Moving Average\nThe calibration curves will computed using the local_ci_scores() (defined in Section 2.3.1) and accompanied by a confidence interval obtained using the binom.confint() function from {binom}.\nLet us first focus on a single replication for which we can plot the calibration curve with its confidence interval.\n\ncalibration_curve_ma_simul &lt;- function(simul, \n                                       method, \n                                       nn = .15,\n                                       prob = .95, \n                                       ci_method = \"probit\") {\n  \n  \n  \n  obs_calib &lt;- simul$data_all_calib$d\n  obs_test &lt;- simul$data_all_test$d\n  \n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_c\n    scores_test &lt;- tb_score_c_test$p_c\n  }\n  \n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n\n  keep_linspace_calib &lt;- which(\n    linspace_raw &gt;= min(scores_calib) & linspace_raw &lt;= max(scores_calib)\n  )\n  linspace_calib &lt;- linspace_raw[keep_linspace_calib]\n  \n  calib_curve_calib &lt;- map(\n      .x = linspace_calib,\n      .f = ~local_ci_scores(\n        obs = obs_calib,\n        scores = scores_calib,\n        tau = .x, \n        nn = nn, prob = prob, method = ci_method)\n    ) |&gt; \n    list_rbind() |&gt; \n    mutate(sample = \"Calibration\")\n  \n  keep_linspace_test &lt;- which(\n    linspace_raw &gt;= min(scores_test) & linspace_raw &lt;= max(scores_test)\n  )\n  linspace_test &lt;- linspace_raw[keep_linspace_test]\n  \n  calib_curve_test &lt;- map(\n    .x = linspace_test,\n    .f = ~local_ci_scores(\n      obs = obs_test,\n      scores = scores_test,\n      tau = .x, \n      nn = nn, prob = prob, method = ci_method)\n  ) |&gt; \n    list_rbind() |&gt; \n    mutate(sample = \"Test\")\n  \n  tb_calibration_curve_ma &lt;- \n    calib_curve_calib |&gt; \n    bind_rows(calib_curve_test) |&gt; \n    mutate(\n      method = method,\n      seed = simul$seed\n    )\n   \n  tb_calibration_curve_ma\n}\n\nFor convenience, we create a function, get_data_plot_calib_ma_simul() that returns two elements::\n\ntb_ci: confidence intervals associated with the calibration curve for a single replication\nn_bins_scores: the count of observation in each bins defined over the [0,1] segment for the scores (uncalibrated and calibrated, for both the train set and the test set).\n\n\n#' @param i index of the simulation to use (in `simul_recalib_alpha` or \n#'   `simul_recalib_gamma`)\n#' @param type type of transformed probabilities (made on `alpha` or `gamma`)\n#' @param method name of the recalibration method to focus on\nget_data_plot_calib_ma_simul &lt;- function(i, type, method) {\n  if (type == \"alpha\") {\n    simul &lt;- simul_recalib_alpha[[i]]\n    transform_scale &lt;- grid_alpha$alpha[i]\n  } else if (type == \"gamma\") {\n    simul &lt;- simul_recalib_gamma[[i]]\n    transform_scale &lt;- grid_gamma$gamma[i]\n  } else {\n    stop(\"Wrong value for argument `type`.\")\n  }\n  \n  # Counting number of obs in bins defined over [0,1]\n  breaks &lt;- seq(0, 1, by = .05)\n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_u\n    scores_test &lt;- tb_score_c_test$p_u\n    scores_c_calib &lt;- tb_score_c_calib$p_c\n    scores_c_test &lt;- tb_score_c_test$p_c\n  }\n  \n  n_bins_calib &lt;- table(cut(scores_calib, breaks = breaks))\n  n_bins_test &lt;- table(cut(scores_test, breaks = breaks))\n  if (!is.null(scores_c_calib)) {\n    n_bins_c_calib &lt;- table(cut(scores_c_calib, breaks = breaks))\n  } else {\n    n_bins_c_calib &lt;- NA_integer_\n  }\n  if (!is.null(scores_c_test)) {\n    n_bins_c_test &lt;- table(cut(scores_c_test, breaks = breaks))\n  } else {\n    n_bins_c_test &lt;- NA_integer_\n  }\n  \n  n_bins_scores &lt;- tibble(\n    bins = names(table(cut(breaks, breaks = breaks))),\n    n_bins_calib = as.vector(n_bins_calib),\n    n_bins_test = as.vector(n_bins_test),\n    n_bins_c_calib = as.vector(n_bins_c_calib),\n    n_bins_c_test = as.vector(n_bins_c_test),\n    method = method,\n    seed = simul$seed,\n    type = type\n  )\n  \n  # Confidence intervals\n  tb_ci &lt;- calibration_curve_ma_simul(\n    simul = simul, \n    method = method, \n    nn = .15, prob = .95,  ci_method = \"probit\"\n  )\n  list(\n    tb_ci = tb_ci,\n    n_bins_scores = n_bins_scores\n  )\n}\n\nWe define a function that will plot the calibration maps computed on the calibration set and those computed on the test set. This function will plot a panel of calibration maps, each row corresponding to a specific value of the scale used to transform the probabilities (\\(\\alpha\\) or \\(\\gamma\\)).\n\nmethods &lt;- c(\n  \"True Prob.\", \"No Calibration\", \"platt\", \"isotonic\", \"beta\", \n  \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\n\ni_alpha &lt;- grid_alpha |&gt; mutate(i = row_number()) |&gt; group_by(alpha) |&gt; slice(1) |&gt; \n  pull(i)\ni_gamma &lt;- grid_gamma |&gt; mutate(i = row_number()) |&gt; group_by(gamma) |&gt; slice(1) |&gt; \n  pull(i)\n\ntab &lt;- tibble(i = i_alpha, type = \"alpha\") |&gt; \n  bind_rows(\n    tibble(i = i_gamma, type = \"gamma\")\n  ) |&gt; \n  mutate(id = str_c(i, \"_\", type))\n\ngrid &lt;- expand_grid(id = tab$id, method = methods) |&gt; \n  separate(id, into = c(\"i\", \"type\"), sep = \"_\") |&gt; \n  mutate(i = as.numeric(i))\n\nmethod_names &lt;- tribble(\n  ~method, ~method_lab,\n  \"True Prob.\", \"True Prob.\",\n  \"No Calibration\", \"No Calibration\", \n  \"platt\", \"Platt Scaling\",\n  \"isotonic\", \"Isotonic Reg.\",\n  \"beta\", \"Beta Calib.\",\n  \"locfit_0\", \"Local Reg. (deg = 0)\", \n  \"locfit_1\", \"Local Reg. (deg = 1)\",\n  \"locfit_2\", \"Local Reg (deg = 2)\"\n)\n\nThe function:\n\nplot_conf_int_ma_simul &lt;- function(method, type) {\n  current_grid &lt;- grid |&gt; \n    filter(method == !!method, type == !!type)\n  \n  if (type == \"alpha\") {\n    transform_scale &lt;- grid_alpha |&gt; slice(current_grid$i) |&gt; pull(alpha)\n  } else if (type == \"gamma\") {\n    transform_scale &lt;- grid_gamma |&gt; slice(current_grid$i) |&gt; pull(gamma)\n  } else {\n    stop(\"Error argument `type`. Wrong value: either \\\"alpha\\\" or \\\"gamma\\\"\")\n  }\n  \n  data_plots &lt;-  map(\n    .x = current_grid$i,\n    .f = ~get_data_plot_calib_ma_simul(i = .x, type = type, method = method)\n  )\n  \n  tb_cis &lt;- map(data_plots, pluck(\"tb_ci\"))\n  n_bins_scores &lt;- map(data_plots, pluck(\"n_bins_scores\"))\n  \n  nb &lt;- length(data_plots)\n  mat &lt;- mat_init &lt;- matrix(c(1:4), ncol = 2)\n  for (j in 1:(nb-1)) {\n    mat &lt;- rbind(mat, mat_init + j * 4)\n  }\n  layout(mat, heights = rep(c(1, 3), nb))\n  \n  y_lim &lt;- c(0, 1)\n  \n  # i_recalib &lt;- 1\n  for (i_recalib in 1:nb) {\n    tb_ci_current &lt;- tb_cis[[i_recalib]]\n    n_bins_scores_current &lt;- n_bins_scores[[i_recalib]]\n    \n    title &lt;- str_c(\"$\\\\\", type, \"=\", round(transform_scale[i_recalib], 2), \"$\")\n    \n    for (sample in c(\"Calibration\", \"Test\")) {\n      # Histogram with values\n      tb_plot &lt;- tb_ci_current |&gt; filter(sample == !!sample)\n      \n      if (sample == \"Calibration\"){\n        colour &lt;- \"#D55E00\"\n        n_bins_current &lt;- n_bins_scores_current$n_bins_calib\n        n_bins_c_current &lt;- n_bins_scores_current$n_bins_c_calib\n      } else {\n        colour &lt;- \"#009E73\"\n        n_bins_current &lt;- n_bins_scores_current$n_bins_test\n        n_bins_c_current &lt;- n_bins_scores_current$n_bins_c_test\n      }\n      \n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n      barplot(\n        n_bins_current,\n        col = adjustcolor(\"#000000\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", \n        main = latex2exp::TeX(title)\n      )\n      barplot(\n        n_bins_c_current,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      \n      par(mar = c(4.1, 4.3, 0.5, 0.5))\n      plot(\n        tb_plot$xlim, tb_plot$mean,\n        pch = 19, ylim = y_lim, xlim = 0:1, type = \"l\",\n        xlab = latex2exp::TeX(\"Predicted score $p$\"), \n        ylab = latex2exp::TeX(\"$E(d | s(x) = p)$\"),\n        col = colour\n      )\n      polygon(\n        c(tb_plot$xlim, rev(tb_plot$xlim)),\n        c(tb_plot$lower, rev(tb_plot$upper)),\n        col = adjustcolor(col = colour, alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\nFor the first two tabs, True Prob. and No Calibration, the calibration curves are those computed using the true probabilities and the uncalibrated scores instead of some recalibrated scores. This is done for comparison purposes.\n\nFor \\(\\alpha\\)For \\(\\gamma\\)\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with a moving average for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for a single replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores."
  },
  {
    "objectID": "recalibration.html#calibration-maps-200-replications",
    "href": "recalibration.html#calibration-maps-200-replications",
    "title": "2  Recalibration",
    "section": "2.7 Calibration Maps (200 replications)",
    "text": "2.7 Calibration Maps (200 replications)\nWe now turn to the same type of visualization, but adapted to the 200 replications instead of a single one.\nFirst, let us create a function, get_count_simul() to get the number of observation in each bin separating the [0,1] segment with uncalibrated and recalibrated scores (both on the calibration and the recalibration sets), for all the simulations. Then, we can compute an average count per bin over the simulations. This will be useful to have an idea of the distributions of scores in the different scenarios (varying values for \\(\\alpha\\) or \\(\\gamma\\)) and each recalibration method (Platt Scaling, Isotonic regression, etc.).\n\n#' @param i index of the simulation to use (in `simul_recalib_alpha` or \n#'   `simul_recalib_gamma`)\n#' @param type type of transformed probabilities (made on `alpha` or `gamma`)\n#' @param method name of the recalibration method to focus on\nget_count_simul &lt;- function(i, type, method) {\n  if (type == \"alpha\") {\n    simul &lt;- simul_recalib_alpha[[i]]\n    transform_scale &lt;- grid_alpha$alpha[i]\n  } else if (type == \"gamma\") {\n    simul &lt;- simul_recalib_gamma[[i]]\n    transform_scale &lt;- grid_gamma$gamma[i]\n  } else {\n    stop(\"Wrong value for argument `type`.\")\n  }\n  \n  # Counting number of obs in bins defined over [0,1]\n  breaks &lt;- seq(0, 1, by = .05)\n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n    scores_c_calib &lt;- scores_c_test &lt;- NULL\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_u\n    scores_test &lt;- tb_score_c_test$p_u\n    scores_c_calib &lt;- tb_score_c_calib$p_c\n    scores_c_test &lt;- tb_score_c_test$p_c\n  }\n  \n  n_bins_calib &lt;- table(cut(scores_calib, breaks = breaks))\n  n_bins_test &lt;- table(cut(scores_test, breaks = breaks))\n  if (!is.null(scores_c_calib)) {\n    n_bins_c_calib &lt;- table(cut(scores_c_calib, breaks = breaks))\n  } else {\n    n_bins_c_calib &lt;- NA_integer_\n  }\n  if (!is.null(scores_c_test)) {\n    n_bins_c_test &lt;- table(cut(scores_c_test, breaks = breaks))\n  } else {\n    n_bins_c_test &lt;- NA_integer_\n  }\n  \n  n_bins_scores &lt;- tibble(\n    bins = names(table(cut(breaks, breaks = breaks))),\n    n_bins_calib = as.vector(n_bins_calib),\n    n_bins_test = as.vector(n_bins_test),\n    n_bins_c_calib = as.vector(n_bins_c_calib),\n    n_bins_c_test = as.vector(n_bins_c_test),\n    method = method,\n    seed = simul$seed,\n    type = type,\n    transform_scale = transform_scale\n  )\n  n_bins_scores\n}\n\nLet us apply this function to all simulations, both for varying values of \\(\\alpha\\) and for \\(\\gamma\\):\n\ngrid_count_alpha &lt;- \n  expand_grid(\n    i = 1:nrow(grid_alpha), \n    method = c(\n      \"True Prob.\", \"No Calibration\",\n      \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\n  )\ngrid_count_gamma &lt;- \n  expand_grid(\n    i = 1:nrow(grid_gamma), \n    method = c(\n      \"True Prob.\", \"No Calibration\",\n      \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\n  )\n\ncount_scores_alpha &lt;- map(\n  .x = 1:nrow(grid_count_alpha),\n  .f = ~get_count_simul(\n    i = grid_count_alpha$i[.x], \n    type = \"alpha\", \n    method = grid_count_alpha$method[.x]\n  ),\n  .progress = TRUE\n)\ncount_scores_gamma &lt;- map(\n  .x = 1:nrow(grid_count_gamma),\n  .f = ~get_count_simul(\n    i = grid_count_gamma$i[.x], \n    type = \"gamma\", \n    method = grid_count_gamma$method[.x]\n  ),\n  .progress = TRUE\n)\n\nWen can then compute the average in each bin for each of the four scores (uncalibrated in the calibration set, uncalibrated in the test set, recalibrated in the calibration set, recalibrated in the test set), for each method, both for varying values of \\(\\alpha\\) and \\(\\gamma\\).\n\ncount_scores_alpha &lt;- \n  count_scores_alpha |&gt; \n  list_rbind() |&gt; \n  group_by(method, type, bins, transform_scale) |&gt; \n  summarise(\n    n_bins_calib = mean(n_bins_calib, na.rm = TRUE),\n    n_bins_test = mean(n_bins_test, na.rm = TRUE),\n    n_bins_c_calib = mean(n_bins_c_calib, na.rm = TRUE),\n    n_bins_c_test = mean(n_bins_c_test, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncount_scores_gamma &lt;- \n  count_scores_gamma |&gt; \n  list_rbind() |&gt; \n  group_by(method,type, bins, transform_scale) |&gt; \n  summarise(\n    n_bins_calib = mean(n_bins_calib, na.rm = TRUE),\n    n_bins_test = mean(n_bins_test, na.rm = TRUE),\n    n_bins_c_calib = mean(n_bins_c_calib, na.rm = TRUE),\n    n_bins_c_test = mean(n_bins_c_test, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n\n2.7.1 Quantile-Based Bins\nInstead of looking at the confidence intervals for a single replication, we can plot the 200 replications on a single plot. The quantiles can slightly change from one replication to another. It is therefore not possible to compute credible intervals.\n\n#' @param simul a single replication result\n#' @param method name of the method used to recalibrate for which to compute the calibration curve\n#' @param k number of bins to create (quantiles, default to `10`)\nget_summary_bins_simul &lt;- function(simul, method, k = 10) {\n  obs_calib &lt;- simul$data_all_calib$d\n  obs_test &lt;- simul$data_all_test$d\n  \n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_c\n    scores_test &lt;- tb_score_c_test$p_c\n  }\n  \n  summary_bins_calib &lt;- get_summary_bins(\n    obs = obs_calib, scores = scores_calib, k = k)\n  summary_bins_test &lt;- get_summary_bins(\n    obs = obs_test, scores = scores_test, k = k)\n  \n  summary_bins_calib |&gt; mutate(sample = \"Calibration\") |&gt; \n    bind_rows(summary_bins_test |&gt; mutate(sample = \"Test\")) |&gt; \n    mutate(method = method, seed = simul$seed)\n}\n\nLet us loop over all the methods and all the replications for each value of \\(\\alpha\\) to get the quantile-based calibration curves.\n\n# For alpha\nmethods &lt;- names(simul_recalib_alpha[[1]]$res_recalibration)\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\nsummary_bins_simuls_alpha &lt;- vector(mode = \"list\", length = length(methods))\nnames(summary_bins_simuls_alpha) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n    summary_bins_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_alpha,\n      .f = ~{\n        p()\n        get_summary_bins_simul(simul = .x, method = methods[i_method], k = 10)\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(summary_bins_simuls_m)) {\n    summary_bins_simuls_m[[j]]$scale_parameter &lt;- grid_alpha$alpha[j]\n  }\n  summary_bins_simuls_alpha[[i_method]] &lt;- summary_bins_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\nsummary_bins_simuls_alpha &lt;- list_rbind(\n  summary_bins_simuls_alpha, names_to = \"method\"\n) |&gt; \n  mutate(type = \"alpha\")\n\nWe do the same for the values of \\(\\gamma\\):\n\n# For gamma\nmethods &lt;- names(simul_recalib_gamma[[1]]$res_recalibration)\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\nsummary_bins_simuls_gamma &lt;- vector(mode = \"list\", length = length(methods))\nnames(summary_bins_simuls_gamma) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n    summary_bins_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_gamma,\n      .f = ~{\n        p()\n        get_summary_bins_simul(simul = .x, method = methods[i_method], k = 10)\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(summary_bins_simuls_m)) {\n    summary_bins_simuls_m[[j]]$scale_parameter &lt;- grid_gamma$gamma[j]\n  }\n  summary_bins_simuls_gamma[[i_method]] &lt;- summary_bins_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\nsummary_bins_simuls_gamma &lt;- list_rbind(\n  summary_bins_simuls_gamma, names_to = \"method\"\n) |&gt; \n  mutate(type = \"gamma\")\n\nThe results are stored in a single tibble:\n\nsummary_bins_simuls &lt;- summary_bins_simuls_alpha |&gt; \n  bind_rows(summary_bins_simuls_gamma)\n\nLet us define a function to plot the calibration curves on the calibration and the test samples.\n\nplot_calib_qbins_simuls &lt;- function(method, type) {\n  tb_calibration_curve &lt;- summary_bins_simuls |&gt; \n    filter(\n      method == !!method,\n      type == !!type\n    )\n  \n  if (type == \"alpha\") {\n    count_scores &lt;- count_scores_alpha |&gt; filter(method == !!method)\n  } else if (type == \"gamma\") {\n    count_scores &lt;- count_scores_gamma |&gt; filter(method == !!method)\n  } else {\n    stop(\"Type should be either \\\"alpha\\\" or \\\"gamma\\\"\")\n  }\n  \n  scale_params &lt;- unique(tb_calibration_curve$scale_parameter)\n  seeds &lt;- unique(tb_calibration_curve$seed)\n  colours &lt;- c(\"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n  \n  nb &lt;- length(scale_params)\n  mat &lt;- mat_init &lt;- matrix(c(1:4), ncol = 2)\n  for (j in 1:(nb-1)) {\n    mat &lt;- rbind(mat, mat_init + j * 4)\n  }\n  layout(mat, heights = rep(c(1, 3), nb))\n  \n  y_lim &lt;- c(0, 1)\n  \n  for (scale_param in scale_params) {\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    \n    for (sample in c(\"Calibration\", \"Test\")) {\n      if (sample == \"Calibration\"){\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_calib\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_calib\")\n      } else {\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_test\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_test\")\n      }\n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n      barplot(\n        n_bins_current,\n        col = adjustcolor(\"#000000\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", \n        main = latex2exp::TeX(title)\n      )\n      barplot(\n        n_bins_c_current,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      par(mar = c(4.1, 4.3, 0.5, 0.5))\n      plot(\n        0:1, 0:1,\n        type = \"l\", col = NULL,\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = \"\"\n      )\n      for (i_simul in seeds) {\n        tb_current &lt;- tb_calibration_curve |&gt; \n          filter(\n            scale_parameter == scale_param,\n            seed == i_simul,\n            sample == !!sample\n          )\n        lines(\n          tb_current$mean_score, tb_current$mean_obs,\n          lwd = 2, col = adjustcolor(colours[sample], alpha.f = 0.1), t = \"b\",\n          cex = .1, pch = 19\n        )\n      }\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\nThe figures below show a panel of graphs with the superimposed calibration curves obtained with the quantile-based bins. Each tab shows the curves for a type of recalibration used. The first two tabs (True Prob. and No Calibration) show the curves obtained using the true probabilities \\(p\\) and the uncalibrated probabilities \\(p^u\\), instead of the recalibrated probabilities \\(p^c\\). Each row of the panel in the Figures corresponds to a value for either \\(\\alpha\\) or \\(\\gamma\\) used to transform \\(p\\) to get \\(p^u\\). The left column shows the calibration curve obtained on the calibration set whereas the right column shows the calibration curve obtained on the test set. The average distribution (computed over the 200 simulations) of the uncalibrated scores and of the calibrated scores are shown in the histograms on top of each graph.\n\nFor \\(\\alpha\\)For \\(\\gamma\\)\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with quantile-defined bins for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\n\n\n\n\n\nLet us visualize the calibration curves in another way.\n\nplot_calib_quant_simuls &lt;- function(calib_curve, method) {\n  tb_calibration_curve_both &lt;- calib_curve |&gt;\n    filter(\n      method == !!method\n    )\n  \n  mat &lt;- c(\n    1, 3, 13, 15,\n    2, 4, 14, 16,\n    5, 7, 17, 19,\n    6, 8, 18, 20,\n    9, 11, 21, 23,\n    10, 12, 22, 24\n  ) |&gt;\n    matrix(ncol = 4, byrow = TRUE)\n  \n  layout(mat, height = rep(c(1, 3), 3))\n  \n  y_lim &lt;- c(0, 1)\n  \n  for (type in c(\"alpha\", \"gamma\")) {\n    \n    if (type == \"alpha\") {\n      count_scores &lt;- count_scores_alpha |&gt; filter(method == !!method)\n    } else if (type == \"gamma\") {\n      count_scores &lt;- count_scores_gamma |&gt; filter(method == !!method)\n    } else {\n      stop(\"Type should be either \\\"alpha\\\" or \\\"gamma\\\"\")\n    }\n    \n    tb_calibration_curve &lt;-\n      tb_calibration_curve_both |&gt;\n      filter(type == !!type)\n    \n    scale_params &lt;- unique(tb_calibration_curve$scale_parameter)\n    seeds &lt;- unique(tb_calibration_curve$seed)\n    colours &lt;- c(\"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n    \n    for (scale_param in scale_params) {\n      title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n      \n      for (sample in c(\"Calibration\", \"Test\")) {\n        \n        if (sample == \"Calibration\"){\n          n_bins_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_calib\")\n          n_bins_c_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_calib\")\n        } else {\n          n_bins_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_test\")\n          n_bins_c_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_test\")\n        }\n        \n        par(mar = c(0.5, 4.3, 1.0, 0.5))\n        y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n        barplot(\n          n_bins_current,\n          col = adjustcolor(\"#000000\", alpha.f = .3),\n          ylim = y_lim_bp,\n          border = \"white\",\n          axes = FALSE,\n          xlab = \"\", ylab = \"\",\n          main = latex2exp::TeX(title)\n        )\n        barplot(\n          n_bins_c_current,\n          col = adjustcolor(\"#0072B2\", alpha.f = .3),\n          ylim = y_lim_bp,\n          border = \"white\",\n          axes = FALSE,\n          xlab = \"\", ylab = \"\", main = \"\",\n          add = TRUE\n        )\n        \n        tb_current &lt;-\n          tb_calibration_curve |&gt;\n          filter(\n            scale_parameter == !!scale_param,\n            sample == !!sample\n          )\n        par(mar = c(4.1, 4.3, 0.5, 0.5), mgp = c(2, 1, 0))\n        plot(\n          0:1, 0:1,\n          type = \"l\", col = NULL,\n          xlim = 0:1, ylim = 0:1,\n          xlab = latex2exp::TeX(\"$p^u$\"), \n          ylab = latex2exp::TeX(\"$E(D | p^u = p^c)$\"),\n          main = \"\"\n        )\n        for (i_simul in seeds) {\n          tb_current &lt;- tb_calibration_curve |&gt; \n            filter(\n              scale_parameter == scale_param,\n              seed == i_simul,\n              sample == !!sample\n            )\n          lines(\n            tb_current$mean_score, tb_current$mean_obs,\n            lwd = 2, col = adjustcolor(colours[sample], alpha.f = 0.1), t = \"b\",\n            cex = .1, pch = 19\n          )\n          segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        }\n      }\n    }\n  }\n}\n\n\nmethods &lt;- c(\n  \"True Prob.\", \n  \"No Calibration\",\n  \"platt\", \"isotonic\", \"beta\", \n  \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\nmethods_labs &lt;- c(\n  \"True Prob.\",\n  \"No Calibration\", \"Platt\", \"Isotonic\", \"Beta\", \n  \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\"\n)\n\n\nTrue Prob.No CalibrationPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"True Prob.\"\n)\n\n\n\nFigure 2.19: Calibration Curves Calculated with True Probabilities as the Scores. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the true probabilities\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"No Calibration\"\n)\n\n\n\nFigure 2.20: Calibration Curves Calculated with Uncalibrated Scores. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the true probabilities.\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"platt\"\n)\n\n\n\nFigure 2.21: Calibration Curves Calculated with Scores Recalibrated Using Platt Scaling. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"isotonic\"\n)\n\n\n\nFigure 2.22: Calibration Curves Calculated with Scores Recalibrated Using Isotonic Regression. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"beta\"\n)\n\n\n\nFigure 2.23: Calibration Curves Calculated with Scores Recalibrated Using Beta Calibration. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_0\"\n)\n\n\n\nFigure 2.24: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 0). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_1\"\n)\n\n\n\nFigure 2.25: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 1). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\nplot_calib_quant_simuls(\n  calib_curve = summary_bins_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_2\"\n)\n\n\n\nFigure 2.26: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 2). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves of the 200 replications of the simulations are superimposed. The histogram on top of each graph show the distribution of the uncalibrated scores (gray), and that of the calibrated scores (blue).\n\n\n\n\n\n\n\n\n\n\n2.7.2 Calibration Curve with Local Regression\nWe will plot the calibration curves estimated using the local regression method, for all type of transformation of the probabilities made (varying either \\(\\alpha\\) or \\(\\gamma\\)).\nContrary to the quantile-based calibration curve, we can make predictions on a segment from 0 to 1 using the fitted local regression.\n\ncalibration_curve_locfit_simul &lt;- function(simul, \n                                           method, \n                                           k = 10) {\n  \n  \n  obs_calib &lt;- simul$data_all_calib$d\n  obs_test &lt;- simul$data_all_test$d\n  \n  if (method == \"True Prob.\") {\n    scores_calib &lt;- simul$data_all_calib$p\n    scores_test &lt;- simul$data_all_test$p\n  } else if (method == \"No Calibration\") {\n    scores_calib &lt;- simul$data_all_calib$p_u\n    scores_test &lt;- simul$data_all_test$p_u\n  } else {\n    tb_score_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib\n    tb_score_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test\n    scores_calib &lt;- tb_score_c_calib$p_c\n    scores_test &lt;- tb_score_c_test$p_c\n  }\n  \n  # Add a little noise (otherwise, R may crash...)\n  scores_calib &lt;- scores_calib + rnorm(length(scores_calib), 0, .001)\n  scores_test &lt;- scores_test + rnorm(length(scores_test), 0, .001)\n  \n  tb_calib &lt;- tibble(\n    obs = obs_calib,\n    scores = scores_calib\n  )\n  \n  tb_test &lt;- tibble(\n    obs = obs_test,\n    scores = scores_test\n  )\n  \n  locfit_0_calib &lt;- locfit(\n    formula = obs ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, data = tb_calib\n  )\n  \n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  \n  # Restricting this space to the range of observed scores\n  keep_linspace_calib &lt;- which(\n    linspace_raw &gt;= min(scores_calib) & linspace_raw &lt;= max(scores_calib)\n  )\n  linspace_calib &lt;- linspace_raw[keep_linspace_calib]\n  \n  score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace_calib)\n  score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n  score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n  \n  locfit_0_test &lt;- locfit(\n    formula = obs ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, data = tb_test\n  )\n  \n  keep_linspace_test &lt;- which(\n    linspace_raw &gt;= min(scores_test) & linspace_raw &lt;= max(scores_test)\n  )\n  linspace_test &lt;- linspace_raw[keep_linspace_test]\n  \n  score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace_test)\n  score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n  score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n  \n  tb_calibration_curve_locfit &lt;- tibble(\n    xlim = linspace_calib,\n    locfit_pred = score_c_locfit_0_calib,\n    method = method,\n    seed = simul$seed,\n    sample = \"calibration\"\n  ) |&gt; \n    bind_rows(\n      tibble(\n        xlim = linspace_test,\n        locfit_pred = score_c_locfit_0_test,\n        method = method,\n        seed = simul$seed,\n        sample = \"test\"\n      )\n    )\n  \n  tb_calibration_curve_locfit\n}\n\nLet us loop over all the methods and all the replications for each value of \\(\\alpha\\) to get the calibration curves based on local regression.\n\n# For alpha\nmethods &lt;- names(simul_recalib_alpha[[1]]$res_recalibration)\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\ncalib_curve_locfit_simuls_alpha &lt;- vector(mode = \"list\", length = length(methods))\nnames(calib_curve_locfit_simuls_alpha) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n    calib_curve_locfit_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_alpha,\n      .f = ~{\n        p()\n        calibration_curve_locfit_simul(\n          simul = .x, method = methods[i_method], k = 10\n        )\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(calib_curve_locfit_simuls_m)) {\n    calib_curve_locfit_simuls_m[[j]]$scale_parameter &lt;- grid_alpha$alpha[j]\n  }\n  calib_curve_locfit_simuls_alpha[[i_method]] &lt;- calib_curve_locfit_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\ncalib_curve_locfit_simuls_alpha &lt;- list_rbind(\n  calib_curve_locfit_simuls_alpha, names_to = \"method\"\n) |&gt; \n  mutate(type = \"alpha\")\n\nLet us loop over the simulations made with varying values for \\(\\gamma\\):\n\n# For gamma\nmethods &lt;- names(simul_recalib_gamma[[1]]$res_recalibration)\n# Remove isotonic which makes the R session crash...\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\ncalib_curve_locfit_simuls_gamma &lt;- vector(mode = \"list\", length = length(methods))\nnames(calib_curve_locfit_simuls_gamma) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n    calib_curve_locfit_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_gamma,\n      .f = ~{\n        p()\n        calibration_curve_locfit_simul(\n          simul = .x, method = methods[i_method], k = 10\n        )\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(calib_curve_locfit_simuls_m)) {\n    calib_curve_locfit_simuls_m[[j]]$scale_parameter &lt;- grid_gamma$gamma[j]\n  }\n  calib_curve_locfit_simuls_gamma[[i_method]] &lt;- calib_curve_locfit_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\ncalib_curve_locfit_simuls_gamma &lt;- list_rbind(\n  calib_curve_locfit_simuls_gamma, names_to = \"method\"\n) |&gt; \n  mutate(type = \"gamma\")\n\nThe results are stored in a single tibble:\n\ncalib_curve_locfit_simuls &lt;- \n  calib_curve_locfit_simuls_alpha |&gt; \n  bind_rows(calib_curve_locfit_simuls_gamma) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"calibration\", \"test\"), labels = c(\"Calibration\", \"Test\")\n    )\n  )\n\nLet us define a function to plot the calibration curves on the calibration and the test samples.\n\nplot_calib_locfit_simuls &lt;- function(method, type) {\n  tb_calibration_curve &lt;- calib_curve_locfit_simuls |&gt; \n    filter(\n      method == !!method,\n      type == !!type\n    )\n  if (type == \"alpha\") {\n    count_scores &lt;- count_scores_alpha |&gt; filter(method == !!method)\n  } else if (type == \"gamma\") {\n    count_scores &lt;- count_scores_gamma |&gt; filter(method == !!method)\n  } else {\n    stop(\"Type should be either alpha or gamma\")\n  }\n  \n  scale_params &lt;- unique(tb_calibration_curve$scale_parameter)\n  seeds &lt;- unique(tb_calibration_curve$seed)\n  colours &lt;- c(\"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n  \n  nb &lt;- length(scale_params)\n  mat &lt;- mat_init &lt;- matrix(c(1:4), ncol = 2)\n  for (j in 1:(nb-1)) {\n    mat &lt;- rbind(mat, mat_init + j * 4)\n  }\n  layout(mat, heights = rep(c(1, 3), nb))\n  \n  y_lim &lt;- c(0, 1)\n  \n  for (scale_param in scale_params) {\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    \n    for (sample in c(\"Calibration\", \"Test\")) {\n      \n      if (sample == \"Calibration\"){\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_calib\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_calib\")\n      } else {\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_test\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_test\")\n      }\n      \n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n      barplot(\n        n_bins_current,\n        col = adjustcolor(\"#000000\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", \n        main = latex2exp::TeX(title)\n      )\n      barplot(\n        n_bins_c_current,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      \n      tb_current &lt;- \n        tb_calibration_curve |&gt; \n        filter(\n          scale_parameter == !!scale_param,\n          sample == !!sample\n        ) |&gt; \n        group_by(type, scale_parameter, xlim) |&gt; \n        summarise(\n          mean = mean(locfit_pred),\n          lower = quantile(locfit_pred, probs = .025),\n          upper = quantile(locfit_pred, probs = .975),\n          .groups = \"drop\"\n        )\n      par(mar = c(4.1, 4.3, 0.5, 0.5))\n      plot(\n        tb_current$xlim, tb_current$mean,\n        type = \"l\", col = colours[sample],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = \"\"\n      )\n      polygon(\n        c(tb_current$xlim, rev(tb_current$xlim)),\n        c(tb_current$lower, rev(tb_current$upper)),\n        col = adjustcolor(col = colours[sample], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\nThe figures below show a panel of graphs with the calibration curves obtained with the local regression method. Each tab shows the average curve obtained on the 200 replications for a type of recalibration used, as well as the 95% bootstrap confidence intervals. The first two tab (True Prob. and No Calibration) show the curves obtained using the true probabilities \\(p\\) and the uncalibrated probabilities \\(p^u\\), instead of the recalibrated probabilities \\(p^c\\). Each row of the panel in the Figures corresponds to a value for either \\(\\alpha\\) or \\(\\gamma\\) used to transform \\(p\\) to get \\(p^u\\). The left column shows the calibration curve obtained on the calibration set whereas the right column shows the calibration curve obtained on the test set. The average distribution (computed over the 200 simulations) of the uncalibrated scores and of the calibrated scores are shown in the histograms on top of each graph.\n\nFor \\(\\alpha\\)For \\(\\gamma\\)\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with local regressions for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nLet us visualize this in another way.\nplot_calib_locfit_simuls_2 &lt;- function(calib_curve, method) {\n  tb_calibration_curve_both &lt;- calib_curve |&gt;\n    filter(\n      method == !!method\n    )\n  \n  mat &lt;- c(\n    1, 3, 13, 15,\n    2, 4, 14, 16,\n    5, 7, 17, 19,\n    6, 8, 18, 20,\n    9, 11, 21, 23,\n    10, 12, 22, 24\n  ) |&gt;\n    matrix(ncol = 4, byrow = TRUE)\n  \n  layout(mat, height = rep(c(1, 3), 3))\n  \n  y_lim &lt;- c(0, 1)\n  \n  for (type in c(\"alpha\", \"gamma\")) {\n    \n    tb_calibration_curve &lt;-\n      tb_calibration_curve_both |&gt;\n      filter(type == !!type)\n    \n    if (type == \"alpha\") {\n      count_scores &lt;- count_scores_alpha |&gt; filter(method == !!method)\n    } else if (type == \"gamma\") {\n      count_scores &lt;- count_scores_gamma |&gt; filter(method == !!method)\n    }\n    \n    scale_params &lt;- unique(tb_calibration_curve$scale_parameter)\n    seeds &lt;- unique(tb_calibration_curve$seed)\n    colours &lt;- c(\"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n    \n    for (scale_param in scale_params) {\n      title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n      \n      for (sample in c(\"Calibration\", \"Test\")) {\n        \n        if (sample == \"Calibration\"){\n          n_bins_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_calib\")\n          n_bins_c_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_calib\")\n        } else {\n          n_bins_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_test\")\n          n_bins_c_current &lt;- count_scores |&gt;\n            filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_test\")\n        }\n        \n        par(mar = c(0.5, 4.3, 1.0, 0.5))\n        y_lim_bp &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n        barplot(\n          n_bins_current,\n          col = adjustcolor(\"#000000\", alpha.f = .3),\n          ylim = y_lim_bp,\n          border = \"white\",\n          axes = FALSE,\n          xlab = \"\", ylab = \"\",\n          main = latex2exp::TeX(title)\n        )\n        barplot(\n          n_bins_c_current,\n          col = adjustcolor(\"#0072B2\", alpha.f = .3),\n          ylim = y_lim_bp,\n          border = \"white\",\n          axes = FALSE,\n          xlab = \"\", ylab = \"\", main = \"\",\n          add = TRUE\n        )\n        \n        tb_current &lt;-\n          tb_calibration_curve |&gt;\n          filter(\n            scale_parameter == !!scale_param,\n            sample == !!sample\n          ) |&gt;\n          group_by(type, scale_parameter, xlim, sample) |&gt;\n          summarise(\n            mean = mean(locfit_pred),\n            lower = quantile(locfit_pred, probs = .025),\n            upper = quantile(locfit_pred, probs = .975),\n            .groups = \"drop\"\n          )\n        par(mar = c(4.1, 4.3, 0.5, 0.5), mgp = c(2, 1, 0))\n        plot(\n          tb_current$xlim, tb_current$mean,\n          type = \"l\", col = colours[sample],\n          xlim = 0:1, ylim = 0:1,\n          xlab = latex2exp::TeX(\"$p^u$\"), \n          ylab = latex2exp::TeX(\"$E(D | p^u = p^c)$\"),\n          main = \"\"\n        )\n        polygon(\n          c(tb_current$xlim, rev(tb_current$xlim)),\n          c(tb_current$lower, rev(tb_current$upper)),\n          col = adjustcolor(col = colours[sample], alpha.f = .4),\n          border = NA\n        )\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n      }\n    }\n  }\n}\n\nTrue Prob.No CalibrationPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"True Prob.\"\n)\n\n\n\nFigure 2.27: Calibration Curves Calculated with True Probabilities as the Scores. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the true probabilities\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"No Calibration\"\n)\n\n\n\nFigure 2.28: Calibration Curves Calculated with Uncalibrated Scores. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the true probabilities\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"platt\"\n)\n\n\n\nFigure 2.29: Calibration Curves Calculated with Scores Recalibrated Using Platt Scaling. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"isotonic\"\n)\n\n\n\nFigure 2.30: Calibration Curves Calculated with Scores Recalibrated Using Isotonic Regression. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"beta\"\n)\n\n\n\nFigure 2.31: Calibration Curves Calculated with Scores Recalibrated Using Beta Calibration. The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_0\"\n)\n\n\n\nFigure 2.32: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 0). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_1\"\n)\n\n\n\nFigure 2.33: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 1). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\nplot_calib_locfit_simuls_2(\n  calib_curve = calib_curve_locfit_simuls |&gt; \n    filter(scale_parameter %in% c(1/3, 1, 3)), \n  method = \"locfit_2\"\n)\n\n\n\nFigure 2.34: Calibration Curves Calculated with Scores Recalibrated Using Local Regression (with degree 2). The curves are obtained with quantile binning, for the calibration set (orange) and for the test set (green) for varying values of \\(\\alpha\\) and \\(\\gamma\\). The curves are the average values obtained on 200 replications of the simulations, the bands correspond to 95% bootstrap interval. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n2.7.3 Calibration Curve with Moving Average\nLet us plot the calibration curve obtained with moving average, for the 200 replications.\n\n# For alpha\nmethods &lt;- names(simul_recalib_alpha[[1]]$res_recalibration)\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\ncalib_curve_ma_simuls_alpha &lt;- vector(mode = \"list\", length = length(methods))\nnames(calib_curve_ma_simuls_alpha) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_alpha))\n    calib_curve_ma_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_alpha,\n      .f = ~{\n        p()\n        calibration_curve_ma_simul(\n          simul = .x, method = methods[i_method],\n          nn = .15, \n          prob = .95, \n          ci_method = \"probit\"\n        )\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(calib_curve_ma_simuls_m)) {\n    calib_curve_ma_simuls_m[[j]]$scale_parameter &lt;- grid_alpha$alpha[j]\n  }\n  calib_curve_ma_simuls_alpha[[i_method]] &lt;- calib_curve_ma_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\ncalib_curve_ma_simuls_alpha &lt;- list_rbind(\n  calib_curve_ma_simuls_alpha, names_to = \"method\"\n) |&gt; \n  mutate(type = \"alpha\")\n\nLet us loop over the simulations made with varying values for \\(\\gamma\\):\n\n# For gamma\nmethods &lt;- names(simul_recalib_gamma[[1]]$res_recalibration)\n# Remove isotonic which makes the R session crash...\nmethods &lt;- c(\"True Prob.\", \"No Calibration\", methods)\ncalib_curve_ma_simuls_gamma &lt;- vector(mode = \"list\", length = length(methods))\nnames(calib_curve_ma_simuls_gamma) &lt;- methods\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\n\nfor (i_method in 1:length(methods)) {\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid_gamma))\n    calib_curve_ma_simuls_m &lt;- furrr::future_map(\n      .x = simul_recalib_gamma,\n      .f = ~{\n        p()\n        calibration_curve_ma_simul(\n          simul = .x, method = methods[i_method],\n          nn = .15, \n          prob = .95, \n          ci_method = \"probit\"\n        )\n      },\n      .options = furrr::furrr_options(seed = FALSE)\n    )\n  })\n  # Add value for alpha\n  for (j in 1:length(calib_curve_ma_simuls_m)) {\n    calib_curve_ma_simuls_m[[j]]$scale_parameter &lt;- grid_gamma$gamma[j]\n  }\n  calib_curve_ma_simuls_gamma[[i_method]] &lt;- calib_curve_ma_simuls_m |&gt; \n    list_rbind(names_to = \"i_row\")\n}\ncalib_curve_ma_simuls_gamma &lt;- list_rbind(\n  calib_curve_ma_simuls_gamma, names_to = \"method\"\n) |&gt; \n  mutate(type = \"gamma\")\n\nThe results are stored in a single tibble:\n\ncalib_curve_ma_simuls &lt;- \n  calib_curve_ma_simuls_alpha |&gt; \n  bind_rows(calib_curve_ma_simuls_gamma) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"Calibration\", \"Test\"), labels = c(\"Calibration\", \"Test\")\n    )\n  )\n\nLet us define a function to plot the calibration curves on the calibration and the test samples.\n\nplot_calib_ma_simuls &lt;- function(method, type) {\n  tb_calibration_curve &lt;- calib_curve_ma_simuls |&gt; \n    filter(\n      method == !!method,\n      type == !!type\n    )\n  if (type == \"alpha\") {\n    count_scores &lt;- count_scores_alpha |&gt; filter(method == !!method)\n  } else if (type == \"gamma\") {\n    count_scores &lt;- count_scores_gamma |&gt; filter(method == !!method)\n  } else {\n    stop(\"Type should be either \\\"alpha\\\" or \\\"gamma\\\"\")\n  }\n  scale_params &lt;- unique(tb_calibration_curve$scale_parameter)\n  seeds &lt;- unique(tb_calibration_curve$seed)\n  colours &lt;- c(\"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n  \n  nb &lt;- length(scale_params)\n  mat &lt;- mat_init &lt;- matrix(c(1:4), ncol = 2)\n  for (j in 1:(nb-1)) {\n    mat &lt;- rbind(mat, mat_init + j * 4)\n  }\n  layout(mat, heights = rep(c(1, 3), nb))\n  \n  y_lim &lt;- c(0, 1)\n  \n  for (scale_param in scale_params) {\n    title &lt;- str_c(\"$\\\\\", type, \" = $\", round(scale_param, 2))\n    \n    for (sample in c(\"Calibration\", \"Test\")) {\n      \n      if (sample == \"Calibration\"){\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_calib\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_calib\")\n      } else {\n        n_bins_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_test\")\n        n_bins_c_current &lt;- count_scores |&gt; \n          filter(transform_scale == !!scale_param) |&gt; pull(\"n_bins_c_test\")\n      }\n      \n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim &lt;- range(c(n_bins_current, n_bins_c_current), na.rm = TRUE)\n      barplot(\n        n_bins_current,\n        col = adjustcolor(\"#000000\", alpha.f = .3),\n        ylim = y_lim,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", \n        main = latex2exp::TeX(title)\n      )\n      barplot(\n        n_bins_c_current,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      par(mar = c(4.1, 4.3, 0.5, 0.5))\n      tb_current &lt;- \n        tb_calibration_curve |&gt; \n        filter(\n          scale_parameter == !!scale_param,\n          sample == !!sample\n        ) |&gt; \n        group_by(type, scale_parameter, xlim) |&gt; \n        summarise(\n          lower = quantile(mean, probs = .025),\n          upper = quantile(mean, probs = .975),\n          mean = mean(mean),\n          .groups = \"drop\"\n        )\n      plot(\n        tb_current$xlim, tb_current$mean,\n        type = \"l\", col = colours[sample],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = \"\"\n      )\n      polygon(\n        c(tb_current$xlim, rev(tb_current$xlim)),\n        c(tb_current$lower, rev(tb_current$upper)),\n        col = adjustcolor(col = colours[sample], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\nThe figures below show a panel of graphs with the calibration curves obtained with the moving average method. Each tab shows the average curve obtained on the 200 replications for a type of recalibration used, as well as the average of the 95% confidence intervals computed on each simulation The first two tab (True Prob. and No Calibration) show the curves obtained using the true probabilities \\(p\\) and the uncalibrated probabilities \\(p^u\\), instead of the recalibrated probabilities \\(p^c\\). Each row of the panel in the Figures corresponds to a value for either \\(\\alpha\\) or \\(\\gamma\\) used to transform \\(p\\) to get \\(p^u\\). The left column shows the calibration curve obtained on the calibration set whereas the right column shows the calibration curve obtained on the test set. The average distribution (computed over the 200 simulations) of the uncalibrated scores and of the calibrated scores are shown in the histograms on top of each graph.\n\nFor \\(\\alpha\\)For \\(\\gamma\\)\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\alpha\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nTrue Prob.No CalibrationPlatt ScalingIsotonic Reg.Beta Calib.Local Reg. (deg = 0)Local Reg. (deg = 1)Local Reg (deg = 2)\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores\n\n\n\n\n\n\n\n\n\n\n\nCalibration curves obtained with recalibrated scores. The curves are obtained with the moving average method for the calibration set (left) and for the test set (right) for varying values of \\(\\gamma\\) for 200 replication of the simulations. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores"
  },
  {
    "objectID": "recalibration.html#boxplots-of-the-metrics",
    "href": "recalibration.html#boxplots-of-the-metrics",
    "title": "2  Recalibration",
    "section": "2.8 Boxplots of the Metrics",
    "text": "2.8 Boxplots of the Metrics\nWe now focus on the calibration metrics\n\ncalib_metrics_simul_alpha &lt;- map(simul_recalib_alpha, \"calib_metrics\") |&gt; \n  list_rbind()\ncalib_metrics_simul_gamma &lt;- map(simul_recalib_gamma, \"calib_metrics\") |&gt; \n  list_rbind()\n\nLet us put all the metrics computed for all the simulations in a single tibble.\n\ncalib_metrics_simul &lt;- calib_metrics_simul_alpha |&gt; \n  bind_rows(calib_metrics_simul_gamma) |&gt; \n  pivot_longer(\n    cols = c(mse, brier, ece, qmse, wmse, lcs),\n    names_to = \"metric\", values_to = \"value\"\n  ) |&gt; \n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\", \"lcs\"),\n      labels = c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\", \"LCS\")\n    ),\n    method = factor(\n      method,\n      levels = c(\n        \"True Prob.\", \"No Calibration\", \n        \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"),\n      labels = c(\n        \"True Prob.\", \"No Calibration\", \n        \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n        \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\"\n      )\n    ),\n    sample = factor(\n      sample,\n      levels = c(\"Calibration\", \"Test\")\n    )\n  )\n\nThen, we create a function, plot_boxplot_metric() to graph boxplots for a metric, for each value of \\(\\alpha\\) or \\(\\gamma\\) (x-asis). The y-axis show the values of the desired metric. Each panel of the figure uses a specific predicted score:\n\nTrue Proba: \\(p^c := p\\) the true probabilities from the DGP\nNo Calibration: \\(p^c := p^u\\) the transformed probabilities\nPlatt Scaling: \\(p^c := g^{\\text{platt}}(p^u)\\) scores recalibrated using Platt Scaling\nIsotonic Reg.: \\(p^c := g^{\\text{iso}}(p^u)\\) scores recalibrated using isotonic regression\nBeta Calib.: \\(p^c := g^{\\text{beta}}(p^u)\\) scores recalibrated using beta calibration\nLocal Reg. (deg = 0): \\(p^c := g^{\\text{locfit}}(p^u, 0)\\) scores recalibrated using local regression with degree 0\nLocal Reg. (deg = 1): \\(p^c := g^{\\text{locfit}}(p^u, 1)\\) scores recalibrated using local regression with degree 1\nLocal Reg. (deg = 2): \\(p^c := g^{\\text{locfit}}(p^u, 2)\\) scores recalibrated using local regression with degree 2.\n\n\nplot_boxplot_metric &lt;- function(metric, \n                                calib_metrics_simul,\n                                type) {\n  data_plot &lt;- calib_metrics_simul |&gt;\n    filter(metric == !!metric, type == !!type) |&gt; \n    arrange(transform_scale)\n  \n  methods &lt;- levels(data_plot$method)\n  labels_y &lt;- unique(data_plot$transform_scale) |&gt; round(2)\n  \n  \n  par(mfrow = c(4,2))\n  for (method in methods) {\n    data_plot_current &lt;- data_plot |&gt; filter(method == !!method)\n    # par(mar = c(2.1, 12.1, 2.1, 2.1))\n    par(mar = c(3.1, 4.1, 2.1, 2.1))\n    boxplot(\n      value ~ sample + transform_scale,\n      data = data_plot_current,\n      col = c(\"#D55E00\", \"#009E73\"),\n      horizontal = FALSE,\n      main = method,\n      las = 1, xlab = \"\", ylab = \"\",\n      xaxt = \"n\"\n    )\n    # ind_benchmark &lt;- which(labels_y == 1)\n    labs_y &lt;- str_c(\"$\\\\\", type, \"=\", labels_y, \"$\")\n    # labs_y[ind_benchmark] &lt;- str_c(labs_y[ind_benchmark], \" (benchmark)\")\n    axis(\n      side = 1, at = seq(1, 2*length(labels_y), by = 2) + .5, \n      labels = latex2exp::TeX(labs_y),\n      las = 1,\n      # col.axis = \"black\"\n    )\n    # # Horizontal lines\n    # for (i in seq(1, 2*(length(labels_y)-1), by = 2) + 1.5) {\n    #   abline(h = i, lty = 1, col = \"gray\")\n    # }\n    # Vertical lines\n    for (i in seq(1, 2*(length(labels_y)-1), by = 2) + 1.5) {\n      abline(v = i, lty = 1, col = \"gray\")\n    }\n  }\n}\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\nTrue MSEBrier ScoreECEQMSEWMSELCS\n\n\n\n\n\n\nDistribution of True MSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Brier Score computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of ECE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of QMSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of WMSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of LCS computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nTrue MSEBrier ScoreECEQMSEWMSELCS\n\n\n\n\n\n\nDistribution of True MSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Brier Score computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of ECE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of QMSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of WMSE computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nDistribution of LCS computed over the 200 replications, on the calibration set and on test set\n\n\n\n\n\n\n\n\n\n\n\nWe can visualize this in a different way.\n\nboxplot_std_metrics_calib &lt;- function(tb_calib_metrics, metric, x_lim = NULL) {\n  scale_parameters &lt;- unique(tb_calib_metrics$transform_scale)\n  nb &lt;- length(scale_parameters)\n  mat &lt;- mat_init &lt;- matrix(c(1:(2*nb), rep(2*nb+1, nb)), ncol = nb, byrow = TRUE)\n  \n  # colours_calib &lt;- c(\n  #   \"#332288\", \n  #   # \"#117733\", \n  #   \"#44AA99\", \"#88CCEE\",\n  #   \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\") |&gt; rev()\n  \n  colours_names &lt;- c(\n    \"True Prob.\",\n    \"No Calibration\", \"Platt\", \"Isotonic\", \"Beta\", \n    \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\"\n  )\n  colours_calib &lt;- colours_legend &lt;- c(\n    \"#332288\", \"#117733\", \"#44AA99\", \"#88CCEE\",\n    \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\"\n  )\n  colours_test &lt;- adjustcolor(colours_calib, alpha.f = .5)\n  \n  colours &lt;- NULL\n  for (k in 1:length(colours_calib)) \n    colours &lt;- c(colours, colours_calib[k], colours_test[k])\n  \n  \n  layout(mat, heights = c(.45, .45, .15))\n  \n  for (type in unique(tb_calib_metrics$type)) {\n    for (i_scale in 1:length(scale_parameters)) {\n      scale_parameter &lt;- scale_parameters[i_scale]\n      tb_metrics_current &lt;- tb_calib_metrics |&gt; \n        filter(\n          transform_scale == !!scale_parameter, \n          type == !!type,\n          metric == !!metric\n        ) |&gt; \n        mutate(\n          method = fct_rev(fct_drop(method))\n        )\n      title &lt;- latex2exp::TeX(\n        str_c(\"$\\\\\", type, \" = \", round(scale_parameter, 2), \"$\")\n      )\n      methods_bp &lt;- tb_metrics_current$method |&gt; levels()\n      \n      ind_colours &lt;- match(methods_bp, colours_names)\n      \n      colours &lt;- NULL\n      for (k in 1:length(colours_calib[ind_colours])) \n        colours &lt;- c(colours, colours_calib[ind_colours][k], \n                     colours_test[ind_colours][k])\n      \n      \n      form &lt;- str_c(\"value~sample + method\")\n      par(mar = c(1.5, 1.5, 3.1, 1))\n      boxplot(\n        formula(form), \n        data = tb_metrics_current |&gt; \n          mutate(\n            sample = fct_rev(sample)\n            # method = fct_rev(method)\n          ),\n        xlab = \"\",\n        ylab = \"\",\n        main = title,\n        horizontal = TRUE,\n        las = 1,\n        col = colours,\n        ylim = x_lim,\n        border = c(\"black\", adjustcolor(\"black\", alpha.f = .5)),\n        # sep = \", \"\n        yaxt = \"n\"\n      )\n      # Horizontal lines\n      for (i in seq(3, length(methods_bp) * 2, by = 2) - .5) {\n        abline(h = i, lty = 1, col = \"gray\")\n      }\n    }\n  }\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = colours_names,\n    fill = colours_legend,\n    # lwd = 2,\n    xpd = TRUE, ncol = 4\n  )\n}\n\n\n# Standard Metrics\nstandard_metrics &lt;- recalib_metrics_alpha |&gt; \n  bind_rows(recalib_metrics_gamma) |&gt; \n  # Without recalibration\n  bind_rows(\n    metrics_alpha |&gt; \n      mutate(method = \"No Calibration\")\n  ) |&gt; \n  bind_rows(\n    metrics_gamma |&gt; \n      mutate(method = \"No Calibration\")\n  ) |&gt; \n  filter(threshold == .5) |&gt; \n  rename(transform_scale = scale_parameter) |&gt; \n  select(\n    sample, seed, transform_scale, type, method, \n    mse, accuracy, sensitivity, specificity, auc\n  ) |&gt; \n  pivot_longer(\n    cols = c(mse, accuracy, sensitivity, specificity, auc), \n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\n# Calibration metrics\ncalib_metrics_simul_alpha &lt;- map(simul_recalib_alpha, \"calib_metrics\") |&gt; \n  list_rbind()\ncalib_metrics_simul_gamma &lt;- map(simul_recalib_gamma, \"calib_metrics\") |&gt; \n  list_rbind()\n\ncalib_metrics_simul &lt;- calib_metrics_simul_alpha |&gt; \n  bind_rows(calib_metrics_simul_gamma) |&gt; \n  pivot_longer(\n    cols = c(mse, brier, ece, qmse, wmse, lcs),\n    names_to = \"metric\", values_to = \"value\"\n  ) |&gt; \n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\", \"lcs\"),\n      labels = c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\", \"LCS\")\n    ),\n    sample = case_when(\n      sample == \"Calibration\"~\"calibration\",\n      sample == \"Test\"~\"test\",\n      TRUE~NA_character_\n    )\n  )\n\nmetrics_all &lt;- \n  calib_metrics_simul |&gt; \n  bind_rows(standard_metrics) |&gt; \n  mutate(\n    method = factor(\n      method,\n      levels = c(\"True Prob.\", \"No Calibration\",\n                 \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"),\n      labels = c(\n        \"True Prob.\",\n        \"No Calibration\", \"Platt\", \"Isotonic\", \"Beta\", \n        \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\")\n    ),\n    sample = factor(\n      sample, levels = c(\"calibration\", \"test\"), labels = c(\"Calibration\", \"Test\")\n    )\n  )\n\n\nTrue MSELCSBrier ScoreECEWMSEAccuracySensitivitySpecificityAUC\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"True MSE\"\n)\n\n\n\nFigure 2.35: True MSE on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"LCS\"\n)\n\n\n\nFigure 2.36: LCS on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"Brier Score\"\n)\n\n\n\nFigure 2.37: Brier Score on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"ECE\"\n)\n\n\n\nFigure 2.38: ECE on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"WMSE\"\n)\n\n\n\nFigure 2.39: WMSE on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"accuracy\"\n)\n\n\n\nFigure 2.40: Accuracy on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"sensitivity\"\n)\n\n\n\nFigure 2.41: Sensitivity on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"specificity\"\n)\n\n\n\nFigure 2.42: Specificity on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores.\n\n\n\n\n\n\n\n\nboxplot_std_metrics_calib(\n  tb_calib_metrics = metrics_all |&gt; \n    filter(transform_scale %in% c(1/3, 1, 3)),\n  metric = \"auc\"\n)\n\n\n\nFigure 2.43: AUC on 200 Simulations for each Value of \\(\\alpha\\) (top) or \\(\\gamma\\) (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). The metrics are computed for different definitions of the scores: using the true probabilities, the non calibrated scores, or the recalibrated scores."
  },
  {
    "objectID": "recalibration.html#summary-tables",
    "href": "recalibration.html#summary-tables",
    "title": "2  Recalibration",
    "section": "2.9 Summary Tables",
    "text": "2.9 Summary Tables\nLet us now report the results in tables. We will focus on a specific probability transformation, and for this transformation, show the computed metrics (in column) depending on the value of the predicted probability used (true probability, transformed probability without calibration, transformed probability with one of the recalibration method).\n\ntable_calib_metrics_all &lt;- \n  calib_metrics_simul |&gt; \n  group_by(method, sample, transform_scale, type, metric) |&gt; \n  summarise(\n    value_mean = mean(value, na.rm = TRUE),\n    value_sd = sd(value, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(value_mean = round(value_mean, 4), value_sd = round(value_sd, 4)) |&gt; \n  mutate(value = str_c(value_mean, \" (\", value_sd, \")\")) |&gt; \n  select(-value_mean, -value_sd) |&gt; \n  pivot_wider(\n    names_from = c(metric, sample), values_from = value, \n    names_sort = TRUE\n  )\n\nWe define a function to print a table depending on the transformation applied to the probabilities (varying either \\(\\alpha\\) or \\(\\gamma\\)).\n\nprint_table &lt;- function(transform_scale, type) {\n  table_calib_metrics_all |&gt; \n    filter(transform_scale == !!transform_scale, type == type) |&gt;\n    select(-type, -transform_scale) |&gt; \n    mutate(\n      across(\n        -method, \n        ~kableExtra::cell_spec(\n          .x, \n          color = ifelse(\n            .x == max(.x), yes = \"#882255\",\n            no = ifelse(.x == min(.x), \"#44AA99\", \"black\")\n          ),\n          bold = ifelse(\n            .x == max(.x), yes = TRUE,\n            no = ifelse(.x == min(.x), TRUE, FALSE)\n          )\n        )\n      )\n    ) |&gt; \n    knitr::kable(\n      escape = FALSE, booktabs = T, digits = 4,\n      format = \"html\",\n      col.names = c(\"Calibration Method\", rep(c(\"Calib.\", \"Test\"), 6))\n    ) |&gt;\n    kableExtra::kable_styling() |&gt;\n    kableExtra::add_header_above(\n      c(\n        \"\", \"MSE (True)\" = 2, \"Brier\" = 2, \"ECE\" = 2, \"QMSE\" = 2, \"WMSE\" = 2, \"LCS\" = 2\n      )\n    )\n}\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\n\\(\\alpha = 1/3\\)\\(\\alpha = 2/3\\)\\(\\alpha = 1.00\\)\\(\\alpha = 3/2\\)\\(\\alpha = 3\\)\n\n\n\n\n\n\nTable 2.1: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\alpha=0.33\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.083 (9e-04)\n0.0831 (0.001)\n0.3186 (0.0083)\n0.3185 (0.0098)\n0.2824 (0.0136)\n0.2826 (0.0166)\n0.0847 (0.0079)\n0.0859 (0.0096)\n0.0765 (0.0092)\n0.0777 (0.0116)\n0.0801 (0.0093)\n0.0827 (0.0118)\n\n\nNo Calibration\n0.006 (2e-04)\n0.006 (3e-04)\n0.2418 (0.0012)\n0.2416 (0.0013)\n0.0998 (0.0123)\n0.1035 (0.0136)\n0.0078 (0.0022)\n0.009 (0.0027)\n0.007 (0.0015)\n0.0083 (0.0018)\n0.0067 (0.0027)\n0.0079 (0.0034)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1032 (0.0173)\n0.1128 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2354 (0.0034)\n0.2359 (0.0037)\n0.1031 (0.0174)\n0.1126 (0.0153)\n0.0016 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0025 (7e-04)\n0.0038 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.0189)\n0.1093 (0.0156)\n0.0014 (6e-04)\n0.0039 (0.0016)\n0.005 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0999 (0.0179)\n0.1097 (0.0163)\n0.0014 (7e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0016 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1024 (0.0188)\n0.1136 (0.016)\n0.001 (6e-04)\n0.0037 (0.0016)\n0.006 (0.0013)\n0.0093 (0.0036)\n0.0029 (0.0017)\n0.0053 (0.0034)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.237 (0.004)\n0.1021 (0.0177)\n0.1135 (0.0164)\n0.001 (6e-04)\n0.0037 (0.0017)\n0.006 (0.0012)\n0.0094 (0.0035)\n0.0031 (0.0018)\n0.0055 (0.0033)\n\n\nlocfit_2\n0.003 (9e-04)\n0.0031 (0.001)\n0.2326 (0.0034)\n0.2384 (0.0044)\n0.106 (0.0174)\n0.1154 (0.0165)\n9e-04 (6e-04)\n0.0042 (0.0019)\n0.0054 (0.001)\n0.0108 (0.0038)\n0.0029 (0.0015)\n0.0066 (0.0036)\n\n\nlocfit_2\n0.003 (0.001)\n0.0031 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1057 (0.0178)\n0.1154 (0.0165)\n9e-04 (5e-04)\n0.0042 (0.0021)\n0.0054 (9e-04)\n0.0109 (0.0039)\n0.0031 (0.0015)\n0.0069 (0.0037)\n\n\nplatt\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2357 (0.0034)\n0.236 (0.0037)\n0.105 (0.017)\n0.1141 (0.0146)\n0.0018 (9e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0081 (0.0029)\n0.0024 (8e-04)\n0.0036 (0.0021)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0033)\n0.2359 (0.0037)\n0.1033 (0.0173)\n0.1129 (0.0153)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0025 (7e-04)\n0.0037 (0.0021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.2: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\alpha=0.67\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.0157 (1e-04)\n0.0157 (1e-04)\n0.2515 (0.0047)\n0.2512 (0.0052)\n0.1313 (0.0138)\n0.1312 (0.0162)\n0.0176 (0.0035)\n0.0187 (0.0043)\n0.0196 (0.0039)\n0.0211 (0.0049)\n0.0163 (0.0036)\n0.0172 (0.0044)\n\n\nNo Calibration\n0.0014 (1e-04)\n0.0014 (1e-04)\n0.2372 (0.0023)\n0.2369 (0.0026)\n0.099 (0.0115)\n0.1037 (0.0127)\n0.0033 (0.0013)\n0.0044 (0.0018)\n0.005 (0.001)\n0.0065 (0.0017)\n0.0021 (9e-04)\n0.0028 (0.0013)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1032 (0.0172)\n0.1128 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2354 (0.0034)\n0.2359 (0.0037)\n0.103 (0.0173)\n0.1125 (0.0152)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0024 (7e-04)\n0.0039 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.0189)\n0.1089 (0.016)\n0.0014 (7e-04)\n0.0038 (0.0017)\n0.0051 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.019)\n0.1096 (0.0162)\n0.0014 (7e-04)\n0.0039 (0.0017)\n0.005 (0.0011)\n0.0083 (0.003)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1029 (0.0187)\n0.1135 (0.0162)\n0.001 (5e-04)\n0.0037 (0.0016)\n0.006 (0.0012)\n0.0092 (0.0036)\n0.0029 (0.0017)\n0.0051 (0.0033)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0016 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1022 (0.0183)\n0.1136 (0.0164)\n0.001 (6e-04)\n0.0037 (0.0017)\n0.006 (0.0013)\n0.0093 (0.0034)\n0.003 (0.0018)\n0.0054 (0.0033)\n\n\nlocfit_2\n0.003 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2384 (0.0044)\n0.1067 (0.0175)\n0.115 (0.0169)\n9e-04 (5e-04)\n0.0041 (0.002)\n0.0054 (0.001)\n0.0108 (0.0039)\n0.0029 (0.0015)\n0.0066 (0.0037)\n\n\nlocfit_2\n0.003 (0.001)\n0.0031 (0.001)\n0.2326 (0.0034)\n0.2384 (0.0045)\n0.1059 (0.0179)\n0.1153 (0.0165)\n9e-04 (5e-04)\n0.0042 (0.0018)\n0.0054 (9e-04)\n0.0108 (0.004)\n0.003 (0.0015)\n0.0066 (0.0038)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0034)\n0.2359 (0.0037)\n0.1048 (0.017)\n0.1135 (0.0147)\n0.0017 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0081 (0.003)\n0.0021 (7e-04)\n0.0033 (0.002)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0033)\n0.2359 (0.0037)\n0.1036 (0.0174)\n0.1132 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0022 (7e-04)\n0.0035 (0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\alpha=1\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0016)\n\n\nNo Calibration\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0016)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1031 (0.0173)\n0.1127 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1031 (0.0173)\n0.1127 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0037)\n0.099 (0.0182)\n0.1092 (0.016)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0037)\n0.099 (0.0182)\n0.1092 (0.016)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1016 (0.0184)\n0.1134 (0.0161)\n9e-04 (6e-04)\n0.0038 (0.0018)\n0.0059 (0.0012)\n0.0092 (0.0035)\n0.0028 (0.0016)\n0.005 (0.0032)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1016 (0.0184)\n0.1134 (0.0161)\n9e-04 (6e-04)\n0.0038 (0.0018)\n0.0059 (0.0012)\n0.0092 (0.0035)\n0.0028 (0.0016)\n0.005 (0.0032)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1058 (0.0179)\n0.1147 (0.0167)\n9e-04 (6e-04)\n0.0041 (0.0018)\n0.0054 (9e-04)\n0.0107 (0.0039)\n0.0029 (0.0014)\n0.0065 (0.0037)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1058 (0.0179)\n0.1147 (0.0167)\n9e-04 (6e-04)\n0.0041 (0.0018)\n0.0054 (9e-04)\n0.0107 (0.0039)\n0.0029 (0.0014)\n0.0065 (0.0037)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0034)\n0.2359 (0.0038)\n0.104 (0.0174)\n0.1136 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0019 (6e-04)\n0.0032 (0.0018)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0034)\n0.2359 (0.0038)\n0.104 (0.0174)\n0.1136 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0019 (6e-04)\n0.0032 (0.0018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\alpha=1.5\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.0192 (1e-04)\n0.0192 (1e-04)\n0.2553 (0.005)\n0.2548 (0.0061)\n0.1927 (0.0136)\n0.1954 (0.0167)\n0.0214 (0.004)\n0.0222 (0.005)\n0.0251 (0.0044)\n0.0256 (0.0058)\n0.0215 (0.0041)\n0.0216 (0.0052)\n\n\nNo Calibration\n0.0025 (1e-04)\n0.0025 (1e-04)\n0.2384 (0.0047)\n0.2379 (0.0051)\n0.1342 (0.0122)\n0.1384 (0.0142)\n0.0045 (0.0016)\n0.0053 (0.0021)\n0.0117 (0.0028)\n0.0129 (0.0037)\n0.008 (0.0024)\n0.0084 (0.003)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1028 (0.0175)\n0.1127 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0024 (6e-04)\n0.004 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1029 (0.0176)\n0.1129 (0.0155)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0023 (6e-04)\n0.004 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0999 (0.019)\n0.1098 (0.0159)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0016 (7e-04)\n0.0017 (7e-04)\n0.2349 (0.0033)\n0.2371 (0.0036)\n0.0995 (0.0184)\n0.1099 (0.0159)\n0.0013 (7e-04)\n0.0039 (0.0017)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.237 (0.004)\n0.1022 (0.0182)\n0.114 (0.0163)\n0.001 (5e-04)\n0.0038 (0.0019)\n0.0059 (0.0012)\n0.0093 (0.0037)\n0.0027 (0.0016)\n0.005 (0.0033)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1023 (0.0175)\n0.1138 (0.0162)\n9e-04 (5e-04)\n0.0038 (0.0018)\n0.006 (0.0013)\n0.0092 (0.0034)\n0.0024 (0.0016)\n0.0047 (0.0029)\n\n\nlocfit_2\n0.0029 (9e-04)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2382 (0.0044)\n0.1062 (0.0176)\n0.1157 (0.0159)\n9e-04 (5e-04)\n0.0042 (0.0019)\n0.0054 (9e-04)\n0.0108 (0.004)\n0.0028 (0.0015)\n0.0065 (0.0037)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2382 (0.0044)\n0.1068 (0.0181)\n0.1156 (0.0173)\n9e-04 (6e-04)\n0.004 (0.0019)\n0.0054 (9e-04)\n0.0106 (0.0039)\n0.0027 (0.0015)\n0.0062 (0.0037)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0033)\n0.236 (0.0037)\n0.1035 (0.0176)\n0.1126 (0.0159)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.0029)\n0.0018 (6e-04)\n0.0032 (0.0017)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0034)\n0.236 (0.0038)\n0.1047 (0.0175)\n0.1142 (0.0149)\n0.0017 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0081 (0.003)\n0.0015 (5e-04)\n0.0028 (0.0017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.5: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\alpha=3\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.1281 (7e-04)\n0.128 (8e-04)\n0.3642 (0.01)\n0.3637 (0.0122)\n0.3459 (0.0141)\n0.3476 (0.0163)\n0.1302 (0.0099)\n0.131 (0.012)\n0.1196 (0.0098)\n0.1183 (0.0117)\n0.1211 (0.0099)\n0.1204 (0.0119)\n\n\nNo Calibration\n0.0243 (6e-04)\n0.0243 (6e-04)\n0.2604 (0.0076)\n0.2596 (0.0084)\n0.2253 (0.0136)\n0.227 (0.0157)\n0.0263 (0.0045)\n0.0268 (0.0051)\n0.031 (0.0049)\n0.0312 (0.0059)\n0.0294 (0.0048)\n0.0294 (0.0058)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1025 (0.0176)\n0.1124 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0026 (7e-04)\n0.0042 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n6e-04 (4e-04)\n0.2353 (0.0034)\n0.2361 (0.0037)\n0.1027 (0.0177)\n0.113 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0023 (7e-04)\n0.004 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2371 (0.0036)\n0.1013 (0.0189)\n0.1106 (0.0161)\n0.0013 (7e-04)\n0.0039 (0.0017)\n0.0051 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0016 (6e-04)\n0.0017 (6e-04)\n0.235 (0.0033)\n0.2371 (0.0038)\n0.1001 (0.0184)\n0.1111 (0.0159)\n0.0012 (6e-04)\n0.0039 (0.0018)\n0.0052 (0.001)\n0.0085 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1018 (0.0176)\n0.1143 (0.0161)\n9e-04 (5e-04)\n0.0037 (0.0018)\n0.0061 (0.0013)\n0.0092 (0.0035)\n0.0025 (0.0015)\n0.0047 (0.003)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2344 (0.0034)\n0.2369 (0.0041)\n0.1027 (0.0185)\n0.1143 (0.0161)\n0.001 (6e-04)\n0.0038 (0.0017)\n0.0061 (0.0013)\n0.0093 (0.0035)\n0.0015 (0.001)\n0.0037 (0.0024)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1067 (0.0185)\n0.1154 (0.0169)\n9e-04 (5e-04)\n0.0042 (0.0019)\n0.0054 (0.001)\n0.0108 (0.0039)\n0.0027 (0.0014)\n0.0063 (0.0036)\n\n\nlocfit_2\n0.0029 (0.001)\n0.0029 (0.001)\n0.2327 (0.0034)\n0.2381 (0.0043)\n0.1069 (0.0179)\n0.1154 (0.0177)\n9e-04 (5e-04)\n0.0042 (0.0018)\n0.0055 (0.001)\n0.0107 (0.0039)\n0.002 (0.0013)\n0.0054 (0.0033)\n\n\nplatt\n0.0011 (4e-04)\n0.0012 (4e-04)\n0.2363 (0.0032)\n0.2367 (0.0036)\n0.1013 (0.0184)\n0.1083 (0.0165)\n0.0023 (0.001)\n0.004 (0.0018)\n0.0059 (0.001)\n0.0081 (0.0027)\n0.0028 (0.0011)\n0.0043 (0.0019)\n\n\nplatt\n8e-04 (4e-04)\n8e-04 (4e-04)\n0.236 (0.0034)\n0.2364 (0.0038)\n0.1077 (0.0176)\n0.1165 (0.0145)\n0.0019 (9e-04)\n0.0037 (0.0018)\n0.0059 (0.001)\n0.0081 (0.0029)\n0.001 (5e-04)\n0.0024 (0.0015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\gamma = 1/3\\)\\(\\gamma = 2/3\\)\\(\\gamma = 1\\)\\(\\gamma = 3/2\\)\\(\\gamma = 3\\)\n\n\n\n\n\n\nTable 2.6: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\gamma=0.33\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.083 (9e-04)\n0.0831 (0.001)\n0.3186 (0.0083)\n0.3185 (0.0098)\n0.2824 (0.0136)\n0.2826 (0.0166)\n0.0847 (0.0079)\n0.0859 (0.0096)\n0.0765 (0.0092)\n0.0777 (0.0116)\n0.0801 (0.0093)\n0.0827 (0.0118)\n\n\nNo Calibration\n0.006 (2e-04)\n0.006 (3e-04)\n0.2418 (0.0012)\n0.2416 (0.0013)\n0.0998 (0.0123)\n0.1035 (0.0136)\n0.0078 (0.0022)\n0.009 (0.0027)\n0.007 (0.0015)\n0.0083 (0.0018)\n0.0067 (0.0027)\n0.0079 (0.0034)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1032 (0.0173)\n0.1128 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2354 (0.0034)\n0.2359 (0.0037)\n0.1031 (0.0174)\n0.1126 (0.0153)\n0.0016 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0025 (7e-04)\n0.0038 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.0189)\n0.1093 (0.0156)\n0.0014 (6e-04)\n0.0039 (0.0016)\n0.005 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0999 (0.0179)\n0.1097 (0.0163)\n0.0014 (7e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0016 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1024 (0.0188)\n0.1136 (0.016)\n0.001 (6e-04)\n0.0037 (0.0016)\n0.006 (0.0013)\n0.0093 (0.0036)\n0.0029 (0.0017)\n0.0053 (0.0034)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.237 (0.004)\n0.1021 (0.0177)\n0.1135 (0.0164)\n0.001 (6e-04)\n0.0037 (0.0017)\n0.006 (0.0012)\n0.0094 (0.0035)\n0.0031 (0.0018)\n0.0055 (0.0033)\n\n\nlocfit_2\n0.003 (9e-04)\n0.0031 (0.001)\n0.2326 (0.0034)\n0.2384 (0.0044)\n0.106 (0.0174)\n0.1154 (0.0165)\n9e-04 (6e-04)\n0.0042 (0.0019)\n0.0054 (0.001)\n0.0108 (0.0038)\n0.0029 (0.0015)\n0.0066 (0.0036)\n\n\nlocfit_2\n0.003 (0.001)\n0.0031 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1057 (0.0178)\n0.1154 (0.0165)\n9e-04 (5e-04)\n0.0042 (0.0021)\n0.0054 (9e-04)\n0.0109 (0.0039)\n0.0031 (0.0015)\n0.0069 (0.0037)\n\n\nplatt\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2357 (0.0034)\n0.236 (0.0037)\n0.105 (0.017)\n0.1141 (0.0146)\n0.0018 (9e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0081 (0.0029)\n0.0024 (8e-04)\n0.0036 (0.0021)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0033)\n0.2359 (0.0037)\n0.1033 (0.0173)\n0.1129 (0.0153)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0025 (7e-04)\n0.0037 (0.0021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.7: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\gamma=0.67\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.0157 (1e-04)\n0.0157 (1e-04)\n0.2515 (0.0047)\n0.2512 (0.0052)\n0.1313 (0.0138)\n0.1312 (0.0162)\n0.0176 (0.0035)\n0.0187 (0.0043)\n0.0196 (0.0039)\n0.0211 (0.0049)\n0.0163 (0.0036)\n0.0172 (0.0044)\n\n\nNo Calibration\n0.0014 (1e-04)\n0.0014 (1e-04)\n0.2372 (0.0023)\n0.2369 (0.0026)\n0.099 (0.0115)\n0.1037 (0.0127)\n0.0033 (0.0013)\n0.0044 (0.0018)\n0.005 (0.001)\n0.0065 (0.0017)\n0.0021 (9e-04)\n0.0028 (0.0013)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1032 (0.0172)\n0.1128 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2354 (0.0034)\n0.2359 (0.0037)\n0.103 (0.0173)\n0.1125 (0.0152)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0024 (7e-04)\n0.0039 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.0189)\n0.1089 (0.016)\n0.0014 (7e-04)\n0.0038 (0.0017)\n0.0051 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0998 (0.019)\n0.1096 (0.0162)\n0.0014 (7e-04)\n0.0039 (0.0017)\n0.005 (0.0011)\n0.0083 (0.003)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1029 (0.0187)\n0.1135 (0.0162)\n0.001 (5e-04)\n0.0037 (0.0016)\n0.006 (0.0012)\n0.0092 (0.0036)\n0.0029 (0.0017)\n0.0051 (0.0033)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0016 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1022 (0.0183)\n0.1136 (0.0164)\n0.001 (6e-04)\n0.0037 (0.0017)\n0.006 (0.0013)\n0.0093 (0.0034)\n0.003 (0.0018)\n0.0054 (0.0033)\n\n\nlocfit_2\n0.003 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2384 (0.0044)\n0.1067 (0.0175)\n0.115 (0.0169)\n9e-04 (5e-04)\n0.0041 (0.002)\n0.0054 (0.001)\n0.0108 (0.0039)\n0.0029 (0.0015)\n0.0066 (0.0037)\n\n\nlocfit_2\n0.003 (0.001)\n0.0031 (0.001)\n0.2326 (0.0034)\n0.2384 (0.0045)\n0.1059 (0.0179)\n0.1153 (0.0165)\n9e-04 (5e-04)\n0.0042 (0.0018)\n0.0054 (9e-04)\n0.0108 (0.004)\n0.003 (0.0015)\n0.0066 (0.0038)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0034)\n0.2359 (0.0037)\n0.1048 (0.017)\n0.1135 (0.0147)\n0.0017 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0081 (0.003)\n0.0021 (7e-04)\n0.0033 (0.002)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0033)\n0.2359 (0.0037)\n0.1036 (0.0174)\n0.1132 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0022 (7e-04)\n0.0035 (0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.8: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\gamma=1\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0016)\n\n\nNo Calibration\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0016)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1031 (0.0173)\n0.1127 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1031 (0.0173)\n0.1127 (0.0153)\n0.0015 (7e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0082 (0.003)\n0.0024 (6e-04)\n0.0039 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0037)\n0.099 (0.0182)\n0.1092 (0.016)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0037)\n0.099 (0.0182)\n0.1092 (0.016)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1016 (0.0184)\n0.1134 (0.0161)\n9e-04 (6e-04)\n0.0038 (0.0018)\n0.0059 (0.0012)\n0.0092 (0.0035)\n0.0028 (0.0016)\n0.005 (0.0032)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1016 (0.0184)\n0.1134 (0.0161)\n9e-04 (6e-04)\n0.0038 (0.0018)\n0.0059 (0.0012)\n0.0092 (0.0035)\n0.0028 (0.0016)\n0.005 (0.0032)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1058 (0.0179)\n0.1147 (0.0167)\n9e-04 (6e-04)\n0.0041 (0.0018)\n0.0054 (9e-04)\n0.0107 (0.0039)\n0.0029 (0.0014)\n0.0065 (0.0037)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1058 (0.0179)\n0.1147 (0.0167)\n9e-04 (6e-04)\n0.0041 (0.0018)\n0.0054 (9e-04)\n0.0107 (0.0039)\n0.0029 (0.0014)\n0.0065 (0.0037)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0034)\n0.2359 (0.0038)\n0.104 (0.0174)\n0.1136 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0019 (6e-04)\n0.0032 (0.0018)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2355 (0.0034)\n0.2359 (0.0038)\n0.104 (0.0174)\n0.1136 (0.0152)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.003)\n0.0019 (6e-04)\n0.0032 (0.0018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.9: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\gamma=1.5\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.0192 (1e-04)\n0.0192 (1e-04)\n0.2553 (0.005)\n0.2548 (0.0061)\n0.1927 (0.0136)\n0.1954 (0.0167)\n0.0214 (0.004)\n0.0222 (0.005)\n0.0251 (0.0044)\n0.0256 (0.0058)\n0.0215 (0.0041)\n0.0216 (0.0052)\n\n\nNo Calibration\n0.0025 (1e-04)\n0.0025 (1e-04)\n0.2384 (0.0047)\n0.2379 (0.0051)\n0.1342 (0.0122)\n0.1384 (0.0142)\n0.0045 (0.0016)\n0.0053 (0.0021)\n0.0117 (0.0028)\n0.0129 (0.0037)\n0.008 (0.0024)\n0.0084 (0.003)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1028 (0.0175)\n0.1127 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0024 (6e-04)\n0.004 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1029 (0.0176)\n0.1129 (0.0155)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0023 (6e-04)\n0.004 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2372 (0.0036)\n0.0999 (0.019)\n0.1098 (0.0159)\n0.0013 (6e-04)\n0.0039 (0.0018)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0016 (7e-04)\n0.0017 (7e-04)\n0.2349 (0.0033)\n0.2371 (0.0036)\n0.0995 (0.0184)\n0.1099 (0.0159)\n0.0013 (7e-04)\n0.0039 (0.0017)\n0.005 (0.0011)\n0.0083 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.237 (0.004)\n0.1022 (0.0182)\n0.114 (0.0163)\n0.001 (5e-04)\n0.0038 (0.0019)\n0.0059 (0.0012)\n0.0093 (0.0037)\n0.0027 (0.0016)\n0.005 (0.0033)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1023 (0.0175)\n0.1138 (0.0162)\n9e-04 (5e-04)\n0.0038 (0.0018)\n0.006 (0.0013)\n0.0092 (0.0034)\n0.0024 (0.0016)\n0.0047 (0.0029)\n\n\nlocfit_2\n0.0029 (9e-04)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2382 (0.0044)\n0.1062 (0.0176)\n0.1157 (0.0159)\n9e-04 (5e-04)\n0.0042 (0.0019)\n0.0054 (9e-04)\n0.0108 (0.004)\n0.0028 (0.0015)\n0.0065 (0.0037)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2382 (0.0044)\n0.1068 (0.0181)\n0.1156 (0.0173)\n9e-04 (6e-04)\n0.004 (0.0019)\n0.0054 (9e-04)\n0.0106 (0.0039)\n0.0027 (0.0015)\n0.0062 (0.0037)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0033)\n0.236 (0.0037)\n0.1035 (0.0176)\n0.1126 (0.0159)\n0.0017 (8e-04)\n0.0034 (0.0016)\n0.006 (0.001)\n0.0081 (0.0029)\n0.0018 (6e-04)\n0.0032 (0.0017)\n\n\nplatt\n4e-04 (4e-04)\n4e-04 (4e-04)\n0.2356 (0.0034)\n0.236 (0.0038)\n0.1047 (0.0175)\n0.1142 (0.0149)\n0.0017 (8e-04)\n0.0034 (0.0017)\n0.006 (0.001)\n0.0081 (0.003)\n0.0015 (5e-04)\n0.0028 (0.0017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.10: Average value for the calibration metrics (in column) over the 200 replications, for \\(\\gamma=3\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n0.1281 (7e-04)\n0.128 (8e-04)\n0.3642 (0.01)\n0.3637 (0.0122)\n0.3459 (0.0141)\n0.3476 (0.0163)\n0.1302 (0.0099)\n0.131 (0.012)\n0.1196 (0.0098)\n0.1183 (0.0117)\n0.1211 (0.0099)\n0.1204 (0.0119)\n\n\nNo Calibration\n0.0243 (6e-04)\n0.0243 (6e-04)\n0.2604 (0.0076)\n0.2596 (0.0084)\n0.2253 (0.0136)\n0.227 (0.0157)\n0.0263 (0.0045)\n0.0268 (0.0051)\n0.031 (0.0049)\n0.0312 (0.0059)\n0.0294 (0.0048)\n0.0294 (0.0058)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nTrue Prob.\n0 (0)\n0 (0)\n0.2359 (0.0033)\n0.2355 (0.0037)\n0.1056 (0.0105)\n0.1113 (0.0126)\n0.002 (8e-04)\n0.003 (0.0014)\n0.0065 (0.0017)\n0.0079 (0.0026)\n0.003 (0.0012)\n0.0035 (0.0017)\n\n\nbeta\n5e-04 (4e-04)\n5e-04 (4e-04)\n0.2354 (0.0034)\n0.236 (0.0037)\n0.1025 (0.0176)\n0.1124 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0026 (7e-04)\n0.0042 (0.0021)\n\n\nbeta\n5e-04 (4e-04)\n6e-04 (4e-04)\n0.2353 (0.0034)\n0.2361 (0.0037)\n0.1027 (0.0177)\n0.113 (0.0154)\n0.0015 (7e-04)\n0.0035 (0.0017)\n0.006 (0.001)\n0.0082 (0.0029)\n0.0023 (7e-04)\n0.004 (0.0021)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nisotonic\n0.0019 (6e-04)\n0.0019 (7e-04)\n0.2314 (0.0036)\n0.2373 (0.0043)\n0.0935 (0.0185)\n0.1134 (0.0164)\n0 (0)\n0.0033 (0.0018)\n0.006 (0.0012)\n0.0102 (0.0036)\n0.0053 (0.0015)\n0.0085 (0.0037)\n\n\nlocfit_0\n0.0017 (7e-04)\n0.0017 (7e-04)\n0.235 (0.0033)\n0.2371 (0.0036)\n0.1013 (0.0189)\n0.1106 (0.0161)\n0.0013 (7e-04)\n0.0039 (0.0017)\n0.0051 (0.0011)\n0.0082 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0015)\n\n\nlocfit_0\n0.0016 (6e-04)\n0.0017 (6e-04)\n0.235 (0.0033)\n0.2371 (0.0038)\n0.1001 (0.0184)\n0.1111 (0.0159)\n0.0012 (6e-04)\n0.0039 (0.0018)\n0.0052 (0.001)\n0.0085 (0.0031)\n6e-04 (3e-04)\n0.0026 (0.0016)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2342 (0.0034)\n0.2369 (0.004)\n0.1018 (0.0176)\n0.1143 (0.0161)\n9e-04 (5e-04)\n0.0037 (0.0018)\n0.0061 (0.0013)\n0.0092 (0.0035)\n0.0025 (0.0015)\n0.0047 (0.003)\n\n\nlocfit_1\n0.0015 (6e-04)\n0.0015 (6e-04)\n0.2344 (0.0034)\n0.2369 (0.0041)\n0.1027 (0.0185)\n0.1143 (0.0161)\n0.001 (6e-04)\n0.0038 (0.0017)\n0.0061 (0.0013)\n0.0093 (0.0035)\n0.0015 (0.001)\n0.0037 (0.0024)\n\n\nlocfit_2\n0.0029 (0.001)\n0.003 (0.001)\n0.2327 (0.0034)\n0.2383 (0.0044)\n0.1067 (0.0185)\n0.1154 (0.0169)\n9e-04 (5e-04)\n0.0042 (0.0019)\n0.0054 (0.001)\n0.0108 (0.0039)\n0.0027 (0.0014)\n0.0063 (0.0036)\n\n\nlocfit_2\n0.0029 (0.001)\n0.0029 (0.001)\n0.2327 (0.0034)\n0.2381 (0.0043)\n0.1069 (0.0179)\n0.1154 (0.0177)\n9e-04 (5e-04)\n0.0042 (0.0018)\n0.0055 (0.001)\n0.0107 (0.0039)\n0.002 (0.0013)\n0.0054 (0.0033)\n\n\nplatt\n0.0011 (4e-04)\n0.0012 (4e-04)\n0.2363 (0.0032)\n0.2367 (0.0036)\n0.1013 (0.0184)\n0.1083 (0.0165)\n0.0023 (0.001)\n0.004 (0.0018)\n0.0059 (0.001)\n0.0081 (0.0027)\n0.0028 (0.0011)\n0.0043 (0.0019)\n\n\nplatt\n8e-04 (4e-04)\n8e-04 (4e-04)\n0.236 (0.0034)\n0.2364 (0.0038)\n0.1077 (0.0176)\n0.1165 (0.0145)\n0.0019 (9e-04)\n0.0037 (0.0018)\n0.0059 (0.001)\n0.0081 (0.0029)\n0.001 (5e-04)\n0.0024 (0.0015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let us normalize the values. We use the calibration metric computed with the uncalibrated estimated probabilities as the reference value and express the metrics computed after recalibration of the scores as deviations from that reference.\n\ntable_calib_metrics_all_rel &lt;- \n  calib_metrics_simul |&gt; \n  filter(!method == \"True Prob.\") |&gt; \n  mutate(\n    reference = ifelse(method == \"No Calibration\", yes = value, no = NA)\n  ) |&gt; \n  group_by(sample, transform_scale, type, metric, seed) |&gt; \n  mutate(\n    reference = sum(reference, na.rm = TRUE),\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(\n    value_norm = value / reference\n  ) |&gt; \n  mutate(\n    value_norm = ifelse(value == 0 & reference == 0, yes = 1, no = value_norm)\n  ) |&gt; \n  group_by(method, sample, transform_scale, type, metric) |&gt; \n  summarise(\n    value_norm_mean = mean(value_norm, na.rm = TRUE),\n    value_norm_sd = sd(value_norm, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    value_norm_mean = round(value_norm_mean, 4), \n    value_norm_sd = round(value_norm_sd, 4)\n  ) |&gt; \n  mutate(value = str_c(value_norm_mean, \" (\", value_norm_sd, \")\")) |&gt; \n  select(-value_norm_mean, -value_norm_sd) |&gt; \n  pivot_wider(\n    names_from = c(metric, sample), values_from = value, \n    names_sort = TRUE\n  )\n\nWe define a function to print a table depending on the transformation applied to the probabilities (varying either \\(\\alpha\\) or \\(\\gamma\\)).\n\nprint_table_rel &lt;- function(transform_scale, type) {\n  table_calib_metrics_all_rel |&gt; \n    filter(transform_scale == !!transform_scale, type == type) |&gt;\n    select(-type, -transform_scale) |&gt; \n    mutate(\n      across(\n        -method, \n        ~kableExtra::cell_spec(\n          .x, \n          color = ifelse(\n            .x == max(.x), yes = \"#882255\",\n            no = ifelse(.x == min(.x), \"#44AA99\", \"black\")\n          ),\n          bold = ifelse(\n            .x == max(.x), yes = TRUE,\n            no = ifelse(.x == min(.x), TRUE, FALSE)\n          )\n        )\n      )\n    ) |&gt; \n    knitr::kable(\n      escape = FALSE, booktabs = T, digits = 4,\n      format = \"html\",\n      col.names = c(\"Calibration Method\", rep(c(\"Calib.\", \"Test\"), 6))\n    ) |&gt;\n    kableExtra::kable_styling() |&gt;\n    kableExtra::add_header_above(\n      c(\n        \"\", \"MSE (True)\" = 2, \"Brier\" = 2, \"ECE\" = 2, \"QMSE\" = 2, \"WMSE\" = 2, \"LCS\" = 2\n      )\n    )\n}\n\n\nVarying \\(\\alpha\\)Varying \\(\\gamma\\)\n\n\n\n\\(\\alpha = 1/3\\)\\(\\alpha = 2/3\\)\\(\\alpha = 1\\)\\(\\alpha = 3/2\\)\\(\\alpha = 3\\)\n\n\n\n\n\n\nTable 2.11: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\alpha=0.33\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0055 (0.0046)\n0.0055 (0.0045)\n0.7392 (0.0187)\n0.7415 (0.0232)\n0.3645 (0.0519)\n0.3991 (0.0499)\n0.0182 (0.0086)\n0.0404 (0.0206)\n0.0797 (0.0163)\n0.1092 (0.048)\n0.0299 (0.0093)\n0.0487 (0.0313)\n\n\nbeta\n0.0658 (0.0616)\n0.0654 (0.0597)\n0.9736 (0.0091)\n0.9762 (0.0104)\n1.0342 (0.1295)\n1.0961 (0.1374)\n0.2131 (0.0987)\n0.3903 (0.1962)\n0.9097 (0.2538)\n1.0229 (0.4015)\n0.46 (0.3116)\n0.67 (0.6865)\n\n\nisotonic\n0.0228 (0.0077)\n0.0232 (0.0081)\n0.7267 (0.0186)\n0.7457 (0.0239)\n0.3296 (0.0556)\n0.4012 (0.0524)\n0 (0)\n0.0386 (0.0223)\n0.0797 (0.0184)\n0.1357 (0.0569)\n0.0676 (0.0226)\n0.1062 (0.0542)\n\n\nisotonic\n0.3141 (0.1088)\n0.3191 (0.1093)\n0.957 (0.0101)\n0.982 (0.0132)\n0.9359 (0.1396)\n1.1067 (0.1738)\n0 (0)\n0.4025 (0.2873)\n0.9113 (0.2842)\n1.2907 (0.5483)\n1.0123 (0.7162)\n1.491 (1.6222)\n\n\nlocfit_0\n0.0205 (0.0085)\n0.0207 (0.0085)\n0.7379 (0.0187)\n0.7453 (0.0231)\n0.3522 (0.057)\n0.387 (0.0518)\n0.0161 (0.0072)\n0.0463 (0.0199)\n0.0665 (0.0164)\n0.1094 (0.0497)\n0.0073 (0.004)\n0.0315 (0.0194)\n\n\nlocfit_0\n0.2816 (0.1183)\n0.2821 (0.1158)\n0.9718 (0.0092)\n0.9815 (0.0104)\n1.0027 (0.1415)\n1.0679 (0.1574)\n0.1881 (0.1159)\n0.4606 (0.2375)\n0.7556 (0.247)\n1.0281 (0.424)\n0.1023 (0.0808)\n0.3982 (0.3399)\n\n\nlocfit_1\n0.0186 (0.0077)\n0.0187 (0.0078)\n0.7355 (0.0187)\n0.7445 (0.0234)\n0.3612 (0.057)\n0.4018 (0.0513)\n0.0117 (0.007)\n0.0439 (0.0194)\n0.0796 (0.0204)\n0.1237 (0.0558)\n0.0366 (0.0222)\n0.0659 (0.0463)\n\n\nlocfit_1\n0.256 (0.106)\n0.2558 (0.1039)\n0.9684 (0.0094)\n0.9806 (0.0118)\n1.0247 (0.1403)\n1.1067 (0.1661)\n0.1376 (0.0933)\n0.4466 (0.2642)\n0.9121 (0.2943)\n1.1822 (0.5038)\n0.6195 (0.6098)\n0.9626 (1.1471)\n\n\nlocfit_2\n0.0356 (0.0114)\n0.0367 (0.0124)\n0.7305 (0.0184)\n0.749 (0.0241)\n0.3743 (0.0524)\n0.4083 (0.0525)\n0.011 (0.0069)\n0.05 (0.0247)\n0.0719 (0.0158)\n0.1434 (0.0597)\n0.0369 (0.0194)\n0.0827 (0.0506)\n\n\nlocfit_2\n0.4947 (0.1607)\n0.5096 (0.1716)\n0.9622 (0.0097)\n0.9862 (0.0139)\n1.0617 (0.14)\n1.1277 (0.1831)\n0.1272 (0.0891)\n0.5218 (0.3434)\n0.8236 (0.2388)\n1.3761 (0.5756)\n0.5872 (0.4857)\n1.1979 (1.2691)\n\n\nplatt\n0.0062 (0.0044)\n0.0062 (0.0044)\n0.74 (0.0187)\n0.7416 (0.0231)\n0.3708 (0.0509)\n0.4039 (0.0482)\n0.0209 (0.0102)\n0.0409 (0.0207)\n0.0803 (0.0175)\n0.1083 (0.0484)\n0.031 (0.0131)\n0.0462 (0.0332)\n\n\nplatt\n0.0619 (0.0617)\n0.0616 (0.0601)\n0.974 (0.0091)\n0.9761 (0.0104)\n1.0361 (0.1287)\n1.0987 (0.1373)\n0.2223 (0.1025)\n0.3882 (0.1954)\n0.9089 (0.2517)\n1.0174 (0.3997)\n0.4558 (0.2903)\n0.6499 (0.6753)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.12: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\alpha=0.67\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0297 (0.0239)\n0.0298 (0.0237)\n0.936 (0.0126)\n0.9397 (0.016)\n0.786 (0.103)\n0.8671 (0.1265)\n0.0884 (0.0407)\n0.1916 (0.1057)\n0.316 (0.0687)\n0.4025 (0.161)\n0.1503 (0.0505)\n0.2434 (0.1696)\n\n\nbeta\n0.3153 (0.2667)\n0.313 (0.2572)\n0.9922 (0.0047)\n0.9958 (0.006)\n1.0389 (0.1109)\n1.0894 (0.1123)\n0.4908 (0.2012)\n0.8086 (0.347)\n1.2121 (0.1554)\n1.2578 (0.2597)\n1.262 (0.496)\n1.5818 (0.9321)\n\n\nisotonic\n0.1201 (0.0409)\n0.1227 (0.0429)\n0.9203 (0.0127)\n0.945 (0.0171)\n0.7097 (0.1053)\n0.8714 (0.1292)\n0 (0)\n0.1846 (0.1217)\n0.3161 (0.0766)\n0.5024 (0.2015)\n0.3437 (0.1348)\n0.5266 (0.2848)\n\n\nisotonic\n1.3525 (0.4675)\n1.3745 (0.4707)\n0.9754 (0.006)\n1.0015 (0.0094)\n0.9414 (0.133)\n1.1007 (0.1511)\n0 (0)\n0.8615 (0.6419)\n1.2168 (0.2343)\n1.5937 (0.437)\n2.8776 (1.3401)\n3.6775 (2.2323)\n\n\nlocfit_0\n0.1073 (0.0433)\n0.1079 (0.0436)\n0.9344 (0.0131)\n0.9444 (0.0165)\n0.7594 (0.1118)\n0.8384 (0.1352)\n0.0803 (0.0424)\n0.2159 (0.1122)\n0.2659 (0.0677)\n0.4045 (0.1824)\n0.037 (0.0203)\n0.1585 (0.1054)\n\n\nlocfit_0\n1.2166 (0.49)\n1.2182 (0.4834)\n0.9905 (0.0053)\n1.001 (0.0072)\n1.0071 (0.1453)\n1.0622 (0.138)\n0.4636 (0.2927)\n0.9667 (0.5047)\n1.0029 (0.1757)\n1.2996 (0.3794)\n0.313 (0.2056)\n1.0259 (0.5627)\n\n\nlocfit_1\n0.097 (0.0398)\n0.0972 (0.04)\n0.9314 (0.013)\n0.9432 (0.0167)\n0.7826 (0.1111)\n0.8716 (0.1255)\n0.0615 (0.0352)\n0.2071 (0.1093)\n0.3172 (0.0865)\n0.4509 (0.1934)\n0.1871 (0.1217)\n0.3153 (0.2323)\n\n\nlocfit_1\n1.1048 (0.4459)\n1.1043 (0.4373)\n0.9872 (0.0055)\n1 (0.0082)\n1.0315 (0.138)\n1.1022 (0.1489)\n0.3482 (0.2712)\n0.9468 (0.5231)\n1.2175 (0.2651)\n1.4517 (0.4187)\n1.6864 (1.2563)\n2.2896 (1.6303)\n\n\nlocfit_2\n0.1883 (0.061)\n0.1936 (0.0644)\n0.9252 (0.0127)\n0.9491 (0.0177)\n0.813 (0.1039)\n0.8827 (0.1259)\n0.056 (0.0357)\n0.2337 (0.1353)\n0.2864 (0.0742)\n0.5333 (0.2408)\n0.1861 (0.1117)\n0.4137 (0.2881)\n\n\nlocfit_2\n2.1273 (0.6846)\n2.1885 (0.7363)\n0.9806 (0.0061)\n1.0059 (0.0107)\n1.0704 (0.1354)\n1.1196 (0.1607)\n0.3226 (0.2733)\n1.1074 (0.7509)\n1.0968 (0.2248)\n1.7016 (0.6048)\n1.6162 (1.0614)\n2.8862 (2.0956)\n\n\nplatt\n0.0268 (0.0233)\n0.027 (0.0234)\n0.9368 (0.0127)\n0.9395 (0.016)\n0.7985 (0.1015)\n0.8733 (0.1226)\n0.0975 (0.0462)\n0.1889 (0.1038)\n0.3176 (0.0722)\n0.3987 (0.1632)\n0.1365 (0.0574)\n0.2127 (0.1647)\n\n\nplatt\n0.2692 (0.2652)\n0.2682 (0.2588)\n0.9928 (0.0047)\n0.9955 (0.006)\n1.0439 (0.1102)\n1.0958 (0.1106)\n0.5215 (0.2094)\n0.7954 (0.3492)\n1.2103 (0.1482)\n1.2456 (0.2626)\n1.1423 (0.3915)\n1.41 (0.8697)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.13: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\alpha=1\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\nInf (NaN)\nInf (NaN)\n0.9978 (0.0017)\n1.0022 (0.0038)\n0.9718 (0.0934)\n1.0142 (0.0938)\n0.7572 (0.1843)\n1.2136 (0.3604)\n0.9602 (0.1492)\n1.0483 (0.1608)\n0.871 (0.3079)\n1.1872 (0.4233)\n\n\nbeta\nInf (NaN)\nInf (NaN)\n0.9978 (0.0017)\n1.0022 (0.0038)\n0.9718 (0.0934)\n1.0142 (0.0938)\n0.7572 (0.1843)\n1.2136 (0.3604)\n0.9602 (0.1492)\n1.0483 (0.1608)\n0.871 (0.3079)\n1.1872 (0.4233)\n\n\nisotonic\nInf (NaN)\nInf (NaN)\n0.981 (0.0031)\n1.0078 (0.0072)\n0.8801 (0.1207)\n1.0221 (0.124)\n0 (0)\n1.3329 (1.0406)\n0.9638 (0.197)\n1.3234 (0.3034)\n1.985 (0.8329)\n2.7361 (1.1835)\n\n\nisotonic\nInf (NaN)\nInf (NaN)\n0.981 (0.0031)\n1.0078 (0.0072)\n0.8801 (0.1207)\n1.0221 (0.124)\n0 (0)\n1.3329 (1.0406)\n0.9638 (0.197)\n1.3234 (0.3034)\n1.985 (0.8329)\n2.7361 (1.1835)\n\n\nlocfit_0\nInf (NaN)\nInf (NaN)\n0.9961 (0.0038)\n1.0073 (0.0066)\n0.934 (0.1213)\n0.9845 (0.1226)\n0.7529 (0.6115)\n1.5462 (0.9137)\n0.8012 (0.1696)\n1.0776 (0.3037)\n0.2136 (0.1464)\n0.8704 (0.6065)\n\n\nlocfit_0\nInf (NaN)\nInf (NaN)\n0.9961 (0.0038)\n1.0073 (0.0066)\n0.934 (0.1213)\n0.9845 (0.1226)\n0.7529 (0.6115)\n1.5462 (0.9137)\n0.8012 (0.1696)\n1.0776 (0.3037)\n0.2136 (0.1464)\n0.8704 (0.6065)\n\n\nlocfit_1\nInf (NaN)\nInf (NaN)\n0.9928 (0.0036)\n1.0062 (0.0066)\n0.9584 (0.1225)\n1.0232 (0.1289)\n0.5337 (0.491)\n1.5053 (0.9351)\n0.9559 (0.2136)\n1.1896 (0.2978)\n1.0437 (0.6984)\n1.5413 (0.7966)\n\n\nlocfit_1\nInf (NaN)\nInf (NaN)\n0.9928 (0.0036)\n1.0062 (0.0066)\n0.9584 (0.1225)\n1.0232 (0.1289)\n0.5337 (0.491)\n1.5053 (0.9351)\n0.9559 (0.2136)\n1.1896 (0.2978)\n1.0437 (0.6984)\n1.5413 (0.7966)\n\n\nlocfit_2\nInf (NaN)\nInf (NaN)\n0.9864 (0.0046)\n1.0118 (0.0088)\n0.9987 (0.1183)\n1.034 (0.1321)\n0.5282 (0.4338)\n1.6578 (1.0619)\n0.8793 (0.1984)\n1.4028 (0.441)\n1.0685 (0.6941)\n2.0871 (1.1422)\n\n\nlocfit_2\nInf (NaN)\nInf (NaN)\n0.9864 (0.0046)\n1.0118 (0.0088)\n0.9987 (0.1183)\n1.034 (0.1321)\n0.5282 (0.4338)\n1.6578 (1.0619)\n0.8793 (0.1984)\n1.4028 (0.441)\n1.0685 (0.6941)\n2.0871 (1.1422)\n\n\nplatt\nInf (NaN)\nInf (NaN)\n0.9985 (0.0016)\n1.0018 (0.0036)\n0.9804 (0.0921)\n1.0213 (0.0845)\n0.8115 (0.1745)\n1.1797 (0.3505)\n0.9593 (0.1463)\n1.0359 (0.1628)\n0.6974 (0.2343)\n0.9334 (0.36)\n\n\nplatt\nInf (NaN)\nInf (NaN)\n0.9985 (0.0016)\n1.0018 (0.0036)\n0.9804 (0.0921)\n1.0213 (0.0845)\n0.8115 (0.1745)\n1.1797 (0.3505)\n0.9593 (0.1463)\n1.0359 (0.1628)\n0.6974 (0.2343)\n0.9334 (0.36)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.14: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\alpha=1.5\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0256 (0.0197)\n0.0257 (0.0196)\n0.9223 (0.0142)\n0.9266 (0.0175)\n0.5317 (0.0682)\n0.5774 (0.0681)\n0.0724 (0.0347)\n0.1624 (0.1157)\n0.2446 (0.0484)\n0.3278 (0.1233)\n0.1143 (0.036)\n0.1932 (0.1385)\n\n\nbeta\n0.2087 (0.1598)\n0.2083 (0.1549)\n0.9871 (0.0058)\n0.9923 (0.0073)\n0.7642 (0.0941)\n0.8173 (0.0945)\n0.351 (0.153)\n0.6891 (0.3311)\n0.5267 (0.0922)\n0.6382 (0.1246)\n0.3099 (0.1096)\n0.4877 (0.2199)\n\n\nisotonic\n0.0983 (0.0336)\n0.1005 (0.0353)\n0.9068 (0.0143)\n0.9318 (0.0188)\n0.4826 (0.0735)\n0.5814 (0.0762)\n0 (0)\n0.1562 (0.1153)\n0.2451 (0.0565)\n0.4097 (0.1514)\n0.2531 (0.075)\n0.4124 (0.2329)\n\n\nisotonic\n0.7664 (0.264)\n0.7798 (0.2675)\n0.9706 (0.0054)\n0.9977 (0.0085)\n0.6934 (0.103)\n0.8213 (0.1023)\n0 (0)\n0.6661 (0.3741)\n0.5281 (0.1094)\n0.7994 (0.1858)\n0.7041 (0.2486)\n1.0463 (0.3602)\n\n\nlocfit_0\n0.0874 (0.0356)\n0.0883 (0.0361)\n0.9207 (0.0144)\n0.9314 (0.0183)\n0.5162 (0.0773)\n0.5622 (0.069)\n0.062 (0.0332)\n0.1853 (0.1294)\n0.2038 (0.0473)\n0.3339 (0.1371)\n0.027 (0.015)\n0.1294 (0.1162)\n\n\nlocfit_0\n0.6667 (0.2736)\n0.6689 (0.2708)\n0.9854 (0.0069)\n0.997 (0.0098)\n0.7388 (0.1033)\n0.7961 (0.1041)\n0.326 (0.1995)\n0.8247 (0.4336)\n0.4416 (0.1015)\n0.6533 (0.1802)\n0.0753 (0.0498)\n0.3471 (0.2853)\n\n\nlocfit_1\n0.0791 (0.0327)\n0.0797 (0.0328)\n0.9177 (0.0145)\n0.9303 (0.0184)\n0.5286 (0.0743)\n0.5844 (0.0755)\n0.0464 (0.0283)\n0.1798 (0.1393)\n0.2403 (0.0542)\n0.3745 (0.1556)\n0.1271 (0.0786)\n0.2485 (0.2035)\n\n\nlocfit_1\n0.6098 (0.2558)\n0.6097 (0.2509)\n0.9823 (0.0067)\n0.996 (0.0088)\n0.7603 (0.0987)\n0.8247 (0.1071)\n0.2235 (0.1429)\n0.7746 (0.3736)\n0.5268 (0.1168)\n0.7197 (0.1766)\n0.3141 (0.2033)\n0.5739 (0.3182)\n\n\nlocfit_2\n0.1522 (0.0491)\n0.1546 (0.0495)\n0.912 (0.0148)\n0.9352 (0.0198)\n0.5492 (0.0707)\n0.594 (0.0799)\n0.0445 (0.0291)\n0.2006 (0.1344)\n0.2209 (0.0465)\n0.4376 (0.1966)\n0.1354 (0.0745)\n0.3218 (0.2706)\n\n\nlocfit_2\n1.1781 (0.397)\n1.195 (0.3996)\n0.9762 (0.0072)\n1.0015 (0.01)\n0.7938 (0.0996)\n0.8373 (0.1097)\n0.2417 (0.2026)\n0.8175 (0.4057)\n0.4809 (0.1029)\n0.834 (0.2492)\n0.3548 (0.2237)\n0.7568 (0.3772)\n\n\nplatt\n0.023 (0.019)\n0.0231 (0.0191)\n0.9232 (0.0141)\n0.9264 (0.0175)\n0.5351 (0.0679)\n0.5769 (0.071)\n0.0805 (0.0375)\n0.1604 (0.1167)\n0.2436 (0.0496)\n0.3243 (0.1252)\n0.0876 (0.0356)\n0.1572 (0.1358)\n\n\nplatt\n0.1795 (0.1493)\n0.1801 (0.1468)\n0.9882 (0.0057)\n0.992 (0.007)\n0.7781 (0.0923)\n0.8266 (0.0847)\n0.3879 (0.1636)\n0.6672 (0.3152)\n0.5251 (0.0919)\n0.6282 (0.1232)\n0.1932 (0.0799)\n0.3383 (0.208)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.15: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\alpha=3\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.004 (0.0031)\n0.004 (0.0031)\n0.6466 (0.0184)\n0.6496 (0.022)\n0.2956 (0.0438)\n0.3233 (0.0409)\n0.0117 (0.0056)\n0.0266 (0.0134)\n0.0506 (0.0097)\n0.0701 (0.0257)\n0.0213 (0.0062)\n0.035 (0.0184)\n\n\nbeta\n0.0226 (0.0182)\n0.0227 (0.0179)\n0.904 (0.0141)\n0.9099 (0.0164)\n0.4547 (0.0642)\n0.4983 (0.0616)\n0.0571 (0.0271)\n0.1336 (0.0646)\n0.1958 (0.0319)\n0.262 (0.0655)\n0.081 (0.0253)\n0.1344 (0.0572)\n\n\nisotonic\n0.0148 (0.0051)\n0.0151 (0.0053)\n0.6358 (0.0183)\n0.6531 (0.0229)\n0.2692 (0.0464)\n0.3261 (0.0434)\n0 (0)\n0.0252 (0.0143)\n0.0507 (0.0114)\n0.0874 (0.0321)\n0.0441 (0.0126)\n0.0713 (0.0323)\n\n\nisotonic\n0.0779 (0.0268)\n0.0794 (0.0274)\n0.8889 (0.0134)\n0.9147 (0.0164)\n0.4132 (0.0679)\n0.4998 (0.0658)\n0 (0)\n0.1238 (0.0665)\n0.1965 (0.0373)\n0.3255 (0.0813)\n0.1833 (0.0529)\n0.285 (0.0933)\n\n\nlocfit_0\n0.013 (0.0053)\n0.0131 (0.0054)\n0.6456 (0.0183)\n0.6527 (0.022)\n0.2921 (0.0478)\n0.3179 (0.0419)\n0.0097 (0.0051)\n0.0298 (0.0136)\n0.0429 (0.0103)\n0.07 (0.0266)\n0.0047 (0.0024)\n0.0214 (0.0133)\n\n\nlocfit_0\n0.0678 (0.0264)\n0.0682 (0.0262)\n0.9025 (0.0147)\n0.914 (0.0172)\n0.4427 (0.0679)\n0.4898 (0.0648)\n0.0472 (0.0279)\n0.1494 (0.0706)\n0.1708 (0.0359)\n0.2694 (0.0724)\n0.0202 (0.0112)\n0.089 (0.0535)\n\n\nlocfit_1\n0.0118 (0.0049)\n0.0119 (0.005)\n0.6435 (0.0183)\n0.6519 (0.0224)\n0.2934 (0.0446)\n0.3286 (0.0412)\n0.0067 (0.0042)\n0.0284 (0.0143)\n0.051 (0.0118)\n0.079 (0.031)\n0.0209 (0.0132)\n0.0401 (0.0273)\n\n\nlocfit_1\n0.0614 (0.0258)\n0.0615 (0.0252)\n0.9002 (0.0145)\n0.9132 (0.0167)\n0.4545 (0.0673)\n0.5038 (0.0635)\n0.0384 (0.0231)\n0.1447 (0.0638)\n0.1999 (0.0414)\n0.296 (0.0816)\n0.0525 (0.0337)\n0.1249 (0.0716)\n\n\nlocfit_2\n0.0229 (0.0076)\n0.0232 (0.0076)\n0.6394 (0.0185)\n0.6558 (0.0233)\n0.3076 (0.0464)\n0.3316 (0.0436)\n0.0073 (0.0043)\n0.0325 (0.0158)\n0.0458 (0.0091)\n0.0927 (0.0359)\n0.0226 (0.0124)\n0.0534 (0.0321)\n\n\nlocfit_2\n0.1195 (0.0399)\n0.1201 (0.0394)\n0.8941 (0.0147)\n0.9178 (0.0175)\n0.473 (0.0651)\n0.5079 (0.0668)\n0.0362 (0.0191)\n0.157 (0.0694)\n0.1788 (0.0338)\n0.344 (0.1047)\n0.0678 (0.0469)\n0.1815 (0.096)\n\n\nplatt\n0.009 (0.003)\n0.009 (0.003)\n0.6492 (0.0181)\n0.6514 (0.022)\n0.292 (0.0467)\n0.3115 (0.0441)\n0.0176 (0.0078)\n0.0309 (0.0144)\n0.0501 (0.0103)\n0.0692 (0.0247)\n0.0234 (0.0101)\n0.0364 (0.0184)\n\n\nplatt\n0.0344 (0.015)\n0.0347 (0.0151)\n0.9065 (0.014)\n0.911 (0.0157)\n0.4765 (0.0623)\n0.5132 (0.0549)\n0.0737 (0.0347)\n0.1387 (0.0661)\n0.1923 (0.0317)\n0.2562 (0.0641)\n0.0356 (0.0182)\n0.0819 (0.0483)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\gamma = 1/3\\)\\(\\gamma = 2/3\\)\\(\\gamma = 1\\)\\(\\gamma = 3/2\\)\\(\\gamma = 2\\)\n\n\n\n\n\n\nTable 2.16: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\gamma=0.33\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0055 (0.0046)\n0.0055 (0.0045)\n0.7392 (0.0187)\n0.7415 (0.0232)\n0.3645 (0.0519)\n0.3991 (0.0499)\n0.0182 (0.0086)\n0.0404 (0.0206)\n0.0797 (0.0163)\n0.1092 (0.048)\n0.0299 (0.0093)\n0.0487 (0.0313)\n\n\nbeta\n0.0658 (0.0616)\n0.0654 (0.0597)\n0.9736 (0.0091)\n0.9762 (0.0104)\n1.0342 (0.1295)\n1.0961 (0.1374)\n0.2131 (0.0987)\n0.3903 (0.1962)\n0.9097 (0.2538)\n1.0229 (0.4015)\n0.46 (0.3116)\n0.67 (0.6865)\n\n\nisotonic\n0.0228 (0.0077)\n0.0232 (0.0081)\n0.7267 (0.0186)\n0.7457 (0.0239)\n0.3296 (0.0556)\n0.4012 (0.0524)\n0 (0)\n0.0386 (0.0223)\n0.0797 (0.0184)\n0.1357 (0.0569)\n0.0676 (0.0226)\n0.1062 (0.0542)\n\n\nisotonic\n0.3141 (0.1088)\n0.3191 (0.1093)\n0.957 (0.0101)\n0.982 (0.0132)\n0.9359 (0.1396)\n1.1067 (0.1738)\n0 (0)\n0.4025 (0.2873)\n0.9113 (0.2842)\n1.2907 (0.5483)\n1.0123 (0.7162)\n1.491 (1.6222)\n\n\nlocfit_0\n0.0205 (0.0085)\n0.0207 (0.0085)\n0.7379 (0.0187)\n0.7453 (0.0231)\n0.3522 (0.057)\n0.387 (0.0518)\n0.0161 (0.0072)\n0.0463 (0.0199)\n0.0665 (0.0164)\n0.1094 (0.0497)\n0.0073 (0.004)\n0.0315 (0.0194)\n\n\nlocfit_0\n0.2816 (0.1183)\n0.2821 (0.1158)\n0.9718 (0.0092)\n0.9815 (0.0104)\n1.0027 (0.1415)\n1.0679 (0.1574)\n0.1881 (0.1159)\n0.4606 (0.2375)\n0.7556 (0.247)\n1.0281 (0.424)\n0.1023 (0.0808)\n0.3982 (0.3399)\n\n\nlocfit_1\n0.0186 (0.0077)\n0.0187 (0.0078)\n0.7355 (0.0187)\n0.7445 (0.0234)\n0.3612 (0.057)\n0.4018 (0.0513)\n0.0117 (0.007)\n0.0439 (0.0194)\n0.0796 (0.0204)\n0.1237 (0.0558)\n0.0366 (0.0222)\n0.0659 (0.0463)\n\n\nlocfit_1\n0.256 (0.106)\n0.2558 (0.1039)\n0.9684 (0.0094)\n0.9806 (0.0118)\n1.0247 (0.1403)\n1.1067 (0.1661)\n0.1376 (0.0933)\n0.4466 (0.2642)\n0.9121 (0.2943)\n1.1822 (0.5038)\n0.6195 (0.6098)\n0.9626 (1.1471)\n\n\nlocfit_2\n0.0356 (0.0114)\n0.0367 (0.0124)\n0.7305 (0.0184)\n0.749 (0.0241)\n0.3743 (0.0524)\n0.4083 (0.0525)\n0.011 (0.0069)\n0.05 (0.0247)\n0.0719 (0.0158)\n0.1434 (0.0597)\n0.0369 (0.0194)\n0.0827 (0.0506)\n\n\nlocfit_2\n0.4947 (0.1607)\n0.5096 (0.1716)\n0.9622 (0.0097)\n0.9862 (0.0139)\n1.0617 (0.14)\n1.1277 (0.1831)\n0.1272 (0.0891)\n0.5218 (0.3434)\n0.8236 (0.2388)\n1.3761 (0.5756)\n0.5872 (0.4857)\n1.1979 (1.2691)\n\n\nplatt\n0.0062 (0.0044)\n0.0062 (0.0044)\n0.74 (0.0187)\n0.7416 (0.0231)\n0.3708 (0.0509)\n0.4039 (0.0482)\n0.0209 (0.0102)\n0.0409 (0.0207)\n0.0803 (0.0175)\n0.1083 (0.0484)\n0.031 (0.0131)\n0.0462 (0.0332)\n\n\nplatt\n0.0619 (0.0617)\n0.0616 (0.0601)\n0.974 (0.0091)\n0.9761 (0.0104)\n1.0361 (0.1287)\n1.0987 (0.1373)\n0.2223 (0.1025)\n0.3882 (0.1954)\n0.9089 (0.2517)\n1.0174 (0.3997)\n0.4558 (0.2903)\n0.6499 (0.6753)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.17: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\gamma=0.67\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0297 (0.0239)\n0.0298 (0.0237)\n0.936 (0.0126)\n0.9397 (0.016)\n0.786 (0.103)\n0.8671 (0.1265)\n0.0884 (0.0407)\n0.1916 (0.1057)\n0.316 (0.0687)\n0.4025 (0.161)\n0.1503 (0.0505)\n0.2434 (0.1696)\n\n\nbeta\n0.3153 (0.2667)\n0.313 (0.2572)\n0.9922 (0.0047)\n0.9958 (0.006)\n1.0389 (0.1109)\n1.0894 (0.1123)\n0.4908 (0.2012)\n0.8086 (0.347)\n1.2121 (0.1554)\n1.2578 (0.2597)\n1.262 (0.496)\n1.5818 (0.9321)\n\n\nisotonic\n0.1201 (0.0409)\n0.1227 (0.0429)\n0.9203 (0.0127)\n0.945 (0.0171)\n0.7097 (0.1053)\n0.8714 (0.1292)\n0 (0)\n0.1846 (0.1217)\n0.3161 (0.0766)\n0.5024 (0.2015)\n0.3437 (0.1348)\n0.5266 (0.2848)\n\n\nisotonic\n1.3525 (0.4675)\n1.3745 (0.4707)\n0.9754 (0.006)\n1.0015 (0.0094)\n0.9414 (0.133)\n1.1007 (0.1511)\n0 (0)\n0.8615 (0.6419)\n1.2168 (0.2343)\n1.5937 (0.437)\n2.8776 (1.3401)\n3.6775 (2.2323)\n\n\nlocfit_0\n0.1073 (0.0433)\n0.1079 (0.0436)\n0.9344 (0.0131)\n0.9444 (0.0165)\n0.7594 (0.1118)\n0.8384 (0.1352)\n0.0803 (0.0424)\n0.2159 (0.1122)\n0.2659 (0.0677)\n0.4045 (0.1824)\n0.037 (0.0203)\n0.1585 (0.1054)\n\n\nlocfit_0\n1.2166 (0.49)\n1.2182 (0.4834)\n0.9905 (0.0053)\n1.001 (0.0072)\n1.0071 (0.1453)\n1.0622 (0.138)\n0.4636 (0.2927)\n0.9667 (0.5047)\n1.0029 (0.1757)\n1.2996 (0.3794)\n0.313 (0.2056)\n1.0259 (0.5627)\n\n\nlocfit_1\n0.097 (0.0398)\n0.0972 (0.04)\n0.9314 (0.013)\n0.9432 (0.0167)\n0.7826 (0.1111)\n0.8716 (0.1255)\n0.0615 (0.0352)\n0.2071 (0.1093)\n0.3172 (0.0865)\n0.4509 (0.1934)\n0.1871 (0.1217)\n0.3153 (0.2323)\n\n\nlocfit_1\n1.1048 (0.4459)\n1.1043 (0.4373)\n0.9872 (0.0055)\n1 (0.0082)\n1.0315 (0.138)\n1.1022 (0.1489)\n0.3482 (0.2712)\n0.9468 (0.5231)\n1.2175 (0.2651)\n1.4517 (0.4187)\n1.6864 (1.2563)\n2.2896 (1.6303)\n\n\nlocfit_2\n0.1883 (0.061)\n0.1936 (0.0644)\n0.9252 (0.0127)\n0.9491 (0.0177)\n0.813 (0.1039)\n0.8827 (0.1259)\n0.056 (0.0357)\n0.2337 (0.1353)\n0.2864 (0.0742)\n0.5333 (0.2408)\n0.1861 (0.1117)\n0.4137 (0.2881)\n\n\nlocfit_2\n2.1273 (0.6846)\n2.1885 (0.7363)\n0.9806 (0.0061)\n1.0059 (0.0107)\n1.0704 (0.1354)\n1.1196 (0.1607)\n0.3226 (0.2733)\n1.1074 (0.7509)\n1.0968 (0.2248)\n1.7016 (0.6048)\n1.6162 (1.0614)\n2.8862 (2.0956)\n\n\nplatt\n0.0268 (0.0233)\n0.027 (0.0234)\n0.9368 (0.0127)\n0.9395 (0.016)\n0.7985 (0.1015)\n0.8733 (0.1226)\n0.0975 (0.0462)\n0.1889 (0.1038)\n0.3176 (0.0722)\n0.3987 (0.1632)\n0.1365 (0.0574)\n0.2127 (0.1647)\n\n\nplatt\n0.2692 (0.2652)\n0.2682 (0.2588)\n0.9928 (0.0047)\n0.9955 (0.006)\n1.0439 (0.1102)\n1.0958 (0.1106)\n0.5215 (0.2094)\n0.7954 (0.3492)\n1.2103 (0.1482)\n1.2456 (0.2626)\n1.1423 (0.3915)\n1.41 (0.8697)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.18: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\gamma=1\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\nInf (NaN)\nInf (NaN)\n0.9978 (0.0017)\n1.0022 (0.0038)\n0.9718 (0.0934)\n1.0142 (0.0938)\n0.7572 (0.1843)\n1.2136 (0.3604)\n0.9602 (0.1492)\n1.0483 (0.1608)\n0.871 (0.3079)\n1.1872 (0.4233)\n\n\nbeta\nInf (NaN)\nInf (NaN)\n0.9978 (0.0017)\n1.0022 (0.0038)\n0.9718 (0.0934)\n1.0142 (0.0938)\n0.7572 (0.1843)\n1.2136 (0.3604)\n0.9602 (0.1492)\n1.0483 (0.1608)\n0.871 (0.3079)\n1.1872 (0.4233)\n\n\nisotonic\nInf (NaN)\nInf (NaN)\n0.981 (0.0031)\n1.0078 (0.0072)\n0.8801 (0.1207)\n1.0221 (0.124)\n0 (0)\n1.3329 (1.0406)\n0.9638 (0.197)\n1.3234 (0.3034)\n1.985 (0.8329)\n2.7361 (1.1835)\n\n\nisotonic\nInf (NaN)\nInf (NaN)\n0.981 (0.0031)\n1.0078 (0.0072)\n0.8801 (0.1207)\n1.0221 (0.124)\n0 (0)\n1.3329 (1.0406)\n0.9638 (0.197)\n1.3234 (0.3034)\n1.985 (0.8329)\n2.7361 (1.1835)\n\n\nlocfit_0\nInf (NaN)\nInf (NaN)\n0.9961 (0.0038)\n1.0073 (0.0066)\n0.934 (0.1213)\n0.9845 (0.1226)\n0.7529 (0.6115)\n1.5462 (0.9137)\n0.8012 (0.1696)\n1.0776 (0.3037)\n0.2136 (0.1464)\n0.8704 (0.6065)\n\n\nlocfit_0\nInf (NaN)\nInf (NaN)\n0.9961 (0.0038)\n1.0073 (0.0066)\n0.934 (0.1213)\n0.9845 (0.1226)\n0.7529 (0.6115)\n1.5462 (0.9137)\n0.8012 (0.1696)\n1.0776 (0.3037)\n0.2136 (0.1464)\n0.8704 (0.6065)\n\n\nlocfit_1\nInf (NaN)\nInf (NaN)\n0.9928 (0.0036)\n1.0062 (0.0066)\n0.9584 (0.1225)\n1.0232 (0.1289)\n0.5337 (0.491)\n1.5053 (0.9351)\n0.9559 (0.2136)\n1.1896 (0.2978)\n1.0437 (0.6984)\n1.5413 (0.7966)\n\n\nlocfit_1\nInf (NaN)\nInf (NaN)\n0.9928 (0.0036)\n1.0062 (0.0066)\n0.9584 (0.1225)\n1.0232 (0.1289)\n0.5337 (0.491)\n1.5053 (0.9351)\n0.9559 (0.2136)\n1.1896 (0.2978)\n1.0437 (0.6984)\n1.5413 (0.7966)\n\n\nlocfit_2\nInf (NaN)\nInf (NaN)\n0.9864 (0.0046)\n1.0118 (0.0088)\n0.9987 (0.1183)\n1.034 (0.1321)\n0.5282 (0.4338)\n1.6578 (1.0619)\n0.8793 (0.1984)\n1.4028 (0.441)\n1.0685 (0.6941)\n2.0871 (1.1422)\n\n\nlocfit_2\nInf (NaN)\nInf (NaN)\n0.9864 (0.0046)\n1.0118 (0.0088)\n0.9987 (0.1183)\n1.034 (0.1321)\n0.5282 (0.4338)\n1.6578 (1.0619)\n0.8793 (0.1984)\n1.4028 (0.441)\n1.0685 (0.6941)\n2.0871 (1.1422)\n\n\nplatt\nInf (NaN)\nInf (NaN)\n0.9985 (0.0016)\n1.0018 (0.0036)\n0.9804 (0.0921)\n1.0213 (0.0845)\n0.8115 (0.1745)\n1.1797 (0.3505)\n0.9593 (0.1463)\n1.0359 (0.1628)\n0.6974 (0.2343)\n0.9334 (0.36)\n\n\nplatt\nInf (NaN)\nInf (NaN)\n0.9985 (0.0016)\n1.0018 (0.0036)\n0.9804 (0.0921)\n1.0213 (0.0845)\n0.8115 (0.1745)\n1.1797 (0.3505)\n0.9593 (0.1463)\n1.0359 (0.1628)\n0.6974 (0.2343)\n0.9334 (0.36)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.19: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\gamma=1.5\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nNo Calibration\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n1 (0)\n\n\nbeta\n0.0256 (0.0197)\n0.0257 (0.0196)\n0.9223 (0.0142)\n0.9266 (0.0175)\n0.5317 (0.0682)\n0.5774 (0.0681)\n0.0724 (0.0347)\n0.1624 (0.1157)\n0.2446 (0.0484)\n0.3278 (0.1233)\n0.1143 (0.036)\n0.1932 (0.1385)\n\n\nbeta\n0.2087 (0.1598)\n0.2083 (0.1549)\n0.9871 (0.0058)\n0.9923 (0.0073)\n0.7642 (0.0941)\n0.8173 (0.0945)\n0.351 (0.153)\n0.6891 (0.3311)\n0.5267 (0.0922)\n0.6382 (0.1246)\n0.3099 (0.1096)\n0.4877 (0.2199)\n\n\nisotonic\n0.0983 (0.0336)\n0.1005 (0.0353)\n0.9068 (0.0143)\n0.9318 (0.0188)\n0.4826 (0.0735)\n0.5814 (0.0762)\n0 (0)\n0.1562 (0.1153)\n0.2451 (0.0565)\n0.4097 (0.1514)\n0.2531 (0.075)\n0.4124 (0.2329)\n\n\nisotonic\n0.7664 (0.264)\n0.7798 (0.2675)\n0.9706 (0.0054)\n0.9977 (0.0085)\n0.6934 (0.103)\n0.8213 (0.1023)\n0 (0)\n0.6661 (0.3741)\n0.5281 (0.1094)\n0.7994 (0.1858)\n0.7041 (0.2486)\n1.0463 (0.3602)\n\n\nlocfit_0\n0.0874 (0.0356)\n0.0883 (0.0361)\n0.9207 (0.0144)\n0.9314 (0.0183)\n0.5162 (0.0773)\n0.5622 (0.069)\n0.062 (0.0332)\n0.1853 (0.1294)\n0.2038 (0.0473)\n0.3339 (0.1371)\n0.027 (0.015)\n0.1294 (0.1162)\n\n\nlocfit_0\n0.6667 (0.2736)\n0.6689 (0.2708)\n0.9854 (0.0069)\n0.997 (0.0098)\n0.7388 (0.1033)\n0.7961 (0.1041)\n0.326 (0.1995)\n0.8247 (0.4336)\n0.4416 (0.1015)\n0.6533 (0.1802)\n0.0753 (0.0498)\n0.3471 (0.2853)\n\n\nlocfit_1\n0.0791 (0.0327)\n0.0797 (0.0328)\n0.9177 (0.0145)\n0.9303 (0.0184)\n0.5286 (0.0743)\n0.5844 (0.0755)\n0.0464 (0.0283)\n0.1798 (0.1393)\n0.2403 (0.0542)\n0.3745 (0.1556)\n0.1271 (0.0786)\n0.2485 (0.2035)\n\n\nlocfit_1\n0.6098 (0.2558)\n0.6097 (0.2509)\n0.9823 (0.0067)\n0.996 (0.0088)\n0.7603 (0.0987)\n0.8247 (0.1071)\n0.2235 (0.1429)\n0.7746 (0.3736)\n0.5268 (0.1168)\n0.7197 (0.1766)\n0.3141 (0.2033)\n0.5739 (0.3182)\n\n\nlocfit_2\n0.1522 (0.0491)\n0.1546 (0.0495)\n0.912 (0.0148)\n0.9352 (0.0198)\n0.5492 (0.0707)\n0.594 (0.0799)\n0.0445 (0.0291)\n0.2006 (0.1344)\n0.2209 (0.0465)\n0.4376 (0.1966)\n0.1354 (0.0745)\n0.3218 (0.2706)\n\n\nlocfit_2\n1.1781 (0.397)\n1.195 (0.3996)\n0.9762 (0.0072)\n1.0015 (0.01)\n0.7938 (0.0996)\n0.8373 (0.1097)\n0.2417 (0.2026)\n0.8175 (0.4057)\n0.4809 (0.1029)\n0.834 (0.2492)\n0.3548 (0.2237)\n0.7568 (0.3772)\n\n\nplatt\n0.023 (0.019)\n0.0231 (0.0191)\n0.9232 (0.0141)\n0.9264 (0.0175)\n0.5351 (0.0679)\n0.5769 (0.071)\n0.0805 (0.0375)\n0.1604 (0.1167)\n0.2436 (0.0496)\n0.3243 (0.1252)\n0.0876 (0.0356)\n0.1572 (0.1358)\n\n\nplatt\n0.1795 (0.1493)\n0.1801 (0.1468)\n0.9882 (0.0057)\n0.992 (0.007)\n0.7781 (0.0923)\n0.8266 (0.0847)\n0.3879 (0.1636)\n0.6672 (0.3152)\n0.5251 (0.0919)\n0.6282 (0.1232)\n0.1932 (0.0799)\n0.3383 (0.208)\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: There were 24 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(...)`.\nCaused by warning in `max()`:\n! no non-missing arguments, returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 23 remaining warnings.\n\n\n\n\nTable 2.20: Deviation of the average calibration metrics from the reference (average metric computed using the uncalibrated predicted probabilities) over the 200 replications, for \\(\\gamma=2\\), computed on the calibration and on the test set, using different predicted probabilities (in rows). Standard deviations are given between brackets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE (True)\n\n\nBrier\n\n\nECE\n\n\nQMSE\n\n\nWMSE\n\n\nLCS\n\n\n\nCalibration Method\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\nCalib.\nTest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKull, Meelis, Telmo M. Silva Filho, and Peter Flach. 2017. “Beyond Sigmoids: How to Obtain Well-Calibrated Probabilities from Binary Classifiers with Beta Calibration.” Electronic Journal of Statistics 11 (2). https://doi.org/10.1214/17-ejs1338si.\n\n\nPlatt, John et al. 1999. “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” Advances in Large Margin Classifiers 10 (3): 61–74.\n\n\nZadrozny, Bianca, and Charles Elkan. 2002. “Transforming Classifier Scores into Accurate Multiclass Probability Estimates.” In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD02. ACM. https://doi.org/10.1145/775047.775151."
  },
  {
    "objectID": "rf-single.html#sec-ml-recalib-dgp",
    "href": "rf-single.html#sec-ml-recalib-dgp",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.1 Data Generating Process",
    "text": "3.1 Data Generating Process\nWe use the same DGP as in Chapter 1.\n\n#' Simulates binary data\n#'\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\nsim_data &lt;- function(n_obs = 2000, \n                     seed) {\n  set.seed(seed)\n  \n  x1 &lt;- runif(n_obs)\n  x2 &lt;- runif(n_obs)\n  x3 &lt;- runif(n_obs)\n  x4 &lt;- runif(n_obs)\n  epsilon_p &lt;- rnorm(n_obs, mean = 0, sd = .5)\n  \n  # True latent score\n  eta &lt;- -0.1*x1 + 0.05*x2 + 0.2*x3 - 0.05*x4  + epsilon_p\n  \n  # True probability\n  p &lt;- (1 / (1 + exp(-eta)))\n  \n  # Observed event\n  d &lt;- rbinom(n_obs, size = 1, prob = p)\n  \n  tibble(\n    # Event Probability\n    p = p,\n    # Binary outcome variable\n    d = d,\n    # Variables\n    x1 = x1,\n    x2 = x2,\n    x3 = x3,\n    x4 = x4\n  )\n}"
  },
  {
    "objectID": "rf-single.html#splitting-the-dataset",
    "href": "rf-single.html#splitting-the-dataset",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.2 Splitting the dataset",
    "text": "3.2 Splitting the dataset\nThe process applied in this chapter is divided the following parts:\n\nget the trained Random Forest classifier or regressions from Chapter 1 to obtain the predicted scores \\(\\hat{s}(\\boldsymbol x_i)\\) (i.e., either \\(\\hat{p}_{\\text{score}}\\) or \\(\\hat{p}_{\\text{vote}}\\))\nrecalibrating the obtained scores through different approaches defined in Chapter 2\nrecalculating the different calibration metrics on the recalibrated predicted scores.\n\nTherefore, it is necessary to split the dataset into three parts:\n\na train set: to train the Random Forest classifier,\na calibration set: to train the recalibrator,\na test set: on which we will compute the calibration metrics.\n\nTo split the data, we define the get_samples() function.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\nget_samples &lt;- function(seed,\n                        n_obs = 2000) {\n  set.seed(seed)\n  data_all &lt;- sim_data(\n    n_obs = n_obs, \n    seed = seed\n  )\n  \n  # Train/calibration/test sets----\n  data &lt;- data_all |&gt; select(d, x1:x4)\n  true_probas &lt;- data_all |&gt; select(p)\n  \n  train_index &lt;- sample(1:nrow(data), size = .6 * nrow(data), replace = FALSE)\n  tb_train &lt;- data |&gt; slice(train_index)\n  tb_calib_test &lt;- data |&gt; slice(-train_index)\n  true_probas_train &lt;- true_probas |&gt; slice(train_index)\n  true_probas_calib_test &lt;- true_probas |&gt; slice(-train_index)\n  \n  calib_index &lt;- sample(\n    1:nrow(tb_calib_test), size = .5 * nrow(tb_calib_test), replace = FALSE\n  )\n  tb_calib &lt;- tb_calib_test |&gt; slice(calib_index)\n  tb_test &lt;- tb_calib_test |&gt; slice(-calib_index)\n  true_probas_calib &lt;- true_probas_calib_test |&gt; slice(calib_index)\n  true_probas_test &lt;- true_probas_calib_test |&gt; slice(-calib_index)\n  \n  list(\n    data_all = data_all,\n    data = data,\n    tb_train = tb_train,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    true_probas_train = true_probas_train,\n    true_probas_calib = true_probas_calib,\n    true_probas_test = true_probas_test,\n    train_index = train_index,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs\n  )\n}\n\nThroughout this chapter, we will adopt the following colours: blue for the train set, orange for the calibration, and green for the test set.\n\ncolours &lt;- c(\"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\")\n\nLet us generate a dataset with 2,000 observations.\n\nseed &lt;- 1\nn_obs &lt;- 2000\n\ndata &lt;- get_samples(seed = 1, n_obs = n_obs)\ntb_train &lt;- data$tb_train\ntb_calib &lt;- data$tb_calib\ntb_test &lt;- data$tb_test"
  },
  {
    "objectID": "rf-single.html#sec-rf-calib-aggregating",
    "href": "rf-single.html#sec-rf-calib-aggregating",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.3 Aggregating: Vote Count or Scores?",
    "text": "3.3 Aggregating: Vote Count or Scores?\nFor a classification task, the randomForest() function from {randomForest} returns the majority vote (\\(\\hat{d}_i = 0\\) or \\(\\hat{d}_i =1\\)) for each tree \\(i\\) of the forest composed of \\(n_T\\) trees. When a user specifically asks a predicted probability, the estimation is made by computing the average of the majority votes among all the trees of the forest: \\[\\hat{p}_{\\text{vote}} = \\sum_{i = 1}^{n_T} \\hat{d}_i. \\tag{3.1}\\]\nWe would like to examine if computing the average of the trees’ predicted probability \\(\\hat{p}_i\\) rather than the trees’ majority vote \\(\\hat{d}_i\\) affects the calibration: \\[\\hat{p}_{\\text{score}} = \\sum_{i=1}^{n_T} \\hat{p}_i. \\tag{3.2}\\]\nWe will train an random forest in both cases. However, to illustrates the methods, we will first have a look at estimations made with a small forest that contains only 4 trees.\n\nlibrary(randomForest)\n\nTo better visualize how the randomForest function works, let us begin with a small forest with only 4 trees. Using the keep.forest argument of the randomForest() function allows to keep the predictions made by each tree.\nLet us begin with generating some data:\n\ntoy_data &lt;- get_samples(seed = 1, n_obs = 2000)\ntoy_data_train &lt;- toy_data$tb_train\ntoy_data_calib &lt;- toy_data$tb_calib\ntoy_data_test &lt;- toy_data$tb_test\n\n\n3.3.1 Vote Count with a Classifier\nWe make sure to build a classifier here, by forcing the d variable to be a factor :\n\nset.seed(123) # for replication\nrf_classif &lt;- randomForest(\n  d ~ ., data = toy_data_train |&gt; mutate(d = factor(d)),\n  nodesize = 0.1 * nrow(toy_data_train), # minimum size of terminal nodes\n  keep.forest   = TRUE,\n  ntree = 4\n)\nrf_classif$type\n\n[1] \"classification\"\n\n\nSetting predict.all = TRUE in the predict.randomForest() function allows to obtain the predictions made by each tree in a matrix where the examples are given in rows and the trees majority votes \\(\\hat{d}_i\\) returned by the trees are given in columns when setting either type = \"prob\" or type == \"response:\n\n# Predictions made by each tree\n#   columns: estimated class for a tree; row: cases\npred_classif_prob &lt;- predict(\n  rf_classif, \n  newdata = toy_data_train, \n  type = \"prob\", predict.all = TRUE\n)\n# Predictions of the 4 trees for the first 6 cases\nhead(pred_classif_prob$individual)\n\n  [,1] [,2] [,3] [,4]\n1 \"1\"  \"1\"  \"0\"  \"0\" \n2 \"0\"  \"0\"  \"0\"  \"0\" \n3 \"1\"  \"1\"  \"0\"  \"1\" \n4 \"1\"  \"0\"  \"0\"  \"0\" \n5 \"0\"  \"0\"  \"1\"  \"0\" \n6 \"1\"  \"1\"  \"1\"  \"1\" \n\n\n\n# Predictions made by each tree\n#  columns: estimated probability for a tree; row: cases\npred_classif_vote &lt;- predict(\n  rf_classif, \n  newdata = toy_data_train, \n  type = \"vote\", predict.all = TRUE\n)\nhead(pred_classif_vote$individual)\n\n  [,1] [,2] [,3] [,4]\n1 \"1\"  \"1\"  \"0\"  \"0\" \n2 \"0\"  \"0\"  \"0\"  \"0\" \n3 \"1\"  \"1\"  \"0\"  \"1\" \n4 \"1\"  \"0\"  \"0\"  \"0\" \n5 \"0\"  \"0\"  \"1\"  \"0\" \n6 \"1\"  \"1\"  \"1\"  \"1\" \n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe help page of the predict.randomForest() function states that setting the argument type = prob returns a matrix of class probabilities when the tree is a classifier (when object$type is classification). This does not seem to be the case.\nWe will thus need a little trick to get the estimated probabilities of each tree, by growing the random forest in a regression context (as we will do in Chapters 4). As the response variable is binary with values 0 and 1, the regression tree and the classification tree are equivalent.\n\n\nWe can see that the predicted individual probabilities of positives are proportions of 1 in the \\(n_T\\) trees of the forest:\n\nhead(pred_classif_prob$aggregate)\n\n     0    1\n1 0.50 0.50\n2 1.00 0.00\n3 0.25 0.75\n4 0.75 0.25\n5 0.75 0.25\n6 0.00 1.00\n\nhead(pred_classif_vote$aggregate) \n\n     0    1\n1 0.50 0.50\n2 1.00 0.00\n3 0.25 0.75\n4 0.75 0.25\n5 0.75 0.25\n6 0.00 1.00\n\n\n\n\n3.3.2 Probabilities with a Regression\nThe response variable \\(d\\) is a numerical variable. The randomForest() function will therefore consider a regression task if we do not turn that response variable into a factor.\n\nset.seed(123) # for replication\nrf_reg &lt;- randomForest(\n  d ~ ., data = toy_data_train,\n  nodesize = 0.1 * nrow(toy_data_train), # minimum size of terminal nodes\n  keep.forest   = TRUE,\n  ntree = 4\n)\nrf_reg$type\n\n[1] \"regression\"\n\n\nThis time, the predict.randomForest() function, when setting type = \"response\" and predict.all = TRUE will return for each individual in row, the predicted probabilities \\(\\hat{p}_i\\) of each tree \\(i\\):\n\n# Predictions made by each tree\n#   columns: estimated probability for a tree; row: example\npred_reg_prob &lt;- predict(\n  rf_reg, \n  newdata = toy_data_train, \n  type = \"response\", predict.all = TRUE\n)\nhead(pred_reg_prob$individual)\n\n       [,1]      [,2]      [,3]      [,4]\n1 0.5593220 0.3636364 0.5876289 0.6111111\n2 0.5076923 0.4333333 0.3495146 0.2666667\n3 0.3725490 0.2962963 0.5086207 0.5470085\n4 0.8823529 0.3636364 0.3235294 0.6111111\n5 0.2727273 0.6800000 0.3235294 0.5692308\n6 0.9642857 0.3504274 0.2000000 0.1111111\n\n\nThe aggregated predictions over the trees for the first individuals:\n\npred_reg_prob$aggregate |&gt; head()\n\n        1         2         3         4         5         6 \n0.5304246 0.3893017 0.4311186 0.5451575 0.4613719 0.4064560 \n\n\nWhich are, as expected, the average of the 4 predictions made by the 4 trees, for each individual:\n\nrowMeans(pred_reg_prob$individual) |&gt; head()\n\n        1         2         3         4         5         6 \n0.5304246 0.3893017 0.4311186 0.5451575 0.4613719 0.4064560"
  },
  {
    "objectID": "rf-single.html#train-the-two-random-forests",
    "href": "rf-single.html#train-the-two-random-forests",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.4 Train the two Random Forests",
    "text": "3.4 Train the two Random Forests\nWe define the function apply_rf() to fit a random forest on the train dataset, in the regression context. The function returns the predicted scores on the train set, the calibration set, and on the test set.\n\n#' Apply Random Forest algorithm\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data, \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  scores_train &lt;- predict(rf, newdata = train_data, type = \"response\")\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"response\")\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"response\")\n  \n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\nWe can train a random forest with 500 trees. We set the minium size of terminal notes to one tenth of the train set.\n\nscores_reg &lt;- apply_rf(\n  train_data = tb_train, calib_data = tb_calib, test_data = tb_test\n)\n\nWe also define the apply_rf_vote() function to fit a random forest on the train dataset, but considering a classification task instead.\n\n#' Apply Random Forest algorithm (regression task)\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf_vote &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data |&gt; mutate(d = factor(d)), \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"vote\")[, \"1\"]\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"vote\")[, \"1\"]\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"vote\")[, \"1\"]\n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\nLet us train the forest in that context:\n\nscores_classif &lt;- apply_rf_vote(\n  train_data = tb_train, calib_data = tb_calib, test_data = tb_test\n)"
  },
  {
    "objectID": "rf-single.html#measuring-the-initial-calibration",
    "href": "rf-single.html#measuring-the-initial-calibration",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.5 Measuring the Initial Calibration",
    "text": "3.5 Measuring the Initial Calibration\nWe would like to have an idea about how well calibrated are both trained random forests. We will adopt two ways of looking at calibration: either with metrics, or visually, using graphs.\n\n3.5.1 Define Metrics\nWe rely on the calibration metrics defined in in Chapter 1.\n\nBrier ScoreECEQMSEWMSE\n\n\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n\n\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n\n\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n\n\n\nlibrary(binom)\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  if (all(is.na(scores))) {\n    wmse &lt;- NA\n  } else {\n    dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  wmse &lt;- local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n  }\n}\n\n\n\n\nWe define a function, compute_metrics(), to compute all these metrics in a single call.\n\n#' Computes the calibration metrics for a set of observed and predicted \n#' probabilities\n#'  - mse: Mean Squared Error based on true proba\n#'  - brier: Brier score\n#'  - ece: Expectec Calibration Error\n#'  - qmse: MSE on bins defined by the quantiles of the predicted scores\n#'  - wmse: MSE weighted by the density of the predicted scores\n#' \n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#' @param linspace vector of values at which to compute the WMSE\n#' @param k number of classes (bins) to create (default to `10`)\ncompute_metrics &lt;- function(obs, \n                            scores, \n                            true_probas = NULL,\n                            linspace = NULL,\n                            k = 10) {\n  \n  ## True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n  \n  \n  brier &lt;- brier_score(obs = obs, scores = scores)\n  if (length(unique(scores)) &gt; 1) {\n    ece &lt;- e_calib_error(obs = obs, scores = scores, k = k, threshold = .5)\n    qmse &lt;- qmse_error(obs = obs, scores = scores, k = k, threshold = .5)\n  } else {\n    ece &lt;- NA\n    qmse &lt;- NA\n  }\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  expected_events &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs, \n      scores = scores,\n      tau = .x, nn = .15, prob = .95, method = \"probit\")\n  ) |&gt; \n    bind_rows()\n  wmse &lt;- weighted_mse(local_scores = expected_events, scores = scores)\n  tibble(\n    mse = mse, \n    brier = brier, \n    ece = ece, \n    qmse = qmse, \n    wmse = wmse\n  )\n}\n\nWe also define a function to compute standard goodness of fit metrics:\n\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities. If `NULL` (default), True MSE is not \n#'   computed and the `NA` value is returned for this metric\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\n#' @returns tibble with MSE, accuracy, missclassification rate, sensititity \n#'  (TPR), specificity (TNR), FPR, and used threshold\ncompute_gof &lt;- function(true_prob = NULL,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  if (!is.null(true_prob)) {\n    mse &lt;- mean((true_prob - pred)^2)\n  } else {\n    mse = NA\n  }\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR\n  )\n}\n\n\n\n3.5.2 Compute Metrics\nLet us compute the calibration metrics for both forests, on each sample (train, validation, and test).\n\nGoodness of FitCalibration Metrics\n\n\n\nRegressionClassification\n\n\n\ncalibration_reg_train &lt;- compute_metrics(\n  obs = tb_train$d, \n  scores = scores_reg$scores_train, \n  true_proba = data$true_probas_train$p,\n  k = 10\n) |&gt; \n  mutate(sample = \"train\")\n\ncalibration_reg_calib &lt;- compute_metrics(\n  obs = tb_calib$d, \n  scores = scores_reg$scores_calib, \n  true_proba = data$true_probas_calib$p,\n  k = 5\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalibration_reg_test &lt;- compute_metrics(\n  obs = tb_test$d, \n  scores = scores_reg$scores_test, \n  true_proba = data$true_probas_test$p,\n  k = 5\n) |&gt; \n  mutate(sample = \"test\")\n\ncalibration_reg &lt;- \n  calibration_reg_train |&gt; \n  bind_rows(calibration_reg_calib) |&gt; \n  bind_rows(calibration_reg_test)\n\n\n\n\ncalibration_classif_train &lt;- compute_metrics(\n  obs = tb_train$d, \n  scores = scores_classif$scores_train, \n  true_proba = data$true_probas_train$p,\n  k = 10\n) |&gt; \n  mutate(sample = \"train\")\n\ncalibration_classif_calib &lt;- compute_metrics(\n  obs = tb_calib$d, \n  scores = scores_classif$scores_calib, \n  true_proba = data$true_probas_calib$p,\n  k = 5\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalibration_classif_test &lt;- compute_metrics(\n  obs = tb_test$d, \n  scores = scores_classif$scores_test, \n  true_proba = data$true_probas_test$p,\n  k = 5\n) |&gt; \n  mutate(sample = \"test\")\n\n\ncalibration_classif &lt;- \n  calibration_classif_train |&gt; \n  bind_rows(calibration_classif_calib) |&gt; \n  bind_rows(calibration_classif_test)\n\n\n\n\n\n\n\nRegressionClassification\n\n\n\ngof_reg_train &lt;- compute_gof(\n  true_prob = data$true_probas_train$p, \n  obs = tb_train$d, \n  pred = scores_reg$scores_train\n) |&gt; mutate(sample = \"train\")\n\ngof_reg_calib &lt;- compute_gof(\n  true_prob = data$true_probas_calib$p, \n  obs = tb_calib$d, \n  pred = scores_reg$scores_calib\n) |&gt; mutate(sample = \"calibration\")\n\ngof_reg_test &lt;- compute_gof(\n  true_prob = data$true_probas_test$p, \n  obs = tb_test$d, \n  pred = scores_reg$scores_test\n) |&gt; mutate(sample = \"test\")\n\ngof_reg &lt;- \n  gof_reg_train |&gt; \n  bind_rows(gof_reg_calib) |&gt; \n  bind_rows(gof_reg_test)\n\n\n\n\ngof_classif_train &lt;- compute_gof(\n  true_prob = data$true_probas_train$p, \n  obs = tb_train$d, \n  pred = scores_classif$scores_train\n) |&gt; mutate(sample = \"train\")\n\ngof_classif_calib &lt;- compute_gof(\n  true_prob = data$true_probas_calib$p, \n  obs = tb_calib$d, \n  pred = scores_classif$scores_calib\n) |&gt; mutate(sample = \"calibration\")\n\ngof_classif_test &lt;- compute_gof(\n  true_prob = data$true_probas_test$p, \n  obs = tb_test$d, \n  pred = scores_classif$scores_test\n) |&gt; mutate(sample = \"test\")\n\ngof_classif &lt;- \n  gof_classif_train |&gt; \n  bind_rows(gof_classif_calib) |&gt; \n  bind_rows(gof_classif_test)\n\n\n\n\n\n\n\nWe bind the goodness of fit metrics of the two forests in a single tibble:\n\ngof &lt;- gof_reg |&gt; \n  mutate(model = \"regression\") |&gt; \n  bind_rows(gof_classif |&gt; mutate(model = \"classification\"))\n\nAnd we do the same for the calibration metrics:\n\ncalibration &lt;- calibration_reg |&gt; mutate(model = \"regression\") |&gt; \n  bind_rows(calibration_classif |&gt; mutate(model = \"classification\"))\n\nThen, we merge all these metrics in a single tibble:\n\nsummary_metrics &lt;- \n  gof |&gt; \n  left_join(calibration, by = c(\"mse\", \"sample\", \"model\")) |&gt; \n  relocate(sample, model, .before = \"mse\") |&gt; \n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"calibration\", \"test\"),\n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    ),\n    model = factor(\n      model,\n      levels = c(\"regression\", \"classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    )\n  ) |&gt; \n  arrange(model, sample)\n\n\n\nDisplay the R codes used to produce the Table.\nsummary_metrics |&gt; select(-model) |&gt; \n  select(\n    Sample = sample, \n    `True MSE` = mse, \n    `Brier Score` = brier, ECE = ece, QMSE = qmse, WMSE = wmse,\n    Accuracy = accuracy,\n    `True Pos. Rate` = sensitivity, `False Pos. Rate` = FPR) |&gt; \n  knitr::kable(escape = FALSE, booktabs = T, digits = 3) |&gt; \n  kableExtra::pack_rows(index = table(summary_metrics$model)) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable 3.1: Goodness of fit and Calibration Before Recalibration.\n\n\nSample\nTrue MSE\nBrier Score\nECE\nQMSE\nWMSE\nAccuracy\nTrue Pos. Rate\nFalse Pos. Rate\n\n\n\n\nRegression\n\n\nTrain\n0.016\n0.210\n0.294\n0.071\n0.069\n0.798\n0.815\n0.218\n\n\nCalibration\n0.015\n0.256\n0.033\n0.009\n0.057\n0.485\n0.519\n0.554\n\n\nTest\n0.017\n0.253\n0.085\n0.006\n0.028\n0.520\n0.568\n0.527\n\n\nClassification\n\n\nTrain\n0.034\n0.200\n0.199\n0.009\n0.007\n0.705\n0.722\n0.313\n\n\nCalibration\n0.035\n0.281\n0.113\n0.028\n0.066\n0.475\n0.477\n0.527\n\n\nTest\n0.038\n0.276\n0.148\n0.023\n0.060\n0.495\n0.538\n0.547\n\n\n\n\n\n\n\n\n\n\n3.5.3 Define Visualization Functions\nTo plot the calibration curve, we rely on three techniques. The first one uses bins, defined by quantiles of the predicted scores. The second and third ones are smoother versions, based on local regression and on moving averages, respectively. The first and third allow to compute confidence intervals.\n\n3.5.3.1 Quantile-based Calibration Curve\nWe define two functions. A first, calib_curve_quant() to obtain the calibration curve, and a second, plot_calib_curve_quant(), to display it on a graph.\n\n#' Confidence interval for binomial data, using quantile-defined bins\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of bins to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @param add_conf_int (logical) if TRUE, confidence intervals are computed\n#' @param prob confidence interval level\n#' @param method Which method to use to construct the interval. Any combination \n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\", \n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with the following columns, where each row corresponds to\n#'   a bin:\n#' - `mean`: estimation of $E(d | s(x) = p)$ where $p$ is the average score in bin b\n#' - `lower`: lower bound of the confidence interval\n#' - `upper`: upper bound of the confidence interval\n#' - `prediction`: average of `s(x)` in bin b\n#' - `score_class`: decile level of bin b\n#' - `nb`: number of observation in bin b\ncalib_curve_quant &lt;- function(obs,\n                              scores,\n                              k = 10,\n                              threshold = .5,\n                              add_conf_int = FALSE,\n                              prob = .95, \n                              method = \"probit\") {\n  \n  summary_bins_calib &lt;- get_summary_bins(obs = obs, scores = scores, k = k)\n  \n  if (add_conf_int == TRUE) {\n    new_k &lt;- nrow(summary_bins_calib)\n    prob_ic &lt;- tibble(\n      mean_obs = rep(NA, new_k),\n      lower_obs = rep(NA, new_k),\n      upper_obs = rep(NA, new_k)\n    )\n    for (i in 1:new_k) {\n      prob_ic[i, 1:3] &lt;- binom.confint(\n        x = summary_bins_calib$sum_obs[i],\n        n = summary_bins_calib$nb[i], \n        conf.level = prob,\n        methods = method\n      )[, c(\"mean\", \"lower\", \"upper\")]\n    }\n    \n    summary_bins_calib &lt;- \n      summary_bins_calib |&gt; \n      mutate(\n        lower_obs = prob_ic$lower_obs,\n        upper_obs = prob_ic$upper_obs\n      )\n    \n  }\n  \n  summary_bins_calib\n}\n\n\n#' Plot calibration curve obtained with quantile-defined bins\n#' \n#' @param calib_curve_quant tibble with calibration curve obtained with \n#'    quantile-defined bins\n#' @param title title of the plot\n#' @param colour colour for the calibration curve\n#' @param add_ci if `TRUE`, add error bars to the points (lower and upper \n#'   bounds must then be found inside `calib_curve_locfit`)\n#' @param add if `TRUE`, creates a new plot, else, add to an existing one\nplot_calib_curve_quant &lt;- function(calib_curve_quant, \n                                   title, \n                                   colour,\n                                   add_ci = FALSE,\n                                   add = FALSE) {\n  if (add == FALSE) {\n    plot(\n      0:1, 0:1,\n      type = \"l\", col = NULL,\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\",\n      ylab = \"Mean predicted probability\",\n      main = title\n    )\n  }\n  lines(\n    calib_curve_quant$mean_score, calib_curve_quant$mean_obs,\n    lwd = 2, col = colour, t = \"b\",\n  )\n  if (add_ci == TRUE) {\n    arrows(\n      x0 = calib_curve_quant$mean_score, \n      y0 = calib_curve_quant$lower_obs, \n      x1 = calib_curve_quant$mean_score, \n      y1 = calib_curve_quant$upper_obs,\n      angle = 90,length = .05, code = 3,\n      col = colour\n    )\n  }\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n3.5.3.2 Local Regression Calibration Curve\nSimilarly, we define two functions: calib_curve_locfit() and plot_calib_curve_locfit().\n\n#' Calibration curve obtained with local regression\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param nn fraction of nearest neighbors\n#' @param deg degree for the local regression\n#' @param linspace vector of values at which to compute averages\ncalib_curve_locfit &lt;- function(obs, \n                               scores,\n                               nn = 0.15,\n                               deg = 0,\n                               linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(score, nn = nn, deg = deg), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = obs, score = scores)\n  )\n  score_locfit_0 &lt;- predict(locfit_0, newdata = linspace)\n  score_locfit_0[score_locfit_0 &lt; 0] &lt;- 0\n  score_locfit_0[score_locfit_0 &gt; 1] &lt;- 1\n  \n  tibble(\n    xlim = linspace,\n    score_c = score_locfit_0\n  )\n  \n}\n\n\n#' Plot calibration curve obtained with local regression\n#' \n#' @param calib_curve_locfit tibble with calibration curve obtained with local\n#'    regression\n#' @param title title of the plot\n#' @param colour colour for the calibration curve\n#' @param add if `TRUE`, creates a new plot, else, add to an existing one\nplot_calib_curve_locfit &lt;- function(calib_curve_locfit, \n                                    title, \n                                    colour,\n                                    add = FALSE) {\n  if (add == FALSE) {\n    plot(\n      0:1, 0:1,\n      type = \"l\", col = NULL,\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\",\n      ylab = \"Mean predicted probability\",\n      main = title\n    )\n  }\n  lines(\n    calib_curve_locfit$xlim, calib_curve_locfit$score_c,\n    lwd = 2, col = colour, type = \"l\",\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n3.5.3.3 Moving Average Calibration Curve\nSimilarly, we define two functions: calib_curve_ma() and plot_calib_curve_ma().\n\n#' @param nn fraction of nearest neighbors to use for confidence interval \n#'   (default to `.15`)\n#' @param prob confidence interval level (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination \n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\", \n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @param linspace vector of values at which to compute the averages\ncalib_curve_ma &lt;- function(obs, \n                           scores,\n                           nn = .15,\n                           prob = .95, \n                           method = \"probit\",\n                           linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs,\n      scores = scores,\n      tau = .x, \n      nn = nn, prob = prob, method = method)\n  ) |&gt; \n    list_rbind()\n}\n\n\n#' Plot calibration curve obtained with moving average\n#' \n#' @param calib_curve_ma tibble with calibration curve obtained with \n#'    quantile-defined bins\n#' @param title title of the plot\n#' @param colour colour for the calibration curve\n#' @param add_ci if `TRUE`, add error bars to the points (lower and upper \n#'   bounds must then be found inside `calib_curve_locfit`)\n#' @param add if `TRUE`, creates a new plot, else, add to an existing one\nplot_calib_curve_ma &lt;- function(calib_curve_ma, \n                                title, \n                                colour,\n                                add_ci = FALSE,\n                                add = FALSE) {\n  if (add == FALSE) {\n    plot(\n      0:1, 0:1,\n      type = \"l\", col = NULL,\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\",\n      ylab = \"Mean predicted probability\",\n      main = title\n    )\n  }\n  lines(\n    calib_curve_ma$xlim, calib_curve_ma$mean,\n    lwd = 2, col = colour, type = \"l\", pch = 19\n  )\n  if (add_ci == TRUE) {\n    polygon(\n      c(calib_curve_ma$xlim, rev(calib_curve_ma$xlim)),\n      c(calib_curve_ma$lower, rev(calib_curve_ma$upper)),\n      col = adjustcolor(col = colour, alpha.f = .4),\n      border = NA\n    )\n  }\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\n3.5.4 Calculate the Calibration Curves\nLet us now get the calibration curves for both forests, using in turn each of the three techniques. We will need the following packages:\n\nlibrary(locfit)\n\n\nQuantile-basedLocal RegressionMoving Average\n\n\n\nRegressionClassification\n\n\n\ncalib_curve_quant_reg_train &lt;- calib_curve_quant(\n  obs = tb_train$d,\n  scores = scores_reg$scores_train, \n  k = 10, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"train\") \n\ncalib_curve_quant_reg_calib &lt;- calib_curve_quant(\n  obs = tb_calib$d,\n  scores = scores_reg$scores_calib, \n  k = 5, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalib_curve_quant_reg_test &lt;- calib_curve_quant(\n  obs = tb_test$d,\n  scores = scores_reg$scores_test, \n  k = 5, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"test\")\n\n\n\n\ncalib_curve_quant_classif_train &lt;- calib_curve_quant(\n  obs = tb_train$d,\n  scores = scores_classif$scores_train, \n  k = 10, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"train\")\n\ncalib_curve_quant_classif_calib &lt;- calib_curve_quant(\n  obs = tb_calib$d,\n  scores = scores_classif$scores_calib, \n  k = 5, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalib_curve_quant_classif_test &lt;- calib_curve_quant(\n  obs = tb_test$d,\n  scores = scores_classif$scores_test, \n  k = 5, threshold = .5,\n  add_conf_int = TRUE\n) |&gt; \n  mutate(sample = \"test\")\n\n\n\n\n\n\n\nRegressionClassification\n\n\n\ncalib_curve_locfit_reg_train &lt;- calib_curve_locfit(\n  obs = tb_train$d, \n  scores = scores_reg$scores_train, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"train\") \n\ncalib_curve_locfit_reg_calib &lt;- calib_curve_locfit(\n  obs = tb_calib$d, \n  scores = scores_reg$scores_calib, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"calibration\") \n\ncalib_curve_locfit_reg_test &lt;- calib_curve_locfit(\n  obs = tb_test$d, \n  scores = scores_reg$scores_test, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"test\") \n\n\n\n\ncalib_curve_locfit_classif_train &lt;- calib_curve_locfit(\n  obs = tb_train$d, \n  scores = scores_classif$scores_train, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"train\") \n\ncalib_curve_locfit_classif_calib &lt;- calib_curve_locfit(\n  obs = tb_calib$d, \n  scores = scores_classif$scores_calib, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"calibration\") \n\ncalib_curve_locfit_classif_test &lt;- calib_curve_locfit(\n  obs = tb_test$d, \n  scores = scores_classif$scores_test, \n  nn = 0.15, \n  deg = 0, linspace = NULL\n) |&gt; mutate(sample = \"test\") \n\n\n\n\n\n\n\nRegressionClassification\n\n\n\ncalib_curve_ma_reg_train &lt;- calib_curve_ma(\n  obs = tb_train$d,\n  scores = scores_reg$scores_train\n) |&gt; \n  mutate(sample = \"train\")\n\ncalib_curve_ma_reg_calib &lt;- calib_curve_ma(\n  obs = tb_calib$d,\n  scores = scores_reg$scores_calib\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalib_curve_ma_reg_test &lt;- calib_curve_ma(\n  obs = tb_test$d,\n  scores = scores_reg$scores_test\n) |&gt; \n  mutate(sample = \"test\")\n\n\n\n\ncalib_curve_ma_classif_train &lt;- calib_curve_ma(\n  obs = tb_train$d,\n  scores = scores_classif$scores_train\n) |&gt; \n  mutate(sample = \"train\")\n\ncalib_curve_ma_classif_calib &lt;- calib_curve_ma(\n  obs = tb_calib$d,\n  scores = scores_classif$scores_calib\n) |&gt; \n  mutate(sample = \"calibration\")\n\ncalib_curve_ma_classif_test &lt;- calib_curve_ma(\n  obs = tb_test$d,\n  scores = scores_classif$scores_test\n) |&gt; \n  mutate(sample = \"test\")\n\n\n\n\n\n\n\n\n\n3.5.5 Display the Calibration Curves\nNow, we can plot the calibration curves. We will plot the calibration curve of each sample (train, calibration, and test) on the same graph, showing the results from the regression forest on the left, and the classification forest on the right.\n\nQuantile-basedLocal RegressionMoving Average\n\n\n\n\nDisplay the R codes used to create the Figure.\nmat &lt;- matrix(\n  c(1, 2,\n    3, 4,\n    5, 5), ncol = 2, byrow = TRUE)\nlayout(mat, heights = c(.2, .7, .1))\n\n# Histograms left\npar(mar = c(1.1, 4.1, 3.1, 2.1))\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n# Histograms right\n\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n# Regression case\npar(mar = c(4.1, 4.1, 3.1, 2.1))\nplot_calib_curve_quant(\n  calib_curve_quant_reg_train, \n  title = \"Regression\", \n  colour = colours[\"Train\"], add_ci = TRUE\n)\nplot_calib_curve_quant(\n  calib_curve_quant_reg_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add_ci = TRUE, add = TRUE\n)\nplot_calib_curve_quant(\n  calib_curve_quant_reg_test, title = \"\", \n  colour = colours[\"Test\"],\n  add_ci = TRUE, add = TRUE\n)\n\n# Classification case\nplot_calib_curve_quant(\n  calib_curve_quant_classif_train, \n  title = \"Classification\", \n  colour = colours[\"Train\"], add_ci = TRUE\n)\nplot_calib_curve_quant(\n  calib_curve_quant_classif_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add_ci = TRUE, add = TRUE\n)\nplot_calib_curve_quant(\n  calib_curve_quant_classif_test, title = \"\", \n  colour = colours[\"Test\"],\n  add_ci = TRUE, add = TRUE\n)\n\npar(fig = c(0, 1, 0, 1),  oma = c(0, 0, 0, 0),  mar = c(0, 0, 0, 0), new = TRUE)\nplot(0, 0, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n')\nlegend(\n  \"bottom\", \n  legend = c(\"Train\", \"Calibration\", \"Test\"), \n  col =  colours,\n  lty = 1, lwd = 2,\n  horiz = TRUE, xpd = TRUE\n)\n\n\n\n\nFigure 3.1: Calibration curve (bottom) calculated using quantile-defined bins for the Random Forest, for the train set, the calibration set and the test set, in a regression context (left) or a classification context (right). The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmat &lt;- matrix(\n  c(1, 2,\n    3, 4,\n    5, 5), ncol = 2, byrow = TRUE)\nlayout(mat, heights = c(.2, .7, .1))\n\n# Histograms left\npar(mar = c(1.1, 4.1, 3.1, 2.1))\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n# Histograms right\n\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n# Regression case\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot_calib_curve_locfit(\n  calib_curve_locfit_reg_train, \n  title = \"Regression\", \n  colour = colours[\"Train\"]\n)\nplot_calib_curve_locfit(\n  calib_curve_locfit_reg_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add = TRUE\n)\nplot_calib_curve_locfit(\n  calib_curve_locfit_reg_test, title = \"\", \n  colour = colours[\"Test\"],\n  add = TRUE\n)\n\n# Classification case\nplot_calib_curve_locfit(\n  calib_curve_locfit_classif_train, \n  title = \"Classification\", \n  colour = colours[\"Train\"]\n)\nplot_calib_curve_locfit(\n  calib_curve_locfit_classif_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add = TRUE\n)\nplot_calib_curve_locfit(\n  calib_curve_locfit_classif_test, title = \"\", \n  colour = colours[\"Test\"],\n  add = TRUE\n)\n\npar(fig = c(0, 1, 0, 1),  oma = c(0, 0, 0, 0),  mar = c(0, 0, 0, 0), new = TRUE)\nplot(0, 0, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n')\nlegend(\n  \"bottom\", \n  legend = c(\"Train\", \"Calibration\", \"Test\"), \n  col =  colours,\n  lty = 1, lwd = 2,\n  horiz = TRUE, xpd = TRUE\n)\n\n\n\n\nFigure 3.2: Calibration curve (bottom) calculated using local regression for the Random Forest, for the train set, the calibration set and the test set, in a regression context (left) or a classification context (right). The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmat &lt;- matrix(\n  c(1, 2,\n    3, 4,\n    5, 5), ncol = 2, byrow = TRUE)\nlayout(mat, heights = c(.2, .7, .1))\n\n# Histograms left\npar(mar = c(1.1, 4.1, 3.1, 2.1))\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n# Histograms right\n\nhist(\n  data$true_probas_train$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Train\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n)\nhist(\n  data$true_probas_calib$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Calibration\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"True Probabilities\",\n  add = TRUE\n)\nhist(\n  data$true_probas_test$p,\n  breaks = seq(0, 1, by = .05),\n  col = adjustcolor(colours[\"Test\"], alpha.f = .2), border = \"white\",\n  axes = FALSE,\n  xlab = \"\", ylab = \"\", main = \"\",\n  add = TRUE\n)\n\n# Regression case\npar(mar = c(4.1, 4.1, 3.1, 2.1))\nplot_calib_curve_ma(\n  calib_curve_ma_reg_train, \n  title = \"Regression\", \n  colour = colours[\"Train\"], add_ci = TRUE\n)\nplot_calib_curve_ma(\n  calib_curve_ma_reg_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add_ci = TRUE, add = TRUE\n)\nplot_calib_curve_ma(\n  calib_curve_ma_reg_test, title = \"\", \n  colour = colours[\"Test\"],\n  add_ci = TRUE, add = TRUE\n)\n\n# Classification case\nplot_calib_curve_ma(\n  calib_curve_ma_classif_train, \n  title = \"Classification\", \n  colour = colours[\"Train\"], add_ci = TRUE\n)\nplot_calib_curve_ma(\n  calib_curve_ma_classif_calib, title = \"\", \n  colour = colours[\"Calibration\"],\n  add_ci = TRUE, add = TRUE\n)\nplot_calib_curve_ma(\n  calib_curve_ma_classif_test, title = \"\", \n  colour = colours[\"Test\"],\n  add_ci = TRUE, add = TRUE\n)\n\npar(fig = c(0, 1, 0, 1),  oma = c(0, 0, 0, 0),  mar = c(0, 0, 0, 0), new = TRUE)\nplot(0, 0, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n')\nlegend(\n  \"bottom\", \n  legend = c(\"Train\", \"Calibration\", \"Test\"), \n  col =  colours,\n  lty = 1, lwd = 2,\n  horiz = TRUE, xpd = TRUE\n)\n\n\n\n\nFigure 3.3: Calibration curve (bottom) calculated using moving averages for the Random Forest, for the train set, the calibration set and the test set, in a regression context (left) or a classification context (right). The distribution of the true probabilities are shown in the histograms (on top)."
  },
  {
    "objectID": "rf-single.html#recalibrate-the-random-forest",
    "href": "rf-single.html#recalibrate-the-random-forest",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.6 Recalibrate the Random Forest",
    "text": "3.6 Recalibrate the Random Forest\nNow, we will recalibrate the random forests, using the various techniques explained in Chapter 2.\nWe will consider the following techniques:\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\nparams &lt;- list(\n  NULL, NULL, NULL, \n  list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n)\nmethod_names &lt;- c(\n  \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\n\nThe function recalibrate() will learn a recalibration on the calibration set and then return the recalibrated scores on the training set, the calibration set, and the test set.\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_train vector of observed events in the train set\n#' @param scores_train vector of predicted probabilities in the train set\n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression, `\"beta\"` for beta calibration, \n#'   `\"locfit\"` for local regression)\n#' @param params list of named parameters to use in the local regression \n#'   (`nn` for fraction of nearest neighbors to use, `deg` for degree)\n#' @param linspace vector of alues at which to compute the recalibrated scores\n#' @returns list of three elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set, and recalibrated scores on a segment \n#'   of values\nrecalibrate &lt;- function(obs_train,\n                        pred_train, \n                        obs_calib,\n                        pred_calib,\n                        obs_test,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\", \"beta\", \"locfit\"),\n                        params = NULL,\n                        linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  data_train &lt;- tibble(d = obs_train, scores = pred_train)\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  # Recalibrator trained on calibration data\n  if (method == \"platt\") {\n    # Recalibrator\n    lr &lt;- glm(\n      d ~ scores, family = binomial(link = 'logit'), data = data_calib\n    )\n    # Recalibrated scores on calib/test sets\n    score_c_train &lt;- predict(lr, newdata = data_train, type = \"response\")\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n    # Recalibrated scores on [0,1]\n    score_c_linspace &lt;- predict(\n      lr, newdata = tibble(scores = linspace), type = \"response\"\n    )\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_train &lt;- fit_iso(data_train$scores)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    score_c_linspace &lt;- fit_iso(linspace)\n  } else if (method == \"beta\") {\n    capture.output({\n      bc &lt;- try(beta_calibration(\n        p = data_calib$scores, \n        y = data_calib$d, \n        parameters = \"abm\" # 3 parameters a, b & m\n      ))\n    })\n    if (!inherits(bc, \"try-error\")) {\n      score_c_train &lt;- beta_predict(p = data_train$scores, bc)\n      score_c_calib &lt;- beta_predict(p = data_calib$scores, bc)\n      score_c_test &lt;- beta_predict(p = data_test$scores, bc)\n      score_c_linspace &lt;- beta_predict(p = linspace, bc)\n    } else {\n      score_c_train &lt;- score_c_calib &lt;- score_c_test &lt;- score_c_linspace &lt;- NA\n    }\n    \n  } else if (method == \"locfit\") {\n    locfit_reg &lt;- locfit(\n      formula = d ~ lp(scores, nn = params$nn, deg = params$deg), \n      kern = \"rect\", maxk = 200, data = data_calib\n    )\n    score_c_train &lt;- predict(locfit_reg, newdata = data_train)\n    score_c_train[score_c_train &lt; 0] &lt;- 0\n    score_c_train[score_c_train &gt; 1] &lt;- 1\n    score_c_calib &lt;- predict(locfit_reg, newdata = data_calib)\n    score_c_calib[score_c_calib &lt; 0] &lt;- 0\n    score_c_calib[score_c_calib &gt; 1] &lt;- 1\n    score_c_test &lt;- predict(locfit_reg, newdata = data_test)\n    score_c_test[score_c_test &lt; 0] &lt;- 0\n    score_c_test[score_c_test &gt; 1] &lt;- 1\n    score_c_linspace &lt;- predict(locfit_reg, newdata = linspace)\n    score_c_linspace[score_c_linspace &lt; 0] &lt;- 0\n    score_c_linspace[score_c_linspace &gt; 1] &lt;- 1\n  } else {\n    stop(str_c(\n      'Wrong method. Use one of the following:',\n      '\"platt\", \"isotonic\", \"beta\", \"locfit\"'\n    ))\n  }\n  \n  # Format results in tibbles:\n  # For train set\n  tb_score_c_train &lt;- tibble(\n    d = obs_train,\n    p_u = pred_train,\n    p_c = score_c_train\n  )\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  # For linear space\n  tb_score_c_linspace &lt;- tibble(\n    linspace = linspace,\n    p_c = score_c_linspace\n  )\n  \n  list(\n    tb_score_c_train = tb_score_c_train,\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test,\n    tb_score_c_linspace = tb_score_c_linspace\n  )\n  \n}\n\nLet us apply the recalibration methods on our two random forests. We initialize empty objects that will contain the recalibrated scores: scores_c_reg and scores_c_classif.\n\nscores_c_reg &lt;- vector(mode = \"list\", length = length(methods))\nscores_c_classif &lt;- vector(mode = \"list\", length = length(methods))\nnames(scores_c_reg) &lt;- names(scores_c_classif) &lt;- method_names\n\nThen, we loop on each name of method to apply and call the recalibrate() function at each iteration, on both forests.\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  params_current &lt;- params[[i_method]]\n  \n  # Regression\n  scores_c_reg[[i_method]] &lt;- recalibrate(\n    obs_train = tb_train$d, \n    pred_train = scores_reg$scores_train, \n    obs_calib = tb_calib$d, \n    pred_calib = scores_reg$scores_calib, \n    obs_test = tb_test$d, \n    pred_test = scores_reg$scores_test, \n    method = method,\n    params = params_current\n  )\n  \n  # Classification\n  scores_c_classif[[i_method]] &lt;- recalibrate(\n    obs_train = tb_train$d, \n    pred_train = scores_classif$scores_train, \n    obs_calib = tb_calib$d, \n    pred_calib = scores_classif$scores_calib, \n    obs_test = tb_test$d, \n    pred_test = scores_classif$scores_test, \n    method = method,\n    params = params_current\n  )\n}\n\nError in beta_calibration(p = data_calib$scores, y = data_calib$d, parameters = \"abm\") : \n  could not find function \"beta_calibration\"\nError in beta_calibration(p = data_calib$scores, y = data_calib$d, parameters = \"abm\") : \n  could not find function \"beta_calibration\""
  },
  {
    "objectID": "rf-single.html#measuring-calibration-after-recalibration",
    "href": "rf-single.html#measuring-calibration-after-recalibration",
    "title": "3  (Re)Calibration of a Random Forest",
    "section": "3.7 Measuring Calibration After Recalibration",
    "text": "3.7 Measuring Calibration After Recalibration\n\n3.7.1 With Metrics\n\nGoodness of FitCalibration Metrics\n\n\n\nRegressionClassification\n\n\n\ngof_c_reg_train &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_train$p, \n    obs = tb_train$d, \n    pred = .x$tb_score_c_train$p_c\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"train\")\n\ngof_c_reg_calib &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_calib$p, \n    obs = tb_calib$d, \n    pred = .x$tb_score_c_calib$p_c\n  ) \n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"calibration\")\n\ngof_c_reg_test &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_test$p, \n    obs = tb_test$d, \n    pred = .x$tb_score_c_test$p_c\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"test\")\n\ngof_c_reg &lt;- \n  gof_c_reg_train |&gt; \n  bind_rows(gof_c_reg_calib) |&gt; \n  bind_rows(gof_c_reg_test)\n\n\n\n\ngof_c_classif_train &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_train$p, \n    obs = tb_train$d, \n    pred = .x$tb_score_c_train$p_c\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"train\")\n\ngof_c_classif_calib &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_calib$p, \n    obs = tb_calib$d, \n    pred = .x$tb_score_c_calib$p_c\n  ) \n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"calibration\")\n\ngof_c_classif_test &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_gof(\n    true_prob = data$true_probas_test$p, \n    obs = tb_test$d, \n    pred = .x$tb_score_c_test$p_c\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"test\")\n\ngof_c_classif &lt;- \n  gof_c_classif_train |&gt; \n  bind_rows(gof_c_classif_calib) |&gt; \n  bind_rows(gof_c_classif_test)\n\n\n\n\n\n\n\nRegressionClassification\n\n\n\ncalibration_c_reg_train &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_metrics(\n    obs = tb_train$d, \n    scores = .x$tb_score_c_train$p_c,\n    true_prob = data$true_probas_train$p,\n    k = 10\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"train\")\n\ncalibration_c_reg_calib &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_metrics(\n    obs = tb_calib$d, \n    scores = .x$tb_score_c_calib$p_c,\n    true_prob = data$true_probas_calib$p,\n    k = 5\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"calibration\")\n\ncalibration_c_reg_test &lt;- map(\n  .x = scores_c_reg,\n  .f = ~compute_metrics(\n    obs = tb_test$d, \n    scores = .x$tb_score_c_test$p_c,\n    true_prob = data$true_probas_test$p,\n    k = 5\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"test\")\n\n\ncalibration_c_reg &lt;- \n  calibration_c_reg_train |&gt; \n  bind_rows(calibration_c_reg_calib) |&gt; \n  bind_rows(calibration_c_reg_test)\n\n\n\n\ncalibration_c_classif_train &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_metrics(\n    obs = tb_train$d, \n    scores = .x$tb_score_c_train$p_c,\n    true_prob = data$true_probas_train$p,\n    k = 10\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"train\")\n\ncalibration_c_classif_calib &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_metrics(\n    obs = tb_calib$d, \n    scores = .x$tb_score_c_calib$p_c,\n    true_prob = data$true_probas_calib$p,\n    k = 5\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"calibration\")\n\ncalibration_c_classif_test &lt;- map(\n  .x = scores_c_classif,\n  .f = ~compute_metrics(\n    obs = tb_test$d, \n    scores = .x$tb_score_c_test$p_c,\n    true_prob = data$true_probas_test$p,\n    k = 5\n  )\n) |&gt; \n  list_rbind(names_to = \"method\") |&gt; \n  mutate(sample = \"test\")\n\n\ncalibration_c_classif &lt;- \n  calibration_c_classif_train |&gt; \n  bind_rows(calibration_c_classif_calib) |&gt; \n  bind_rows(calibration_c_classif_test)\n\n\n\n\n\n\n\nWe bind the goodness of fit metrics of the two forests in a single tibble:\n\ngof_c &lt;- gof_c_reg |&gt; mutate(model = \"regression\") |&gt; \n  bind_rows(gof_c_classif |&gt; mutate(model = \"classification\"))\n\nAnd we do the same for the calibration metrics:\n\ncalibration_c &lt;- \n  calibration_c_reg |&gt; mutate(model = \"regression\") |&gt; \n  bind_rows(\n    calibration_c_classif |&gt; mutate(model = \"classification\")\n  )\n\nThen, we merge all these metrics in a single tibble:\n\nsummary_metrics_c &lt;- \n  gof_c |&gt; \n  left_join(calibration_c, by = c(\"mse\", \"sample\", \"model\", \"method\")) |&gt; \n  relocate(sample, model, method, .before = \"mse\") |&gt; \n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"calibration\", \"test\"),\n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    ),\n    model = factor(\n      model,\n      levels = c(\"regression\", \"classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = c(\n        \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n      ),\n      labels = c(\n        \"Platt Scaling\", \"Isotonic\", \"Beta\", \"Locfit (deg=0)\", \n        \"Locfit (deg=1)\", \"Locfit (deg=2)\"\n      )\n    )\n  ) |&gt; \n  arrange(model, sample, method)\n\n\n\nDisplay the R codes used to produce the Table.\nsummary_metrics_c |&gt; select(-model) |&gt; \n  select(\n    Sample = sample, \n    Method = method, `True MSE` = mse, \n    `Brier Score` = brier, ECE = ece, QMSE = qmse, WMSE = wmse,\n    Accuracy = accuracy,\n    `True Pos. Rate` = sensitivity, `False Pos. Rate` = FPR) |&gt; \n  knitr::kable(escape = FALSE, booktabs = T, digits = 3) |&gt; \n  kableExtra::pack_rows(index = table(summary_metrics_c$model)) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable 3.2: Goodness of fit and Calibration After Recalibration.\n\n\nSample\nMethod\nTrue MSE\nBrier Score\nECE\nQMSE\nWMSE\nAccuracy\nTrue Pos. Rate\nFalse Pos. Rate\n\n\n\n\nRegression\n\n\nTrain\nPlatt Scaling\n0.017\n0.268\n0.279\n0.128\n0.318\n0.461\n0.914\n1.000\n\n\nTrain\nIsotonic\n0.023\n0.235\n0.051\n0.003\n0.007\n0.504\n1.000\n1.000\n\n\nTrain\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nTrain\nLocfit (deg=0)\n0.019\n0.268\n0.204\n0.065\n0.113\n0.469\n0.595\n0.659\n\n\nTrain\nLocfit (deg=1)\n0.026\n0.266\n0.140\n0.034\n0.075\n0.483\n0.636\n0.672\n\n\nTrain\nLocfit (deg=2)\n0.025\n0.266\n0.123\n0.033\n0.083\n0.475\n0.658\n0.711\n\n\nCalibration\nPlatt Scaling\n0.014\n0.248\n0.031\n0.001\n0.020\n0.535\n0.977\n0.984\n\n\nCalibration\nIsotonic\n0.015\n0.245\n0.000\n0.000\n0.026\n0.540\n1.000\n1.000\n\n\nCalibration\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nCalibration\nLocfit (deg=0)\n0.015\n0.244\n0.064\n0.005\n0.011\n0.588\n0.690\n0.533\n\n\nCalibration\nLocfit (deg=1)\n0.019\n0.242\n0.068\n0.002\n0.016\n0.585\n0.722\n0.576\n\n\nCalibration\nLocfit (deg=2)\n0.020\n0.240\n0.054\n0.001\n0.011\n0.580\n0.764\n0.636\n\n\nTest\nPlatt Scaling\n0.015\n0.252\n0.069\n0.005\n0.046\n0.495\n0.975\n0.980\n\n\nTest\nIsotonic\n0.017\n0.255\n0.043\n0.002\n0.039\n0.498\n1.000\n1.000\n\n\nTest\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nTest\nLocfit (deg=0)\n0.018\n0.252\n0.064\n0.005\n0.034\n0.522\n0.588\n0.542\n\n\nTest\nLocfit (deg=1)\n0.021\n0.256\n0.077\n0.010\n0.041\n0.510\n0.603\n0.582\n\n\nTest\nLocfit (deg=2)\n0.022\n0.255\n0.085\n0.004\n0.028\n0.532\n0.663\n0.597\n\n\nClassification\n\n\nTrain\nPlatt Scaling\n0.017\n0.261\n0.217\n0.070\n0.198\n0.499\n0.990\n1.000\n\n\nTrain\nIsotonic\n0.020\n0.242\n0.128\n0.022\n0.018\n0.507\n1.000\n0.993\n\n\nTrain\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nTrain\nLocfit (deg=0)\n0.017\n0.254\n0.118\n0.027\n0.065\n0.533\n0.727\n0.664\n\n\nTrain\nLocfit (deg=1)\n0.020\n0.256\n0.103\n0.012\n0.035\n0.542\n0.669\n0.588\n\n\nTrain\nLocfit (deg=2)\n0.026\n0.263\n0.129\n0.021\n0.050\n0.496\n0.579\n0.588\n\n\nCalibration\nPlatt Scaling\n0.014\n0.248\n0.029\n0.001\n0.027\n0.540\n1.000\n1.000\n\n\nCalibration\nIsotonic\n0.015\n0.245\n0.000\n0.000\n0.033\n0.540\n0.995\n0.995\n\n\nCalibration\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nCalibration\nLocfit (deg=0)\n0.016\n0.243\n0.063\n0.003\n0.013\n0.593\n0.829\n0.685\n\n\nCalibration\nLocfit (deg=1)\n0.018\n0.241\n0.090\n0.004\n0.011\n0.605\n0.787\n0.609\n\n\nCalibration\nLocfit (deg=2)\n0.023\n0.238\n0.107\n0.003\n0.012\n0.608\n0.713\n0.516\n\n\nTest\nPlatt Scaling\n0.015\n0.252\n0.042\n0.003\n0.027\n0.498\n1.000\n1.000\n\n\nTest\nIsotonic\n0.017\n0.252\n0.042\n0.002\n0.047\n0.498\n1.000\n1.000\n\n\nTest\nBeta\nNA\nNA\nNA\nNA\nNA\n0.000\n0.000\n0.000\n\n\nTest\nLocfit (deg=0)\n0.015\n0.249\n0.057\n0.004\n0.020\n0.527\n0.744\n0.687\n\n\nTest\nLocfit (deg=1)\n0.017\n0.250\n0.053\n0.002\n0.019\n0.537\n0.658\n0.582\n\n\nTest\nLocfit (deg=2)\n0.020\n0.249\n0.096\n0.002\n0.020\n0.555\n0.633\n0.522\n\n\n\n\n\n\n\n\nLet us print the results obtained before calibration (in Table 3.1) here as well, for comparison:\n\n\nDisplay the R codes used to produce the Table.\nsummary_metrics |&gt; select(-model) |&gt; \n  select(\n    Sample = sample, \n    `True MSE` = mse, \n    `Brier Score` = brier, ECE = ece, QMSE = qmse, WMSE = wmse,\n    Accuracy = accuracy,\n    `True Pos. Rate` = sensitivity, `False Pos. Rate` = FPR) |&gt; \n  knitr::kable(escape = FALSE, booktabs = T, digits = 3) |&gt; \n  kableExtra::pack_rows(index = table(summary_metrics$model)) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable 3.3: Goodness of fit and Calibration Before Recalibration.\n\n\nSample\nTrue MSE\nBrier Score\nECE\nQMSE\nWMSE\nAccuracy\nTrue Pos. Rate\nFalse Pos. Rate\n\n\n\n\nRegression\n\n\nTrain\n0.016\n0.210\n0.294\n0.071\n0.069\n0.798\n0.815\n0.218\n\n\nCalibration\n0.015\n0.256\n0.033\n0.009\n0.057\n0.485\n0.519\n0.554\n\n\nTest\n0.017\n0.253\n0.085\n0.006\n0.028\n0.520\n0.568\n0.527\n\n\nClassification\n\n\nTrain\n0.034\n0.200\n0.199\n0.009\n0.007\n0.705\n0.722\n0.313\n\n\nCalibration\n0.035\n0.281\n0.113\n0.028\n0.066\n0.475\n0.477\n0.527\n\n\nTest\n0.038\n0.276\n0.148\n0.023\n0.060\n0.495\n0.538\n0.547\n\n\n\n\n\n\n\n\n\n\n3.7.2 With Calibration Maps\nNow, we can turn to visualization of the calibration curves.\n\n\n\n\n\n\nNote\n\n\n\nIf a calibration curve is missing, it was not possible to compute the recalibrated it.\n\n\n\n3.7.2.1 Define Visualization Functions\n\nQuantile-based Calibration CurveMoving Average Calibration Curve\n\n\n\nplot_recalib_curve_quant &lt;- function(method,\n                                     method_label,\n                                     curve_calibration, \n                                     curve_test, \n                                     curve_c_calibration, \n                                     curve_c_test,\n                                     title = NULL) {\n  mat &lt;- matrix(c(\n    # Title\n    1, 1, \n    # Subtitle\n    2, 3,\n    # Histograms,\n    4, 5,\n    # Calibration curves\n    6, 7,\n    8, 9\n    # # Legend\n    # 10, 10\n  ), ncol = 2, byrow = TRUE)\n  heights &lt;- c(.1, .1, .2, .6, .6)\n  \n  if (is.null(title)) {\n    mat &lt;- (mat - 1)[-1,]\n    heights &lt;- heights[-1]\n  }\n  layout(mat, heights = heights)\n  \n  par(oma = c(0, 0, 0, 0),  mar = c(0, 0, 0, 0))\n  if (!is.null(title)) {\n    plot.new()\n    text(\n      0.5,0.5,\n      title,\n      cex=1.6,font=2\n    )\n  }\n  plot.new()\n  text(\n    0.5,0.5,\n    \"Calibration set\", \n    cex=1.6,font=2\n  )\n  plot.new()\n  text(\n    0.5,0.5,\n    \"Test set\", \n    cex=1.5,font=2\n  )\n  \n  # Distribution of True probabilities on top\n  par(mar = c(1.1, 4.1, 3.1, 2.1))\n  hist(\n    data$true_probas_calib$p,\n    breaks = seq(0, 1, by = .05),\n    col = colours[\"Calibration\"], border = \"white\",\n    axes = FALSE,\n    xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n  )\n  hist(\n    data$true_probas_test$p,\n    breaks = seq(0, 1, by = .05),\n    col = colours[\"Test\"], border = \"white\",\n    axes = FALSE,\n    xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n  )\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot_calib_curve_quant(\n    curve_calibration, \n    title = \"Uncalibrated\", \n    colour = colours[\"Calibration\"], add_ci = TRUE\n  )\n  \n  plot_calib_curve_quant(\n    curve_test, \n    title = \"Uncalibrated\", \n    colour = colours[\"Test\"],\n    add_ci = TRUE, add = FALSE\n  )\n  \n  if (is.null(curve_c_calibration[[method]])) {\n    plot.new()\n  } else {\n    plot_calib_curve_quant(\n      curve_c_calibration[[method]], \n      title = \"Recalibrated\", \n      colour = colours[\"Calibration\"], add_ci = TRUE, add = FALSE\n    )\n  }\n  \n  if (is.null(curve_c_calibration[[method]])) {\n    plot.new()\n  } else {\n    plot_calib_curve_quant(\n      curve_c_test[[method]], \n      title = \"Recalibrated\", \n      colour = colours[\"Test\"],\n      add_ci = TRUE, add = FALSE\n    )\n  }\n}\n\n\n\n\nplot_recalib_curve_ma &lt;- function(method,\n                                  method_label,\n                                  curve_calibration, \n                                  curve_test, \n                                  curve_c_calibration, \n                                  curve_c_test,\n                                  title = NULL) {\n  mat &lt;- matrix(c(\n    # Title\n    1, 1, \n    # Subtitle\n    2, 3,\n    # Histograms,\n    4, 5,\n    # Calibration curves\n    6, 7,\n    8, 9\n    # # Legend\n    # 10, 10\n    ), ncol = 2, byrow = TRUE)\n  heights &lt;- c(.1, .1, .2, .6, .6)\n  \n  if (is.null(title)) {\n    mat &lt;- (mat - 1)[-1,]\n    heights &lt;- heights[-1]\n  }\n  layout(mat, heights = heights)\n  \n  par(oma = c(0, 0, 0, 0),  mar = c(0, 0, 0, 0))\n  if (!is.null(title)) {\n    plot.new()\n    text(\n      0.5,0.5,\n      title,\n      cex=1.6,font=2\n    )\n  }\n  plot.new()\n  text(\n    0.5,0.5,\n    \"Calibration set\", \n    cex=1.6,font=2\n  )\n  plot.new()\n  text(\n    0.5,0.5,\n    \"Test set\", \n    cex=1.5,font=2\n  )\n  \n  # Distribution of True probabilities on top\n  par(mar = c(1.1, 4.1, 3.1, 2.1))\n  hist(\n    data$true_probas_calib$p,\n    breaks = seq(0, 1, by = .05),\n    col = colours[\"Calibration\"], border = \"white\",\n    axes = FALSE,\n    xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n  )\n  hist(\n    data$true_probas_test$p,\n    breaks = seq(0, 1, by = .05),\n    col = colours[\"Test\"], border = \"white\",\n    axes = FALSE,\n    xlab = \"\", ylab = \"\", main = \"True Probabilities\"\n  )\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  \n  plot_calib_curve_ma(\n    curve_calibration, \n    title = \"Uncalibrated\", \n    colour = colours[\"Calibration\"], add_ci = TRUE\n  )\n  \n  plot_calib_curve_ma(\n    curve_test, \n    title = \"Uncalibrated\", \n    colour = colours[\"Test\"],\n    add_ci = TRUE, add = FALSE\n  )\n  \n  \n  plot_calib_curve_ma(\n    curve_c_calibration[[method]], \n    title = \"Recalibrated\", \n    colour = colours[\"Calibration\"], add_ci = TRUE, add = FALSE\n  )\n  \n  plot_calib_curve_ma(\n    curve_c_test[[method]], \n    title = \"Recalibrated\", \n    colour = colours[\"Test\"],\n    add_ci = TRUE, add = FALSE\n  )\n}\n\n\n\n\n\n\n3.7.2.2 Calculate the Calibration Curves\nFirst, we need to get the calibration curves. We will use the same function as earlier (calib_curve_quant() and calib_curve_ma()) and apply those to the recalibrated scores obtained with the various recalibration techniques.\n\nQuantile-basedMoving Average\n\n\n\nRegressionClassification\n\n\n\nconsider_reg_train &lt;- map(scores_c_reg, \"tb_score_c_train\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\nconsider_reg_calib &lt;- map(scores_c_reg, \"tb_score_c_calib\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\nconsider_reg_test &lt;- map(scores_c_reg, \"tb_score_c_test\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\n\ncalib_curve_quant_c_reg_train &lt;- map(\n  .x = scores_c_reg[which(consider_reg_train)],\n  .f = ~calib_curve_quant(\n    obs = tb_train$d,\n    scores = .x$tb_score_c_train$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\ncalib_curve_quant_c_reg_calib &lt;- map(\n  .x = scores_c_reg[which(consider_reg_calib)],\n  .f = ~calib_curve_quant(\n    obs = tb_calib$d,\n    scores = .x$tb_score_c_calib$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\ncalib_curve_quant_c_reg_test &lt;- map(\n  .x = scores_c_reg[which(consider_reg_test)],\n  .f = ~calib_curve_quant(\n    obs = tb_test$d,\n    scores = .x$tb_score_c_test$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\n\n\n\nconsider_classif_train &lt;- map(scores_c_classif, \"tb_score_c_train\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\nconsider_classif_calib &lt;- map(scores_c_classif, \"tb_score_c_calib\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\nconsider_classif_test &lt;- map(scores_c_classif, \"tb_score_c_test\") |&gt; \n  map(~.x |&gt; pull(p_c)) |&gt; \n  map_lgl(~all(!is.na(.x)))\n\ncalib_curve_quant_c_classif_train &lt;- map(\n  .x = scores_c_classif[which(consider_classif_train)],\n  .f = ~calib_curve_quant(\n    obs = tb_train$d,\n    scores = .x$tb_score_c_train$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\ncalib_curve_quant_c_classif_calib &lt;- map(\n  .x = scores_c_classif[which(consider_classif_calib)],\n  .f = ~calib_curve_quant(\n    obs = tb_calib$d,\n    scores = .x$tb_score_c_calib$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\ncalib_curve_quant_c_classif_test &lt;- map(\n  .x = scores_c_classif[which(consider_classif_test)],\n  .f = ~calib_curve_quant(\n    obs = tb_test$d,\n    scores = .x$tb_score_c_test$p_c, \n    k = 10, threshold = .5,\n    add_conf_int = TRUE\n  )\n)\n\n\n\n\n\n\n\nRegressionClassification\n\n\n\ncalib_curve_ma_c_reg_train &lt;- map(\n  .x = scores_c_reg[which(consider_reg_train)],\n  .f = ~calib_curve_ma(\n    obs = tb_train$d, \n    scores = .x$tb_score_c_train$p_c\n  )\n)\ncalib_curve_ma_c_reg_calib &lt;- map(\n  .x = scores_c_reg[which(consider_reg_calib)],\n  .f = ~calib_curve_ma(\n    obs = tb_calib$d, \n    scores = .x$tb_score_c_calib$p_c\n  )\n)\ncalib_curve_ma_c_reg_test &lt;- map(\n  .x = scores_c_reg[which(consider_reg_test)],\n  .f = ~calib_curve_ma(\n    obs = tb_test$d, \n    scores = .x$tb_score_c_test$p_c\n  )\n)\n\n\n\n\ncalib_curve_ma_c_classif_train &lt;- map(\n  .x = scores_c_classif[which(consider_classif_train)],\n  .f = ~calib_curve_ma(\n    obs = tb_train$d, \n    scores = .x$tb_score_c_train$p_c\n  )\n)\ncalib_curve_ma_c_classif_calib &lt;- map(\n  .x = scores_c_classif[which(consider_classif_train)],\n  .f = ~calib_curve_ma(\n    obs = tb_calib$d, \n    scores = .x$tb_score_c_calib$p_c\n  )\n)\ncalib_curve_ma_c_classif_test &lt;- map(\n  .x = scores_c_classif[which(consider_classif_train)],\n  .f = ~calib_curve_ma(\n    obs = tb_test$d, \n    scores = .x$tb_score_c_test$p_c\n  )\n)\n\n\n\n\n\n\n\n\n\n3.7.2.3 Display the Calibration Curves\nLet us first show the results for the random forest used in a regression case.\n\nQuantile-basedMoving Average\n\n\n\nPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"platt\"\nmethod_label &lt;- \"Platt Scaling\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.4: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Platt-scaling (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"isotonit\"\nmethod_label &lt;- \"Isotonic Regression\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.5: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Isotonit regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"beta\"\nmethod_label &lt;- \"Beta Regression\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.6: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Beta regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_0\"\nmethod_label &lt;- \"Local Regression (deg=0)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.7: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=0 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_1\"\nmethod_label &lt;- \"Local Regression (deg=1)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.8: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=1 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_2\"\nmethod_label &lt;- \"Local Regression (deg=2)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_reg_calib, \n  curve_test = calib_curve_quant_reg_test, \n  curve_c_calibration = calib_curve_quant_c_reg_calib, \n  curve_c_test = calib_curve_quant_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.9: Calibration curve calculated using quantile-defined bins for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=2 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\n\n\nPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"platt\"\nmethod_label &lt;- \"Platt Scaling\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.10: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Platt-scaling (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"isotonic\"\nmethod_label &lt;- \"Isotonic Regression\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.11: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using isotonic regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"beta\"\nmethod_label &lt;- \"Beta Calibration\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.12: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Beta calibration (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_0\"\nmethod_label &lt;- \"Local Regression (deg=0)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.13: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=0 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_1\"\nmethod_label &lt;- \"Local Regression (deg=1)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.14: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=1 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_2\"\nmethod_label &lt;- \"Local Regression (deg=2)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_reg_test, \n  curve_c_calibration = calib_curve_ma_c_reg_calib, \n  curve_c_test = calib_curve_ma_c_reg_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.15: Calibration curve calculated using moving averages for the Random Forest (regression), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=2 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\n\n\nThen, we can plot the calibration curves obtained with the forest trained in a classification context.\n\nQuantile-basedMoving Average\n\n\n\nPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"platt\"\nmethod_label &lt;- \"Platt Scaling\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.16: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Platt-scaling (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"isotonit\"\nmethod_label &lt;- \"Isotonic Regression\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.17: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Isotonit regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"beta\"\nmethod_label &lt;- \"Beta Regression\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.18: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Beta regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_0\"\nmethod_label &lt;- \"Local Regression (deg=0)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.19: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=0 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_1\"\nmethod_label &lt;- \"Local Regression (deg=1)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.20: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=1 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_2\"\nmethod_label &lt;- \"Local Regression (deg=2)\"\nplot_recalib_curve_quant(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_quant_classif_calib, \n  curve_test = calib_curve_quant_classif_test, \n  curve_c_calibration = calib_curve_quant_c_classif_calib, \n  curve_c_test = calib_curve_quant_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.21: Calibration curve calculated using quantile-defined bins for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=2 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\n\n\nPlattIsotonicBetaLocfit (deg=0)Locfit (deg=1)Locfit (deg=2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"platt\"\nmethod_label &lt;- \"Platt Scaling\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_reg_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.22: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Platt-scaling (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"isotonic\"\nmethod_label &lt;- \"Isotonic Regression\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_classif_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.23: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using isotonic regression (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"beta\"\nmethod_label &lt;- \"Beta Calibration\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_classif_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.24: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Beta calibration (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_0\"\nmethod_label &lt;- \"Local Regression (deg=0)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_classif_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.25: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=0 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_1\"\nmethod_label &lt;- \"Local Regression (deg=1)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_classif_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.26: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=1 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top).\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nmethod &lt;- \"locfit_2\"\nmethod_label &lt;- \"Local Regression (deg=2)\"\nplot_recalib_curve_ma(\n  method = method, \n  method_label = method_label, \n  curve_calibration = calib_curve_ma_classif_calib, \n  curve_test = calib_curve_ma_classif_test, \n  curve_c_calibration = calib_curve_ma_c_classif_calib, \n  curve_c_test = calib_curve_ma_c_classif_test,\n  title = NULL\n)\n\n\n\n\nFigure 3.27: Calibration curve calculated using moving averages for the Random Forest (classification), using uncalibrated scores (middle) or scores recalibrated using Local regression, with deg=2 (bottom), for the calibration set (left) and the test set (right), in a regression context. The distribution of the true probabilities are shown in the histograms (on top)."
  },
  {
    "objectID": "rf-calibration.html#sec-ml-recalib-dgp",
    "href": "rf-calibration.html#sec-ml-recalib-dgp",
    "title": "4  Calibration of Random Forests",
    "section": "4.1 Data Generating Process",
    "text": "4.1 Data Generating Process\nWe use the same DGP as that presented in Section 1.1 in Chapter 1. However, here, we only need the observed events \\(d\\) and the different features \\(x_1\\), \\(x_2\\), \\(x_3\\) and \\(x_4\\). Let us redefine here the function which simulates a single dataset:\n\n#' Simulates data\n#'\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\nsim_data &lt;- function(n_obs = 2000, \n                     seed) {\n  set.seed(seed)\n\n  x1 &lt;- runif(n_obs)\n  x2 &lt;- runif(n_obs)\n  x3 &lt;- runif(n_obs)\n  x4 &lt;- runif(n_obs)\n  epsilon_p &lt;- rnorm(n_obs, mean = 0, sd = .5)\n  \n  # True latent score\n  eta &lt;- -0.1*x1 + 0.05*x2 + 0.2*x3 - 0.05*x4  + epsilon_p\n  \n  # True probability\n  p &lt;- (1 / (1 + exp(-eta)))\n\n  # Observed event\n  d &lt;- rbinom(n_obs, size = 1, prob = p)\n\n  tibble(\n    # Event Probability\n    p = p,\n    # Binary outcome variable\n    d = d,\n    # Variables\n    x1 = x1,\n    x2 = x2,\n    x3 = x3,\n    x4 = x4\n  )\n}"
  },
  {
    "objectID": "rf-calibration.html#splitting-the-dataset",
    "href": "rf-calibration.html#splitting-the-dataset",
    "title": "4  Calibration of Random Forests",
    "section": "4.2 Splitting the dataset",
    "text": "4.2 Splitting the dataset\nThe process applied in this chapter is divided into two parts: 1. training the Random Forest classifier to obtain the predicted scores \\(\\hat{s}(\\boldsymbol x_i)\\) 2. calculating the different calibration metrics presented in Chapter 1\nWe split the dataset into three parts: 1. a train set: to train the Random Forest classifier, 2. a calibration set: to train the recalibrator, 3. a test set: on which we will compute the calibration metrics.\nTo split the data, we modify the function get_samples() from Chapter 1 to obtain three different sets instead of two:\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\nget_samples &lt;- function(seed,\n                        n_obs = 2000) {\n  set.seed(seed)\n  data_all &lt;- sim_data(\n    n_obs = n_obs, \n    seed = seed\n  )\n  \n  # Train/calibration/test sets----\n  data &lt;- data_all |&gt; select(d, x1:x4)\n  true_probas &lt;- data_all |&gt; select(p)\n  \n  train_index &lt;- sample(1:nrow(data), size = .6 * nrow(data), replace = FALSE)\n  tb_train &lt;- data |&gt; slice(train_index)\n  tb_calib_test &lt;- data |&gt; slice(-train_index)\n  true_probas_train &lt;- true_probas |&gt; slice(train_index)\n  true_probas_calib_test &lt;- true_probas |&gt; slice(-train_index)\n\n  calib_index &lt;- sample(\n    1:nrow(tb_calib_test), size = .5 * nrow(tb_calib_test), replace = FALSE\n  )\n  tb_calib &lt;- tb_calib_test |&gt; slice(calib_index)\n  tb_test &lt;- tb_calib_test |&gt; slice(-calib_index)\n  true_probas_calib &lt;- true_probas_calib_test |&gt; slice(calib_index)\n  true_probas_test &lt;- true_probas_calib_test |&gt; slice(-calib_index)\n\n  list(\n    data_all = data_all,\n    data = data,\n    tb_train = tb_train,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    true_probas_train = true_probas_train,\n    true_probas_calib = true_probas_calib,\n    true_probas_test = true_probas_test,\n    train_index = train_index,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs\n  )\n}\n\nWe will consider 200 replications for the simulations. In each simulation, we will draw 2,000 observation from the data generation process.\n\nn_repl &lt;- 200\nn_obs &lt;- 2000"
  },
  {
    "objectID": "rf-calibration.html#sec-rf-calib-regression",
    "href": "rf-calibration.html#sec-rf-calib-regression",
    "title": "4  Calibration of Random Forests",
    "section": "4.3 Simulations: Regression Task",
    "text": "4.3 Simulations: Regression Task\nLet us define a function to train the Random Forest and predict it on a new dataset in order to estimate our binary outcome variable \\(D\\) (and so, \\(P(D=1)\\)) using the different features \\(X_1\\), \\(X_2\\), \\(X_3\\) and \\(X_4\\).\n\n\n\n\n\n\nNote\n\n\n\nUsing a Random Forest regressor for classification task provides actual probability scores whereas a Random Forest classifier yields, for one observation, the average predicted classes across the ensemble of trees (see Section 3.3).\n\n\n\nlibrary(randomForest)\n#' Apply Random Forest algorithm\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data, \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  scores_train &lt;- predict(rf, newdata = train_data, type = \"response\")\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"response\")\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"response\")\n  \n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\nWe define function simul_rf() which will generate some data from the PGD, train the forest, and return predictions made on the train set, the calibration set, and the test set.\n\nsimul_rf &lt;- function(n_obs,\n                     seed) {\n  # Generate Data\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  \n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # True probabilities (from DGP)\n  true_probas_train &lt;- data$true_probas_train$p\n  true_probas_calib &lt;- data$true_probas_calib$p\n  true_probas_test &lt;- data$true_probas_test$p\n  \n  true_probas &lt;- list(\n    true_probas_train = true_probas_train,\n    true_probas_calib = true_probas_calib,\n    true_probas_test = true_probas_test\n  )\n  \n  # Fit the RF\n  scores &lt;- apply_rf(\n    train_data = tb_train,\n    calib_data = tb_calib,\n    test_data = tb_test\n  )\n  \n  list(\n    n_obs = n_obs,\n    seed = seed,\n    true_probas = true_probas,\n    scores = scores\n  )\n}\n\nWe apply this function over 200 replications. To that end, we just make the seed vary so that the data differ a little in each replication.\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  scores_simul_rf &lt;- furrr::future_map(\n    .x = seq_len(n_repl),\n    .f = ~{\n      p()\n      simul_rf(n_obs = n_obs, seed = .x)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use (in Chapter 5).\n\nsave(scores_simul_rf, file = \"output/simulations/scores_simul_rf.rda\")\n\n\n4.3.1 Probability distributions\nLet us visualize the different distributions of the scores predicted by the Random Forest regressor, first on a single replication:\n\n\nDisplay the R codes used to create the Figure.\nlibrary(tibble)\ntoy_scores &lt;- scores_simul_rf[[1]]\ntoy_data &lt;- tibble(\n  true_probas = unlist(toy_scores$true_probas),\n  scores = unlist(toy_scores$scores),\n  sample = c(\n    rep(\"train\", length(toy_scores$scores$scores_train)),\n    rep(\"calibration\", length(toy_scores$scores$scores_calib)),\n    rep(\"test\", length(toy_scores$scores$scores_test))\n  )\n)\n\ntoy_data &lt;- \n  toy_data |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nvalues &lt;- c(\"true_probas\", \"scores\")\nvalues_lab &lt;- c(\"True probabilities\", \"Predicted scores\")\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(2,1))\nfor (i in 1:length(values)) {\n  value &lt;- values[i]\n  title &lt;- values_lab[i]\n  form &lt;- str_c(value, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = toy_data,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\n\nFigure 4.1: Distribution of true probabilities and scores predicted by the Random Forest regressor for one replication\n\n\n\n\n\nWe can display the distribution of true probabilities regarding the predicted scores:\n\n\nDisplay the R codes used to create the Figure.\ntoy_scores &lt;- scores_simul_rf[[1]]\ntoy_data &lt;- tibble(\n  true_probas = unlist(toy_scores$true_probas),\n  scores = unlist(toy_scores$scores),\n  sample = c(\n    rep(\"train\", length(toy_scores$scores$scores_train)),\n    rep(\"calibration\", length(toy_scores$scores$scores_calib)),\n    rep(\"test\", length(toy_scores$scores$scores_test))\n  )\n)\n\ntoy_data &lt;- \n  toy_data |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\n\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"True probability\", ylab = \"Predicted probability\",\n    main = sample_label\n  )\n  tb_current &lt;- toy_data |&gt; \n    filter(sample == !!sample_label)\n  points(\n    tb_current$true_probas, tb_current$scores,\n    lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.3),\n    cex = .1, pch = 19\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  abline(h = quantile(tb_current$scores, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(h = quantile(tb_current$scores, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(tb_current$true_probas, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(tb_current$true_probas, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n}\n\n\n\n\nFigure 4.2: Scores predicted by the Random Forest regressor according to the true probabilities for one replication. The horizontal grey bars define the quantiles at 5% of predicted probabilities. The vertical grey bars define the quantiles at 5% of true probabilities.\n\n\n\n\n\nNow, we can look at these distributions on all replications by first defining a function that aggregates simulated results for train, calibration and test sets:\n\n#' Computes calibration metrics on a simulation, with uncalibrated scores\n#' \n#' @param simul results of a simulation obtained with simul_rf\ncalib_dist_rf &lt;- function(simul) {\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  \n  true_probas_train &lt;- data$true_probas_train |&gt; pull(p)\n  true_probas_calib &lt;- data$true_probas_calib |&gt; pull(p)\n  true_probas_test &lt;- data$true_probas_test |&gt; pull(p)\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  res_train &lt;- tibble(\n    true_probas = true_probas_train,\n    scores = scores_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    true_probas = true_probas_calib,\n    scores = scores_calib,\n    sample = \"calibration\"\n  )\n res_test &lt;- tibble(\n    true_probas = true_probas_test,\n    scores = scores_test,\n    sample = \"test\"\n  )\n  \n  res_train |&gt; \n    bind_rows(res_calib) |&gt; \n    bind_rows(res_test) |&gt; \n    mutate(\n      seed = seed,\n      n_obs = n_obs\n    )\n}\n\nLet us apply the calib_dist_rf() function to all the simulations within the regression framework:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  dist_rf_simuls &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_dist_rf(simul = scores_simul_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ndist_rf_simuls &lt;- list_rbind(dist_rf_simuls)\n\nFirst, let us turn the sample column of the results as a factor.\n\ndist_rf_simuls &lt;- \n  dist_rf_simuls |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nLet us visualize how the probability distributions vary given the dataset, within true probabilities and probabilities predicted by a Random Forest regressor:\n\n\nDisplay the R codes used to create the Figure.\nvalues &lt;- c(\"true_probas\", \"scores\")\nvalues_lab &lt;- c(\"True probabilities\", \"Predicted scores\")\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(2,1))\nfor (i in 1:length(values)) {\n  value &lt;- values[i]\n  title &lt;- values_lab[i]\n  form &lt;- str_c(value, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = dist_rf_simuls,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\n\nFigure 4.3: Distribution of true probabilities and scores predicted by the Random Forest regressor using all replications\n\n\n\n\n\nWe can display the distribution of true probabilities regarding the predicted scores, for the 200 replications:\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"True probability\", ylab = \"Predicted probability\",\n    main = sample_label\n  )\n  for (i_simul in 1:length(unique(dist_rf_simuls$seed))){\n    tb_current &lt;- dist_rf_simuls |&gt; \n    filter(sample == !!sample_label & seed == !!i_simul)\n    points(\n      tb_current$true_probas, tb_current$scores,\n      lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.3),\n      cex = .1, pch = 19\n    )\n  }\n  sample_dist &lt;- dist_rf_simuls |&gt; \n    filter(sample == !!sample_label)\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  abline(h = quantile(sample_dist$scores, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(h = quantile(sample_dist$scores, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(sample_dist$true_probas, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(sample_dist$true_probas, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n}\n\n\n\n\nFigure 4.4: Scores predicted by the Random Forest regressor according to the true probabilities for all replications. The horizontal grey bars define the quantiles at 5% of predicted probabilities. The vertical grey bars define the quantiles at 5% of true probabilities."
  },
  {
    "objectID": "rf-calibration.html#sec-rf-calib-classification",
    "href": "rf-calibration.html#sec-rf-calib-classification",
    "title": "4  Calibration of Random Forests",
    "section": "4.4 Simulations: Classification Task",
    "text": "4.4 Simulations: Classification Task\nWe now run some simulations to obtain \\(\\hat{p}_{\\text{vote}}\\).\nWe modify the apply_rf() function (regression) to match with the regression task.\n\n#' Apply Random Forest algorithm as a classifier\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf_vote &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data |&gt; mutate(d = factor(d)), \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"vote\")[, \"1\"]\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"vote\")[, \"1\"]\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"vote\")[, \"1\"]\n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\nWe also redefine the simul_rf function which will generate some data from the PGD, train the forest (as a classifier), and return predictions made on the train set, the calibration set, and the test set.\n\nsimul_rf_vote &lt;- function(n_obs,\n                          seed) {\n  \n  # Generate Data\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  \n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # True probabilities (from DGP)\n  true_probas_train &lt;- data$true_probas_train$p\n  true_probas_calib &lt;- data$true_probas_calib$p\n  true_probas_test &lt;- data$true_probas_test$p\n  \n  true_probas &lt;- list(\n    true_probas_train = true_probas_train,\n    true_probas_calib = true_probas_calib,\n    true_probas_test = true_probas_test\n  )\n  \n  # Fit the RF\n  scores &lt;- apply_rf_vote(\n    train_data = tb_train,\n    calib_data = tb_calib,\n    test_data = tb_test\n  )\n  \n  list(\n    n_obs = n_obs,\n    seed = seed,\n    true_probas = true_probas,\n    scores =scores\n  )\n}\n\nWe apply this function over 200 replications. To that end, we just make the seed vary so that the data differ a little in each replication.\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  scores_simul_rf_vote &lt;- furrr::future_map(\n    .x = seq_len(n_repl),\n    .f = ~{\n      p()\n      simul_rf_vote(n_obs = n_obs, seed = .x)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use (in Chapter 5).\n\nsave(scores_simul_rf_vote, file = \"output/simulations/scores_simul_rf_vote.rda\")\n\n\n4.4.1 Probability distributions\nLet us observe the different distributions of the scores predicted by the Random Forest classifier, first on a single replication:\n\ntoy_scores &lt;- scores_simul_rf_vote[[1]]\ntoy_data &lt;- tibble(\n  true_probas = unlist(toy_scores$true_probas),\n  scores = unlist(toy_scores$scores),\n  sample = c(\n    rep(\"train\", length(toy_scores$scores$scores_train)),\n    rep(\"calibration\", length(toy_scores$scores$scores_calib)),\n    rep(\"test\", length(toy_scores$scores$scores_test))\n  )\n)\n\ntoy_data &lt;- \n  toy_data |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nvalues &lt;- c(\"true_probas\", \"scores\")\nvalues_lab &lt;- c(\"True probabilities\", \"Predicted scores\")\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(2,1))\nfor (i in 1:length(values)) {\n  value &lt;- values[i]\n  title &lt;- values_lab[i]\n  form &lt;- str_c(value, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = toy_data,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\nFigure 4.5: Distribution of true probabilities and scores predicted by the Random Forest classifier for one replication\n\n\n\n\n\nWe can display the distribution of the true probabilities regarding the predicted scores:\n\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"True probability\", ylab = \"Predicted probability\",\n    main = sample_label\n  )\n  tb_current &lt;- toy_data |&gt; \n    filter(sample == !!sample_label)\n  points(\n    tb_current$true_probas, tb_current$scores,\n    lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.3),\n    cex = .1, pch = 19\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  abline(h = quantile(tb_current$scores, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(h = quantile(tb_current$scores, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(tb_current$true_probas, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(tb_current$true_probas, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  \n}\n\n\n\nFigure 4.6: Scores predicted by the Random Forest classifier according to the true probabilities for one replication. The horizontal grey bars define the quantiles at 5% of predicted probabilities. The vertical grey bars define the quantiles at 5% of true probabilities.\n\n\n\n\n\nNow, we can look at these distributions on all replications by applying the function calib_dist_rf(), defined in the section Section 4.3:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  dist_rf_simuls_vote &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf_vote),\n    .f = ~{\n      p()\n      calib_dist_rf(simul = scores_simul_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ndist_rf_simuls_vote &lt;- list_rbind(dist_rf_simuls_vote)\n\nFirst, let us turn the sample column of the results as a factor.\n\ndist_rf_simuls_vote &lt;- \n  dist_rf_simuls_vote |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nLet us visualize how the probability distributions vary given the dataset, within true probabilities and probabilities predicted by a Random Forest classifier:\n\n\nDisplay the R codes used to create the Figure.\nvalues &lt;- c(\"true_probas\", \"scores\")\nvalues_lab &lt;- c(\"True probabilities\", \"Predicted scores\")\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(2,1))\nfor (i in 1:length(values)) {\n  value &lt;- values[i]\n  title &lt;- values_lab[i]\n  form &lt;- str_c(value, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = dist_rf_simuls_vote,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\n\nFigure 4.7: Distribution of true probabilities and scores predicted by the Random Forest classifier for all replications\n\n\n\n\n\nWe can display the distribution of true probabilities regarding the predicted scores, for the 200 replications:\n\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"True probability\", ylab = \"Predicted probability\",\n    main = sample_label\n  )\n  for (i_simul in 1:length(unique(dist_rf_simuls_vote$seed))){\n    tb_current &lt;- dist_rf_simuls_vote |&gt; \n    filter(sample == !!sample_label & seed == !!i_simul)\n    points(\n      tb_current$true_probas, tb_current$scores,\n      lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.3),\n      cex = .1, pch = 19\n    )\n  }\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  sample_dist &lt;- dist_rf_simuls_vote |&gt; \n    filter(sample == !!sample_label)\n  abline(h = quantile(sample_dist$scores, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(h = quantile(sample_dist$scores, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(sample_dist$true_probas, 0.05), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  abline(v = quantile(sample_dist$true_probas, 0.95), col = \"darkgrey\", \n         lty = 1, lwd = 1.5)\n  \n}\n\n\n\nFigure 4.8: Scores predicted by the Random Forest classifier according to the true probabilities for all replications. The horizontal grey bars define the quantiles at 5% of predicted probabilities. The vertical grey bars define the quantiles at 5% of true probabilities."
  },
  {
    "objectID": "rf-calibration.html#sec-rf-gof-metrics",
    "href": "rf-calibration.html#sec-rf-gof-metrics",
    "title": "4  Calibration of Random Forests",
    "section": "4.5 Standard Metrics",
    "text": "4.5 Standard Metrics\nLet us have a look at the goodness of fit of the random forests obtained in both cases (regression or classification). To do so, we compute the standard performance metrics defined in Chapter 1.\n\n4.5.1 Helper Functions\nWe (re)define a helper function to compute standard metrics (see Section 1.4 in Chapter 1):\n\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\ncompute_gof &lt;- function(true_prob,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  mse &lt;- mean((true_prob - pred)^2)\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR\n  )\n}\n\nWe (re)define the function compute_gof_simul() to apply compute_gof(), defined above, to compute the different standard performance metrics on predicted scores of one replication (see Section 1.4 in Chapter 1):\n\n#' Computes goodness of fit metrics for a replication\n#'\n#' @param n_obs desired number of observation\n#' @param seed random seed to use\n#' @type type of Random Forest to use (either `regression` or `classification`) \ncompute_gof_simul &lt;- function(n_obs,\n                              seed,\n                              type = c(\"regression\", \"classification\")) {\n  current_seed &lt;- seed\n  \n  # Generate Data\n  current_data &lt;- get_samples(\n    n_obs = n_obs, seed = current_seed\n  )\n  \n  # Get the calib/test datasets with true probabilities\n  data_all_train &lt;- current_data$tb_train\n  data_all_calib &lt;- current_data$tb_calib\n  data_all_test &lt;- current_data$tb_test\n  \n  # Test set\n  true_prob_test &lt;- current_data$true_probas_test$p\n  obs_test &lt;- data_all_test$d\n  ## Fit the RF\n  if (type == \"regression\"){\n    scores &lt;- apply_rf(\n    train_data = data_all_train,\n    calib_data = data_all_calib,\n    test_data = data_all_test\n    )\n  } else if (type == \"classification\"){\n    scores &lt;- apply_rf_vote(\n    train_data = data_all_train,\n    calib_data = data_all_calib,\n    test_data = data_all_test\n    )\n  } else {\n    stop(\"Random Forest type should be either regression or classification.\")\n  }\n  \n  pred_test &lt;- scores$scores_test\n\n  metrics_simul_test &lt;- map(\n    .x = seq(0, 1, by = .01), # we vary the probability threshold\n    .f = ~compute_gof(\n      true_prob = true_prob_test,\n      obs = obs_test,\n      pred = pred_test,\n      threshold = .x\n      )\n    ) |&gt;\n      list_rbind()\n    \n    metrics_simul_test &lt;- metrics_simul_test |&gt;\n      mutate(\n        seed = current_seed\n      )\n    \n    metrics_simul_test\n}\n\n\n\n4.5.2 Calculating the Standard Metrics\n\nRegressionClassification\n\n\nLet us apply this function to the different simulations obtained with a Random Forest regressor:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  regression_metrics &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      p()\n      compute_gof_simul(\n        n_obs = n_obs,\n        seed = .x,\n        type = \"regression\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nregression_metrics &lt;- list_rbind(regression_metrics)\n\n\n\nLet us apply this function to the different simulations obtained with a Random Forest classifier:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  classification_metrics &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      p()\n      compute_gof_simul(\n        n_obs = n_obs,\n        seed = .x,\n        type = \"classification\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nclassification_metrics &lt;- list_rbind(classification_metrics)\n\n\n\n\n\n\n4.5.3 Results\nLet us visualize how the performance metrics vary from one replication to another, on the test set. The boxplots are shown in Figure 4.9 (regression) and in Figure 4.10 (classification).\n\n#' Boxplots for the simulations to visualize the distribution of some \n#' traditional metrics as a function of the probability threshold.\n#' And, ROC curves\n#' The resulting figure is a panel of graphs, with vayring values for the \n#' transformation applied to the probabilities (in columns) and different \n#' metrics (in rows).\n#' \n#' @param tb_metrics tibble with computed metrics for the simulations\n#' @param metrics names of the metrics computed\nboxplot_simuls_metrics &lt;- function(tb_metrics,\n                                   metrics) {\n  \n  par(mfrow = c(2, length(metrics)%/%2+1))\n  for (i_metric in 1:length(metrics)) {\n    metric &lt;- metrics[i_metric]\n    tb_metrics_current &lt;- tb_metrics\n    if (metric == \"roc\") {\n      seeds &lt;- unique(tb_metrics_current$seed)\n      par(mar = c(4.1, 4.1, 2.1, 2.1))\n      plot(\n        0:1, 0:1,\n        type = \"l\", col = NULL,\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"False Positive Rate\",\n        ylab = \"True Positive Rate\",\n        main = \"\"\n      )\n      for (i_seed in 1:length(seeds)) {\n        tb_metrics_current_seed &lt;- \n          tb_metrics_current |&gt; \n          filter(seed == seeds[i_seed])\n        lines(\n          x = tb_metrics_current_seed$FPR,\n          y = tb_metrics_current_seed$sensitivity,\n          lwd = 2, col = adjustcolor(\"black\", alpha.f = .04)\n        )\n      }\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n    } else {\n      # not ROC\n      tb_metrics_current &lt;- \n        tb_metrics_current |&gt; \n        filter(threshold %in% seq(0, 1, by = .1))\n      form &lt;- str_c(metric, \"~threshold\")\n      par(mar = c(4.1, 4.1, 2.1, 2.1))\n      boxplot(\n        formula(form), data = tb_metrics_current,\n        xlab = \"Threshold\", ylab = metric\n      )\n    }\n  }\n}\n\n\n4.5.3.1 Regression\nWe aim to create a set of boxplots to visually assess the Random Forest in the case of regression.\n\nmetrics &lt;- c(\"mse\", \"accuracy\", \"sensitivity\", \"specificity\", \"roc\")\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_metrics, metrics = metrics)\n\n\n\n\nFigure 4.9: Standard metrics on Random Forest predicted scores (regression).\n\n\n\n\n\n\n\n4.5.3.2 Classification\nWe aim to create a set of boxplots to visually assess the Random Forest in the case of classification.\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_metrics, metrics = metrics)\n\n\n\n\nFigure 4.10: Standard metrics on Random Forest predicted scores (classification)."
  },
  {
    "objectID": "rf-calibration.html#sec-rf-calib-metrics",
    "href": "rf-calibration.html#sec-rf-calib-metrics",
    "title": "4  Calibration of Random Forests",
    "section": "4.6 Calibration Metrics",
    "text": "4.6 Calibration Metrics\nLet us have a look at the calibration of the random forests obtained in both cases (regression or classification). To do so, we compute the calibration metrics defined in Chapter 1.\n\n4.6.1 Helper Functions\n\nBrier ScoreECEQMSEWMSE\n\n\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n\n\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n\n\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n\n\n\nlibrary(binom)\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  if (all(is.na(scores))) {\n    wmse &lt;- NA\n  } else {\n    dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  wmse &lt;- local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n  }\n}\n\n\n\n\n\n\n4.6.2 Calculating the Metrics\nNext, we define the function calib_metrics_rf_uncalib() which computes the true MSE and the calibration metrics from Chapter 1, from a single replication of the simulations.\n\n#' Computes calibration metrics on a simulation, with uncalibrated scores\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when computing the WMSE\ncalib_metrics_rf_uncalib &lt;- function(simul, linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  true_probas_train &lt;- data$true_probas_train |&gt; pull(p)\n  true_probas_calib &lt;- data$true_probas_calib |&gt; pull(p)\n  true_probas_test &lt;- data$true_probas_test |&gt; pull(p)\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  \n  # Mean observed events----\n  expected_events_train &lt;- map(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = tb_train$d,\n    scores = scores_train, \n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n  expected_events_calib &lt;- map(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = tb_calib$d,\n    scores = scores_calib, \n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n  expected_events_test &lt;- map(\n  .x = linspace,\n  .f = ~local_ci_scores(\n    obs = tb_test$d,\n    scores = scores_test, \n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n) |&gt; \n  bind_rows()\n  \n  # Compute Metrics----\n  \n  ## True MSE\n  mse_train &lt;- mean((true_probas_train - scores_train)^2)\n  mse_calib &lt;- mean((true_probas_calib - scores_calib)^2)\n  mse_test &lt;- mean((true_probas_test - scores_test)^2)\n  \n  ## Brier Score\n  brier_train &lt;- brier_score(obs = tb_train$d, score = scores_train)\n  brier_calib &lt;- brier_score(obs = tb_calib$d, score = scores_calib)\n  brier_test &lt;- brier_score(obs = tb_test$d, score = scores_test)\n  \n  ## Expected Calibration Error\n  ece_train &lt;- e_calib_error(\n    obs = tb_train$d, scores = scores_train, k = 10, threshold = .5\n  )\n  ece_calib &lt;- e_calib_error(\n    obs = tb_calib$d, scores = scores_calib, k = 5, threshold = .5\n  )\n  ece_test &lt;- e_calib_error(\n    obs = tb_test$d, scores = scores_test, k = 5, threshold = .5\n  )\n  \n  # Quantile Mean Squared Error\n  qmse_train &lt;- qmse_error(\n    obs = tb_train$d, score = scores_train, k = 10, threshold = .5\n  )\n  qmse_calib &lt;- qmse_error(\n    obs = tb_calib$d, score = scores_calib, k = 5, threshold = .5\n  )\n  qmse_test &lt;- qmse_error(\n    obs = tb_test$d, score = scores_test, k = 5, threshold = .5\n  )\n  \n  # Weighted Mean Squared Error\n  wmse_train &lt;- weighted_mse(\n    local_scores = expected_events_train, scores = scores_train\n  )\n  wmse_calib &lt;- weighted_mse(\n    local_scores = expected_events_calib, scores = scores_calib\n  )\n  wmse_test &lt;- weighted_mse(\n    local_scores = expected_events_test, scores = scores_test\n  )\n  \n  res_train &lt;- tibble(\n    mse = mse_train,\n    brier = brier_train,\n    ece = ece_train,\n    qmse = qmse_train,\n    wmse = wmse_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    mse = mse_calib,\n    brier = brier_calib,\n    ece = ece_calib,\n    qmse = qmse_calib,\n    wmse = wmse_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    mse = mse_test,\n    brier = brier_test,\n    ece = ece_test,\n    qmse = qmse_test,\n    wmse = wmse_test,\n    sample = \"test\"\n  )\n  \n  res_train |&gt; \n    bind_rows(res_calib) |&gt; \n    bind_rows(res_test) |&gt; \n    mutate(\n      seed = seed,\n      n_obs = n_obs\n    )\n}\n\nLet us apply the calib_metrics_rf_uncalib() function to all the simulations.\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  calibration_rf_simuls &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_metrics_rf_uncalib(simul = scores_simul_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ncalibration_rf_simuls &lt;- list_rbind(calibration_rf_simuls)\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  calibration_rf_simuls_vote &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_metrics_rf_uncalib(simul = scores_simul_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ncalibration_rf_simuls_vote &lt;- list_rbind(calibration_rf_simuls_vote)\n\n\n\n\n\n\n4.6.3 Results\nLet us visualize how the metrics vary from one replication to another, on the train set, on the calibration set, and on the test set. Note that since we did not recalibrate the scores \\(s(\\boldsymbol x)\\), the calibration and the test set should exhibit similar values for the different metrics: they both are unseen data from the same DGP. The boxplots are shown in Figure 4.11 (regression) and in Figure 4.13 (classification).\n\n4.6.3.1 Regression\nFirst, let us turn the sample column of the results as a factor.\n\ncalibration_rf_simuls &lt;- \n  calibration_rf_simuls |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nThen, we can create the Figure.\n\n\nDisplay the R codes used to create the Figure.\nmetrics &lt;- c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\")\nmetrics_lab &lt;- c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(3,2))\nfor (i in 1:length(metrics)) {\n  metric &lt;- metrics[i]\n  title &lt;- metrics_lab[i]\n  form &lt;- str_c(metric, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = calibration_rf_simuls,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\n\nFigure 4.11: Goodness of fit and calibration metrics of the Random Forest (regression).\n\n\n\n\n\nLet us also show some summary statistics of these metrics. The values are reported in Table 4.1.\n\n\nDisplay the R codes used to produce the Table.\nsummary_table_rf &lt;- calibration_rf_simuls |&gt; \n  pivot_longer(cols = c(mse, brier, ece, qmse, wmse), names_to = \"metric\") |&gt; \n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\"),\n      labels = c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\n    )\n  ) |&gt; \n  select(-seed, -n_obs) |&gt; \n  rename(Metric = metric, Sample = sample) |&gt; \n  group_by(Metric, Sample) |&gt; \n  summarise(\n    Mean = mean(value),\n    `Std` = sd(value),\n    Q1 = quantile(value, probs = 0.25),\n    Median = quantile(value, probs = 0.5),\n    Q3 = quantile(value, probs = 0.75),\n    Min = min(value),\n    Max = max(value),\n    .groups = \"drop\"\n  )\n\nsummary_table_rf |&gt; select(-Metric) |&gt; \n  knitr::kable(escape = FALSE, booktabs = T, digits = 3) |&gt; \n  kableExtra::pack_rows(index = table(summary_table_rf$Metric)) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable 4.1: True MSE and Calibration Metrics for the Random Forest (regression).\n\n\nSample\nMean\nStd\nQ1\nMedian\nQ3\nMin\nMax\n\n\n\n\nTrue MSE\n\n\nTrain\n0.017\n0.001\n0.017\n0.017\n0.018\n0.015\n0.020\n\n\nCalibration\n0.018\n0.001\n0.017\n0.018\n0.019\n0.015\n0.022\n\n\nTest\n0.018\n0.001\n0.017\n0.018\n0.019\n0.014\n0.023\n\n\nBrier Score\n\n\nTrain\n0.208\n0.001\n0.207\n0.209\n0.209\n0.205\n0.212\n\n\nCalibration\n0.254\n0.004\n0.252\n0.254\n0.257\n0.244\n0.263\n\n\nTest\n0.254\n0.004\n0.252\n0.254\n0.257\n0.243\n0.264\n\n\nECE\n\n\nTrain\n0.272\n0.022\n0.256\n0.273\n0.287\n0.216\n0.329\n\n\nCalibration\n0.063\n0.020\n0.049\n0.063\n0.075\n0.019\n0.128\n\n\nTest\n0.061\n0.018\n0.048\n0.060\n0.071\n0.020\n0.115\n\n\nQMSE\n\n\nTrain\n0.061\n0.009\n0.055\n0.061\n0.068\n0.040\n0.084\n\n\nCalibration\n0.007\n0.004\n0.004\n0.007\n0.010\n0.001\n0.019\n\n\nTest\n0.007\n0.004\n0.004\n0.006\n0.009\n0.001\n0.019\n\n\nWMSE\n\n\nTrain\n0.059\n0.009\n0.052\n0.059\n0.067\n0.038\n0.082\n\n\nCalibration\n0.041\n0.014\n0.031\n0.040\n0.049\n0.010\n0.088\n\n\nTest\n0.039\n0.013\n0.028\n0.037\n0.048\n0.012\n0.072\n\n\n\n\n\n\n\n\n\n\n4.6.3.2 Classification\nFirst, let us turn the sample column of the results as a factor.\n\ncalibration_rf_simuls_vote &lt;- \n  calibration_rf_simuls_vote |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"train\", \"calibration\", \"test\"), \n      labels = c(\"Train\", \"Calibration\", \"Test\")\n    )\n  )\n\nThen, we can create the Figure.\n\n\nDisplay the R codes used to create the Figure.\nmetrics &lt;- c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\")\nmetrics_lab &lt;- c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calidation\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(3,2))\nfor (i in 1:length(metrics)) {\n  metric &lt;- metrics[i]\n  title &lt;- metrics_lab[i]\n  form &lt;- str_c(metric, \"~sample\") |&gt; as.formula()\n  par(mar = c(3.5, 4.1, 3.1, 2.1))\n  boxplot(\n    form, data = calibration_rf_simuls_vote,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    col = colours_samples\n  )\n}\n\n\n\n\nFigure 4.12: Goodness of fit and calibration metrics of the Random Forest (classification).\n\n\n\n\n\nLet us also show some summary statistics of these metrics. The values are reported in Table 4.2.\n\n\nDisplay the R codes used to produce the Table.\nsummary_table_rf_vote &lt;- calibration_rf_simuls_vote |&gt; \n  pivot_longer(cols = c(mse, brier, ece, qmse, wmse), names_to = \"metric\") |&gt; \n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\"),\n      labels = c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\n    )\n  ) |&gt; \n  select(-seed, -n_obs) |&gt; \n  rename(Metric = metric, Sample = sample) |&gt; \n  group_by(Metric, Sample) |&gt; \n  summarise(\n    Mean = mean(value),\n    `Std` = sd(value),\n    Q1 = quantile(value, probs = 0.25),\n    Median = quantile(value, probs = 0.5),\n    Q3 = quantile(value, probs = 0.75),\n    Min = min(value),\n    Max = max(value),\n    .groups = \"drop\"\n  )\n\nsummary_table_rf_vote |&gt; select(-Metric) |&gt; \n  knitr::kable(escape = FALSE, booktabs = T, digits = 3) |&gt; \n  kableExtra::pack_rows(index = table(summary_table_rf_vote$Metric)) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable 4.2: True MSE and Calibration Metrics for the Random Forest (regression).\n\n\nSample\nMean\nStd\nQ1\nMedian\nQ3\nMin\nMax\n\n\n\n\nTrue MSE\n\n\nTrain\n0.040\n0.006\n0.036\n0.040\n0.044\n0.030\n0.058\n\n\nCalibration\n0.041\n0.006\n0.037\n0.041\n0.045\n0.029\n0.060\n\n\nTest\n0.041\n0.006\n0.037\n0.040\n0.045\n0.026\n0.058\n\n\nBrier Score\n\n\nTrain\n0.202\n0.003\n0.200\n0.202\n0.204\n0.193\n0.211\n\n\nCalibration\n0.277\n0.011\n0.270\n0.277\n0.285\n0.250\n0.306\n\n\nTest\n0.277\n0.011\n0.269\n0.276\n0.284\n0.251\n0.312\n\n\nECE\n\n\nTrain\n0.185\n0.025\n0.167\n0.182\n0.199\n0.133\n0.271\n\n\nCalibration\n0.134\n0.025\n0.117\n0.134\n0.150\n0.060\n0.211\n\n\nTest\n0.134\n0.029\n0.115\n0.133\n0.152\n0.064\n0.211\n\n\nQMSE\n\n\nTrain\n0.007\n0.003\n0.004\n0.006\n0.008\n0.001\n0.019\n\n\nCalibration\n0.029\n0.010\n0.021\n0.028\n0.035\n0.006\n0.063\n\n\nTest\n0.029\n0.010\n0.021\n0.027\n0.035\n0.009\n0.058\n\n\nWMSE\n\n\nTrain\n0.005\n0.003\n0.004\n0.005\n0.006\n0.002\n0.016\n\n\nCalibration\n0.059\n0.015\n0.049\n0.059\n0.067\n0.028\n0.114\n\n\nTest\n0.057\n0.015\n0.047\n0.056\n0.068\n0.022\n0.092\n\n\n\n\n\n\n\n\n\n\n4.6.3.3 Both\nWe can bind together the metrics obtained either with the regression (calibration_rf_simuls) or with the classifier (calibration_rf_simuls_vote).\n\ncalibration_rf_simuls_both &lt;- \n  calibration_rf_simuls |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    calibration_rf_simuls_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Classification\", \"Regression\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\n\nThen, we can create the Figure.\n\n\nDisplay the R codes used to create the Figure.\nmetrics &lt;- c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\")\nmetrics_lab &lt;- c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(3,2))\nfor (i in 1:length(metrics)) {\n  metric &lt;- metrics[i]\n  title &lt;- metrics_lab[i]\n  form &lt;- str_c(metric, \" ~ sample + model\") |&gt; as.formula()\n  labels_y &lt;- unique(calibration_rf_simuls_both$lab_y)\n  # ind &lt;- which(!str_detect(labels_y, \"Calibration\"))\n  # labels_y[ind] &lt;- str_remove(labels_y[ind], \" \\\\(.*\\\\)\")\n  par(mar = c(2.1, 10.1, 2.1, 2.1))\n  boxplot(\n    form,\n    data = calibration_rf_simuls_both,\n    main = title,\n    col = rep(colours_samples, 2),\n    horizontal = TRUE,\n    las = 1, xlab = \"\", ylab = \"\",\n    yaxt = \"n\"\n  )\n  axis(\n    side = 2, at = 1:length(labels_y), \n    labels = latex2exp::TeX(labels_y), las = 2\n  )\n  abline(h = 3.5, lty = 1, col = \"gray\")\n}\n\n\n\n\nFigure 4.13: Goodness of fit and calibration metrics of the Random Forest (not recalibrated), estimated in a regression context or in a classification context."
  },
  {
    "objectID": "rf-calibration.html#sec-rf-calib-viz",
    "href": "rf-calibration.html#sec-rf-calib-viz",
    "title": "4  Calibration of Random Forests",
    "section": "4.7 Calibration Visualizations",
    "text": "4.7 Calibration Visualizations\nLet us now turn to visualization techniques to observe the calibration of random forest, as in Section 1.6 in Chapter 1.\n\n4.7.1 Helper Functions\n\n4.7.1.1 Quantile-Based Bins\nWe rely here on the get_summary_bins() function defined in the helper functions (Section 4.6.1) for the ECE.\nLet us define a function, calib_curve_quant_simul_rf(), to get the calibration curve for a single simulation of the random forest.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the quantile-based approach\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_quant_simul_rf &lt;- function(simul,\n                                       linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  summary_bins_train &lt;- get_summary_bins(\n    obs = tb_train$d,\n    scores = scores_train, \n    k = 10, threshold = .5)\n  summary_bins_calib &lt;- get_summary_bins(\n    obs = tb_calib$d,\n    scores = scores_calib, \n    k = 10, threshold = .5)\n  summary_bins_test &lt;- get_summary_bins(\n    obs = tb_test$d,\n    scores = scores_test, \n    k = 10, threshold = .5)\n  \n  summary_bins_train |&gt; mutate(sample = \"train\") |&gt; \n    bind_rows(\n      summary_bins_calib |&gt; mutate(sample = \"calibration\")\n    ) |&gt; \n    bind_rows(\n      summary_bins_test |&gt; mutate(sample = \"test\")\n    ) |&gt; \n    select(score_class, mean_score, mean_obs, sample) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed\n    )\n}\n\n\n\n4.7.1.2 Calibration Curve with Local Regression\nTo visualize the calibration of the Random Forest using a smoother version of the calibration curve, we rely on a local regression instead of quantiles that define bins in which we can compute the average value of the event.\nWe define the function calib_curve_locfit_simul_rf() to get the calibration curve for a single replication of the simulations for the random forest.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_locfit_simul_rf &lt;- function(simul,\n                                        linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  locfit_0_train &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_train$d, score = scores_train)\n  )\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_c_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n  score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  \n  # Make sure to have values in [0,1]\n  score_c_locfit_0_train[score_c_locfit_0_train &gt; 1] &lt;- 1\n  score_c_locfit_0_train[score_c_locfit_0_train &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n  score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n  score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n  \n  res_train &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_test,\n    sample = \"test\"\n  )\n  \n  res_train |&gt; \n    bind_rows(\n      res_calib\n    ) |&gt; \n    bind_rows(\n      res_test\n    ) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed\n    )\n}\n\n\n\n4.7.1.3 Calibration Curve with Moving Average and Confidence Intervals\nWe define the function calib_curve_ma_simul_rf() to get the calibration curve for a single replication of the simulations for the random forest, using another smooth version of the calibration curve, obtained with a moving average.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using moving averages\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_ma_simul_rf &lt;- function(simul,\n                                        linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  calib_ma_train &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n    obs = tb_train$d,\n    scores = scores_train,\n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n  ) |&gt; bind_rows() |&gt; \n    mutate(sample = \"train\")\n  \n  calib_ma_calib &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n    obs = tb_calib$d,\n    scores = scores_calib,\n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n  ) |&gt; bind_rows() |&gt; \n    mutate(sample = \"calibration\")\n  \n  calib_ma_test &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n    obs = tb_test$d,\n    scores = scores_test,\n    tau = .x, \n    nn = .15, prob = .5, method = \"probit\")\n  ) |&gt; bind_rows() |&gt; \n    mutate(sample = \"test\")\n  \n  \n  calib_ma_train |&gt; \n    bind_rows(\n      calib_ma_calib\n    ) |&gt; \n    bind_rows(\n      calib_ma_test\n    ) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed\n    )\n}\n\n\n\n\n4.7.2 Quantile-based Bins\nLet us get the different curves for all the simulations.\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_quant_rf &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_quant_simul_rf(simul = scores_simul_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_quant_rf &lt;- list_rbind(tb_calibration_curve_quant_rf)\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_quant_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_quant_simul_rf(simul = scores_simul_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_quant_rf_vote &lt;- \n  list_rbind(tb_calibration_curve_quant_rf_vote)\n\n\n\n\nNow, we can plot the calibration curve on each sample (train, calibration, test). Each line corresponds to the calibration curve of a single replicaiton of the simulations. The curves are shown in Figure 4.14 (regression), in Figure 4.15 (classification), and in Figure 4.16 (comparison of the two).\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  for (i_simul in unique(tb_calibration_curve_quant_rf$seed)) {\n    tb_current &lt;- tb_calibration_curve_quant_rf |&gt; \n      filter(seed == !!i_simul, sample == !!sample)\n    lines(\n      tb_current$mean_score, tb_current$mean_obs,\n      lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\n\n\n\nFigure 4.14: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with quantile-based bins.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  plot(\n    0:1, 0:1,\n    type = \"l\", col = NULL,\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  for (i_simul in unique(tb_calibration_curve_quant_rf_vote$seed)) {\n    tb_current &lt;- tb_calibration_curve_quant_rf_vote |&gt; \n      filter(seed == !!i_simul, sample == !!sample)\n    lines(\n      tb_current$mean_score, tb_current$mean_obs,\n      lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\n\n\n\nFigure 4.15: Calibration curve for the 200 replications of the estimation of the Random Forest (classification), obtained with quantile-based bins.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\ntb_calibration_curve_quant_rf_both &lt;- \n  tb_calibration_curve_quant_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_quant_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\npar(mfrow = c(2,3))\n\nfor (model in c(\"Regression\", \"Classification\")) {\n  for (i_sample in 1:3) {\n    sample &lt;- samples[i_sample]\n    sample_label &lt;- sample_labels[i_sample]\n    par(mar = c(4.1, 4.1, 3.1, 2.1))\n    plot(\n      0:1, 0:1,\n      type = \"l\", col = NULL,\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n      main = str_c(sample_label, \" (\", model, \")\")\n    )\n    for (i_simul in unique(tb_calibration_curve_quant_rf_both$seed)) {\n      tb_current &lt;- tb_calibration_curve_quant_rf_both |&gt; \n        filter(seed == !!i_simul, sample == !!sample, model == !!model)\n      lines(\n        tb_current$mean_score, tb_current$mean_obs,\n        lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\n\n\n\nFigure 4.16: Calibration curve for the 200 replications of the estimation of the Random Forest estimated in a regression context or a calibration context, obtained with quantile-based bins.\n\n\n\n\n\n\n\n\n\n\n4.7.3 Local Regression\nLet us get the different curves for all the simulations.\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_locfit_rf &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_locfit_simul_rf(simul = scores_simul_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_locfit_rf &lt;- list_rbind(tb_calibration_curve_locfit_rf)\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_locfit_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_locfit_simul_rf(simul = scores_simul_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_locfit_rf_vote &lt;- list_rbind(tb_calibration_curve_locfit_rf_vote)\n\n\n\n\nLastly, we can plot the calibration curve obtained with local regressions on each sample (train, calibration, test). Contrary to the calibration curve obtained with the quantile-defined bins, we do not need to plot all the curves: the mean predicted probability is predicted at different values over the segment [0,1], and not only computed in bins for which the cutoff are a function of the data.\nThe curves are shown in Figure 4.17 (regression), in Figure 4.18 (classification), and in Figure 4.19 (comparison of the two).\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_locfit_rf |&gt; \n  group_by(sample, xlim) |&gt; \n  summarise(\n    mean = mean(locfit_pred),\n    lower = quantile(locfit_pred, probs = .025),\n    upper = quantile(locfit_pred, probs = .975),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  \n  df_plot_current &lt;- df_plot |&gt; \n    filter(sample == !!sample)\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  \n  plot(\n    df_plot_current$xlim,\n    df_plot_current$mean,\n    type = \"l\", col = colours_samples[sample_label],\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  polygon(\n    c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n    c(df_plot_current$lower, rev(df_plot_current$upper)),\n    col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 4.17: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with local regressions.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_locfit_rf_vote |&gt; \n  group_by(sample, xlim) |&gt; \n  summarise(\n    mean = mean(locfit_pred),\n    lower = quantile(locfit_pred, probs = .025),\n    upper = quantile(locfit_pred, probs = .975),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  \n  df_plot_current &lt;- df_plot |&gt; \n    filter(sample == !!sample)\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  \n  plot(\n    df_plot_current$xlim,\n    df_plot_current$mean,\n    type = \"l\", col = colours_samples[sample_label],\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  polygon(\n    c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n    c(df_plot_current$lower, rev(df_plot_current$upper)),\n    col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 4.18: Calibration curve for the 200 replications of the estimation of the Random Forest (classification), obtained with local regressions.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\ntb_calibration_curve_locfit_rf_vote_both &lt;- \n  tb_calibration_curve_locfit_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_locfit_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\n\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_locfit_rf_vote_both |&gt; \n  group_by(model, sample, xlim) |&gt; \n  summarise(\n    mean = mean(locfit_pred),\n    lower = quantile(locfit_pred, probs = .025),\n    upper = quantile(locfit_pred, probs = .975),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(2,3))\n\nfor (model in c(\"Regression\", \"Classification\")) {\n  for (i_sample in 1:3) {\n    sample &lt;- samples[i_sample]\n    sample_label &lt;- sample_labels[i_sample]\n    df_plot_current &lt;- df_plot |&gt; \n      filter(model == !!model, sample == !!sample)\n    par(mar = c(4.1, 4.1, 3.1, 2.1))\n    plot(\n      df_plot_current$xlim,\n      df_plot_current$mean,\n      type = \"l\", col = colours_samples[sample_label],\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n      main = str_c(sample_label, \" (\", model, \")\")\n    )\n    polygon(\n      c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n      c(df_plot_current$lower, rev(df_plot_current$upper)),\n      col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n      border = NA\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\n\n\n\nFigure 4.19: Calibration curve for the 200 replications of the estimation of the Random Forest estimated in a regression context or a calibration context, obtained with local regressions.\n\n\n\n\n\n\n\n\n\n\n4.7.4 Moving Average\nLet us get the different curves for all the simulations.\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_ma_rf &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_ma_simul_rf(simul = scores_simul_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\ntb_calibration_curve_ma_rf &lt;- list_rbind(tb_calibration_curve_ma_rf)\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  tb_calibration_curve_ma_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(scores_simul_rf),\n    .f = ~{\n      p()\n      calib_curve_ma_simul_rf(simul = scores_simul_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\ntb_calibration_curve_ma_rf_vote &lt;- list_rbind(tb_calibration_curve_ma_rf_vote)\n\n\n\n\nLastly, we can plot the calibration curve obtained with moving averages on each sample (train, calibration, test).\nThe curves are shown in Figure 4.20 (regression), Figure 4.21 (classification), and in Figure 4.22 (comparison of the two).\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_ma_rf |&gt; \n  group_by(sample, xlim) |&gt; \n  summarise(\n    lower = quantile(mean, probs = .025),\n    upper = quantile(mean, probs = .975),\n    mean = mean(mean),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  \n  df_plot_current &lt;- df_plot |&gt; \n    filter(sample == !!sample)\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  \n  plot(\n    df_plot_current$xlim,\n    df_plot_current$mean,\n    type = \"l\", col = colours_samples[sample_label],\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  polygon(\n    c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n    c(df_plot_current$lower, rev(df_plot_current$upper)),\n    col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 4.20: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with moving averages.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_ma_rf_vote |&gt; \n  group_by(sample, xlim) |&gt; \n  summarise(\n    lower = quantile(mean, probs = .025),\n    upper = quantile(mean, probs = .975),\n    mean = mean(mean),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(1,3))\n\nfor (i_sample in 1:3) {\n  sample &lt;- samples[i_sample]\n  sample_label &lt;- sample_labels[i_sample]\n  \n  df_plot_current &lt;- df_plot |&gt; \n    filter(sample == !!sample)\n  \n  par(mar = c(4.1, 4.1, 3.1, 2.1))\n  \n  plot(\n    df_plot_current$xlim,\n    df_plot_current$mean,\n    type = \"l\", col = colours_samples[sample_label],\n    xlim = 0:1, ylim = 0:1,\n    xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n    main = sample_label\n  )\n  polygon(\n    c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n    c(df_plot_current$lower, rev(df_plot_current$upper)),\n    col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n    border = NA\n  )\n  segments(0, 0, 1, 1, col = \"black\", lty = 2)\n}\n\n\n\n\nFigure 4.21: Calibration curve for the 200 replications of the estimation of the Random Forest (classification), obtained with moving averages.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\ntb_calibration_curve_ma_rf_both &lt;- \n  tb_calibration_curve_ma_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_ma_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\n\nsamples &lt;- c(\"train\", \"calibration\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ndf_plot &lt;- tb_calibration_curve_ma_rf_both |&gt; \n  group_by(model, sample, xlim) |&gt; \n  summarise(\n    lower = quantile(mean, probs = .025),\n    upper = quantile(mean, probs = .975),\n    mean = mean(mean),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(2,3))\n\nfor (model in c(\"Regression\", \"Classification\")) {\n  for (i_sample in 1:3) {\n    sample &lt;- samples[i_sample]\n    sample_label &lt;- sample_labels[i_sample]\n    df_plot_current &lt;- df_plot |&gt; \n      filter(model == !!model, sample == !!sample)\n    par(mar = c(4.1, 4.1, 3.1, 2.1))\n    plot(\n      df_plot_current$xlim,\n      df_plot_current$mean,\n      type = \"l\", col = colours_samples[sample_label],\n      xlim = 0:1, ylim = 0:1,\n      xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n      main = str_c(sample_label, \" (\", model, \")\")\n    )\n    polygon(\n      c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n      c(df_plot_current$lower, rev(df_plot_current$upper)),\n      col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n      border = NA\n    )\n    segments(0, 0, 1, 1, col = \"black\", lty = 2)\n  }\n}\n\n\n\n\nFigure 4.22: Calibration curve for the 200 replications of the estimation of the Random Forest estimated in a regression context or a calibration context, obtained with moving averages."
  },
  {
    "objectID": "rf-recalibration.html#sec-ml-recalib-dgp",
    "href": "rf-recalibration.html#sec-ml-recalib-dgp",
    "title": "5  Recalibration of Random Forests",
    "section": "5.1 Data Generating Process",
    "text": "5.1 Data Generating Process\nWe use the same DGP as in Chapter 4.\n\n#' Simulates data\n#'\n#' @param n_obs number of desired observations\n#' @param seed seed to use to generate the data\nsim_data &lt;- function(n_obs = 2000, \n                     seed) {\n  set.seed(seed)\n\n  x1 &lt;- runif(n_obs)\n  x2 &lt;- runif(n_obs)\n  x3 &lt;- runif(n_obs)\n  x4 &lt;- runif(n_obs)\n  epsilon_p &lt;- rnorm(n_obs, mean = 0, sd = .5)\n  \n  # True latent score\n  eta &lt;- -0.1*x1 + 0.05*x2 + 0.2*x3 - 0.05*x4  + epsilon_p\n  \n  # True probability\n  p &lt;- (1 / (1 + exp(-eta)))\n\n  # Observed event\n  d &lt;- rbinom(n_obs, size = 1, prob = p)\n\n  tibble(\n    # Event Probability\n    p = p,\n    # Binary outcome variable\n    d = d,\n    # Variables\n    x1 = x1,\n    x2 = x2,\n    x3 = x3,\n    x4 = x4\n  )\n}"
  },
  {
    "objectID": "rf-recalibration.html#splitting-the-dataset",
    "href": "rf-recalibration.html#splitting-the-dataset",
    "title": "5  Recalibration of Random Forests",
    "section": "5.2 Splitting the dataset",
    "text": "5.2 Splitting the dataset\nThe process applied in this chapter is divided the following parts: 1. get the trained Random Forest classifier or regressions from Chapter 4 to obtain the predicted scores \\(\\hat{s}(\\boldsymbol x_i)\\) (i.e., either \\(\\hat{p}_{\\text{score}}\\) or \\(\\hat{p}_{\\text{vote}}\\)) 2. recalibrating the obtained scores through different approaches defined in Chapter 2 3. recalculating the different calibration metrics on the recalibrated predicted scores.\nTherefore, it is necessary to split the dataset into three parts: 1. a train set: to train the Random Forest classifier, 2. a calibration set: to train the recalibrator, 3. a test set: on which we will compute the calibration metrics.\nTo split the data, use the get_samples() from Chapter 4.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\nget_samples &lt;- function(seed,\n                        n_obs = 2000) {\n  set.seed(seed)\n  data_all &lt;- sim_data(\n    n_obs = n_obs, \n    seed = seed\n  )\n  \n  # Train/calibration/test sets----\n  data &lt;- data_all |&gt; select(d, x1:x4)\n  true_probas &lt;- data_all |&gt; select(p)\n  \n  train_index &lt;- sample(1:nrow(data), size = .6 * nrow(data), replace = FALSE)\n  tb_train &lt;- data |&gt; slice(train_index)\n  tb_calib_test &lt;- data |&gt; slice(-train_index)\n  true_probas_train &lt;- true_probas |&gt; slice(train_index)\n  true_probas_calib_test &lt;- true_probas |&gt; slice(-train_index)\n\n  calib_index &lt;- sample(\n    1:nrow(tb_calib_test), size = .5 * nrow(tb_calib_test), replace = FALSE\n  )\n  tb_calib &lt;- tb_calib_test |&gt; slice(calib_index)\n  tb_test &lt;- tb_calib_test |&gt; slice(-calib_index)\n  true_probas_calib &lt;- true_probas_calib_test |&gt; slice(calib_index)\n  true_probas_test &lt;- true_probas_calib_test |&gt; slice(-calib_index)\n\n  list(\n    data_all = data_all,\n    data = data,\n    tb_train = tb_train,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    true_probas_train = true_probas_train,\n    true_probas_calib = true_probas_calib,\n    true_probas_test = true_probas_test,\n    train_index = train_index,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs\n  )\n}\n\nWe considered 200 replications for the simulations in Chapter 4. In each simulation, we drew 2,000 observation from the data generation process.\n\nn_repl &lt;- 200\nn_obs &lt;- 2000"
  },
  {
    "objectID": "rf-recalibration.html#load-previous-simulations",
    "href": "rf-recalibration.html#load-previous-simulations",
    "title": "5  Recalibration of Random Forests",
    "section": "5.3 Load Previous Simulations",
    "text": "5.3 Load Previous Simulations\nLet us load the simulations obtained in Chapter 4. The regressions tasks:\n\nload(\"output/simulations/scores_simul_rf.rda\")\n\nAnd the classification tasks:\n\nload(\"output/simulations/scores_simul_rf_vote.rda\")\n\n\nload(\"output/simulations/scores_simul_rf.rda\")\nload(\"output/simulations/scores_simul_rf_vote.rda\")"
  },
  {
    "objectID": "rf-recalibration.html#example-of-recalibration-a-single-replication",
    "href": "rf-recalibration.html#example-of-recalibration-a-single-replication",
    "title": "5  Recalibration of Random Forests",
    "section": "5.4 Example of Recalibration a Single Replication",
    "text": "5.4 Example of Recalibration a Single Replication\nWe use a simulated single toy dataset to begin with. Simulations made on replications will be done later.\nLet us get predicted values from the first simulation:\n\ntoy_scores &lt;- scores_simul_rf[[1]]\n\nLet us get the train/calib/test data used in that simulation:\n\ntoy_data &lt;- get_samples(seed = 1, n_obs = n_obs)\n\n\ntb_toy_train &lt;- toy_data$tb_train\ntb_toy_calib &lt;- toy_data$tb_calib\ntb_toy_test &lt;- toy_data$tb_test\n\nWe extract the train/calib/test predicted returned by the random forest:\n\nscores_train &lt;- toy_scores$scores$scores_train\nscores_calib &lt;- toy_scores$scores$scores_calib\nscores_test &lt;- toy_scores$scores$scores_test\n\nWe construct a table to apply recalibration methods on:\n\ndata_train &lt;- tb_toy_train |&gt; mutate(scores = scores_train)\ndata_calib &lt;- tb_toy_calib |&gt; mutate(scores = scores_calib)\ndata_test &lt;- tb_toy_test |&gt; mutate(scores = scores_test)\n\nNow, we can apply different recalibration techniques on the recalibration dataset. Then, will apply the recalibration method on the test set.\n\n5.4.1 Platt scaling\n\n# Logistic regression\nlr &lt;- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n\nThe predicted values in the calibration set and in the test set:\n\nscore_c_platt_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\nscore_c_platt_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n\nLet us create a vector of values to estimate the calibration curve.\n\nlinspace &lt;- seq(0, 1, length.out = 101)\n\nWe can then use the fitted logistic regression to make predictions on this vector of values:\n\nscore_c_platt_linspace &lt;- predict(\n  lr, \n  newdata = tibble(scores = linspace), \n  type = \"response\"\n)\n\nLet us put these values in a tibble:\n\ntb_scores_c_platt &lt;- tibble(\n  linspace = linspace,\n  scores_c = score_c_platt_linspace #recalibrated score\n)\ntb_scores_c_platt\n\n# A tibble: 101 × 2\n   linspace scores_c\n      &lt;dbl&gt;    &lt;dbl&gt;\n 1     0       0.621\n 2     0.01    0.620\n 3     0.02    0.618\n 4     0.03    0.617\n 5     0.04    0.615\n 6     0.05    0.613\n 7     0.06    0.612\n 8     0.07    0.610\n 9     0.08    0.609\n10     0.09    0.607\n# ℹ 91 more rows\n\n\nThe predicted probabilities \\(\\hat{s}\\) will then be transformed according to the logistic model depicted in Figure 5.1.\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0,1)\n)\nlines(\n  x = tb_scores_c_platt$linspace, y = tb_scores_c_platt$scores_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.1: Recalibration Using Platt Scaling\n\n\n\n\n\n\n\n5.4.2 Isotonic regression\nLet us compute the isotonic least squares regression on the scores \\(p_u\\):\n\niso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n\nTransforming the fit into a function:\n\nfit_iso &lt;- as.stepfun(iso)\n\nThe predicted values on the calibration set and on the test set:\n\nscore_c_isotonic_calib &lt;- fit_iso(data_calib$scores)\nscore_c_isotonic_test &lt;- fit_iso(data_test$scores)\n\nThen, we can use this function to get estimated probabilities at some specific values (linspace):\n\nscore_c_isotonic_linspace &lt;- fit_iso(linspace)\n\nLet us recreate the tibble with the recalibrated scores:\n\ntb_scores_c_isotonic &lt;- tibble(\n  linspace = linspace,\n  scores_c = score_c_isotonic_linspace\n)\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.2.\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = tb_scores_c_isotonic$linspace, y = tb_scores_c_isotonic$scores_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.2: Recalibration Using Isotonic Regression\n\n\n\n\n\n\n\n5.4.3 Beta calibration\n\nlibrary(betacal)\n# Beta calibration using the paper package\nbc &lt;- beta_calibration(\n  p = data_calib$scores, \n  y = data_calib$d, \n  parameters = \"abm\" # 3 parameters a, b & m\n)\n\n[1] -0.2880062\n[1] -6.936872\n\n\nThe predicted values on the calibration set and on the test set:\n\nscore_c_beta_calib &lt;- beta_predict(p = data_calib$scores, bc)\nscore_c_beta_test &lt;- beta_predict(p = data_test$scores, bc)\n\nWe can then use the beta calibration model to make predictions at the desired values (linspace).\n\nscore_c_beta_linspace &lt;- beta_predict(linspace, bc)\n\nLet us recreate the tibble with the recalibrated scores:\n\ntb_scores_c_beta &lt;- tibble(\n  linspace = linspace,\n  scores_c = score_c_beta_linspace\n)\n\nThe predicted probabilities \\(p_u\\) will then be transformed according to the logistic model depicted in Figure 5.3\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = tb_scores_c_beta$linspace, y = tb_scores_c_beta$scores_c, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.3: Recalibration Using Beta Calibration\n\n\n\n\n\n\n\n5.4.4 Local regression\n\nlibrary(locfit)\n\nWe consider three versions here, with different degrees for the polynomials (0, 1, or 2). We set the number of nearest neighbors to use to nn = 0.15, that is, 15%.\n\nDeg 0Deg 1Deg 2\n\n\n\n# Deg 0\nlocfit_0 &lt;- locfit(\n  formula = d ~ lp(scores, nn = 0.15, deg = 0), \n  kern = \"rect\", maxk = 200, data = data_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_0_calib &lt;- predict(locfit_0, newdata = data_calib)\nscore_c_locfit_0_test &lt;- predict(locfit_0, newdata = data_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_0_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.4: Recalibration Using Local Regression\n\n\n\n\n\n\n\n\n# Deg 1\nlocfit_1 &lt;- locfit(\n  formula = d ~ lp(scores, nn = 0.15, deg = 1), \n  kern = \"rect\", maxk = 200, data = data_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_1_calib &lt;- predict(locfit_1, newdata = data_calib)\nscore_c_locfit_1_test &lt;- predict(locfit_1, newdata = data_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_1_linspace &lt;- predict(locfit_1, newdata = linspace)\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_1_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.5: Recalibration Using Local Regression\n\n\n\n\n\n\n\n\n# Deg 2\nlocfit_2 &lt;- locfit(\n  formula = d ~ lp(scores, nn = 0.15, deg = 2), \n  kern = \"rect\", maxk = 200, data = data_calib\n)\n\nLet us get the predicted values in the calibration data:\n\nscore_c_locfit_2_calib &lt;- predict(locfit_2, newdata = data_calib)\nscore_c_locfit_2_test &lt;- predict(locfit_2, newdata = data_test)\n\nThen, we can use the estimated mapping to get estimated probabilities at some specific values (linspace):\n\nscore_c_locfit_2_linspace &lt;- predict(locfit_2, newdata = linspace)\n\n\npar(mar = c(4.1, 4.1, 2.1, 2.1))\nplot(\n  data_calib$scores, data_calib$d, type = \"p\", cex = .5, pch = 19,\n  col = adjustcolor(\"black\", alpha.f = .4),\n  xlab = \"p\", ylab = \"g(p)\",\n  xlim = c(0, 1)\n)\nlines(\n  x = linspace, y = score_c_locfit_2_linspace, \n  type = \"l\", col = \"#D55E00\"\n)\n\n\n\nFigure 5.6: Recalibration Using Local Regression"
  },
  {
    "objectID": "rf-recalibration.html#helper-functions",
    "href": "rf-recalibration.html#helper-functions",
    "title": "5  Recalibration of Random Forests",
    "section": "5.5 Helper Functions",
    "text": "5.5 Helper Functions\nFirst, we need the functions that fit the Random Forest (for both regression and classification) and the functions that compute the performance metrics and the calibration metrics (those defined in Chapter 1).\n\n5.5.1 Fit the Random Forest\nIn the case of regression:\n\nlibrary(randomForest)\n#' Apply Random Forest algorithm\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data, \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  scores_train &lt;- predict(rf, newdata = train_data, type = \"response\")\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"response\")\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"response\")\n  \n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\nIn the case of classification:\n\n#' Apply Random Forest algorithm as a classifier\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf_vote &lt;- function(train_data, calib_data, test_data) {\n  rf &lt;- randomForest(\n    d ~ ., data = train_data |&gt; mutate(d = factor(d)), \n    nodesize = 0.1 * nrow(train_data),\n    ntree = 500\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"vote\")[, \"1\"]\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"vote\")[, \"1\"]\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"vote\")[, \"1\"]\n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n\n5.5.2 Standard Metrics\n\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\ncompute_gof &lt;- function(true_prob,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  mse &lt;- mean((true_prob - pred)^2)\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR\n  )\n}\n\n\n\n5.5.3 Calibration Metrics\n\nBrier ScoreECEQMSEWMSE\n\n\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n\n\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n\n\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n\n\n\nlibrary(binom)\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  if (all(is.na(scores))) {\n    wmse &lt;- NA\n  } else {\n    dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  wmse &lt;- local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n  }\n}\n\n\n\n\n\n\n5.5.4 Recalibration Functions\nWe define the recalibrate() function which recalibrates a model using the observed events \\(d\\), the predicted associated probabilities \\(p\\) and a given recalibration technique (as presented above in Section 2.2).\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' #' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression, `\"beta\"` for beta calibration, \n#'   `\"locfit\"` for local regression)\n#' @param params list of named parameters to use in the local regression \n#'   (`nn` for fraction of nearest neighbors to use, `deg` for degree)\n#' @param linspace vector of alues at which to compute the recalibrated scores\n#' @returns list of three elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set, and recalibrated scores on a segment \n#'   of values\nrecalibrate &lt;- function(obs_calib, \n                        pred_calib,\n                        obs_test,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\", \"beta\", \"locfit\"),\n                        params = NULL,\n                        linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    # Recalibrator\n    lr &lt;- glm(\n      d ~ scores, family = binomial(link = 'logit'), data = data_calib\n    )\n    # Recalibrated scores on calib/test sets\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n    # Recalibrated scores on [0,1]\n    score_c_linspace &lt;- predict(\n      lr, newdata = tibble(scores = linspace), type = \"response\"\n    )\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    score_c_linspace &lt;- fit_iso(linspace)\n  } else if (method == \"beta\") {\n    capture.output({\n      bc &lt;- try(beta_calibration(\n        p = data_calib$scores, \n        y = data_calib$d, \n        parameters = \"abm\" # 3 parameters a, b & m\n      ))\n    })\n    if (!inherits(bc, \"try-error\")) {\n      score_c_calib &lt;- beta_predict(p = data_calib$scores, bc)\n      score_c_test &lt;- beta_predict(p = data_test$scores, bc)\n      score_c_linspace &lt;- beta_predict(p = linspace, bc)\n    } else {\n      score_c_calib &lt;- score_c_test &lt;- score_c_linspace &lt;- NA\n    }\n    \n  } else if (method == \"locfit\") {\n    locfit_reg &lt;- locfit(\n      formula = d ~ lp(scores, nn = params$nn, deg = params$deg), \n      kern = \"rect\", maxk = 200, data = data_calib\n    )\n    score_c_calib &lt;- predict(locfit_reg, newdata = data_calib)\n    score_c_calib[score_c_calib &lt; 0] &lt;- 0\n    score_c_calib[score_c_calib &gt; 1] &lt;- 1\n    score_c_test &lt;- predict(locfit_reg, newdata = data_test)\n    score_c_test[score_c_test &lt; 0] &lt;- 0\n    score_c_test[score_c_test &gt; 1] &lt;- 1\n    score_c_linspace &lt;- predict(locfit_reg, newdata = linspace)\n    score_c_linspace[score_c_linspace &lt; 0] &lt;- 0\n    score_c_linspace[score_c_linspace &gt; 1] &lt;- 1\n  } else {\n    stop(str_c(\n      'Wrong method. Use one of the following:',\n      '\"platt\", \"isotonic\", \"beta\", \"locfit\"'\n    ))\n  }\n  \n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  # For linear space\n  tb_score_c_linspace &lt;- tibble(\n    linspace = linspace,\n    p_c = score_c_linspace\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test,\n    tb_score_c_linspace = tb_score_c_linspace\n  )\n  \n}\n\nLet us also define a function that computes the different calibration metrics for a single replication of the simulations.\n\n#' Computes the calibration metrics for a set of observed and predicted \n#' probabilities\n#' \n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#' @param linspace vector of values at which to compute the WMSE\n#' @param k number of classes (bins) to create (default to `10`)\ncompute_metrics &lt;- function(obs, \n                            scores, \n                            true_probas,\n                            linspace,\n                            k = 10) {\n  mse &lt;- mean((true_probas - scores)^2)\n  brier &lt;- brier_score(obs = obs, scores = scores)\n  if (length(unique(scores)) &gt; 1) {\n    ece &lt;- e_calib_error(obs = obs, scores = scores, k = k, threshold = .5)\n    qmse &lt;- qmse_error(obs = obs, scores = scores, k = k, threshold = .5)\n  } else {\n    ece &lt;- NA\n    qmse &lt;- NA\n  }\n  \n  expected_events &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs, \n      scores = scores,\n      tau = .x, nn = .15, prob = .95, method = \"probit\")\n  ) |&gt; \n    bind_rows()\n  wmse &lt;- weighted_mse(local_scores = expected_events, scores = scores)\n  tibble(\n    mse = mse, brier = brier, ece = ece, qmse = qmse, wmse = wmse\n  )\n}\n\nLastly, we define the f_simul_recalib_rf() function to perform recalibration for a single simulation.\n\n#' Reclibrates scores for a simulation and computes calibration metrics\n#' on true probas, uncalibrated scores, and calibrated scores\n#' \n#' @param simul simulation result obtained with `calib_metrics_rf_uncalib()`\n#' @param linspace values at which to compute the mean observed event when computing the WMSE\nf_simul_recalib_rf &lt;- function(simul, linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  \n  ## 1. Generate Data----\n  data &lt;- get_samples(seed = seed, n_obs = n_obs)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # True probabilities (from DGP)\n  true_probas_train &lt;- data$true_probas_train |&gt; pull(p)\n  true_probas_calib &lt;- data$true_probas_calib |&gt; pull(p)\n  true_probas_test &lt;- data$true_probas_test |&gt; pull(p)\n  \n  # 2. Extract predicted values by the random forest\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  ## 3. Recalibration----\n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n  \n  res_recalibration &lt;- map2(\n    .x = methods,\n    .y = params,\n    .f = ~recalibrate(\n      obs_calib = tb_calib$d, \n      pred_calib = scores_calib, \n      obs_test = tb_test$d, \n      pred_test = scores_test,\n      method = .x,\n      params = .y,\n      linspace = linspace\n    )\n  )\n  names(res_recalibration) &lt;- method_names\n  \n  ## 4. Calibration metrics----\n  \n  ### Using True Probabilities\n  #### Calibration Set\n  calib_metrics_true_calib &lt;- compute_metrics(\n    obs         = tb_calib$d, \n    scores      = true_probas_calib, \n    true_probas = true_probas_calib,\n    linspace = linspace,\n    k = 5\n  ) |&gt; \n    mutate(method = \"True Prob.\", sample = \"Calibration\")\n  #### Test Set\n  calib_metrics_true_test &lt;- compute_metrics(\n    obs         = tb_test$d, \n    scores      = true_probas_test, \n    true_probas = true_probas_test,\n    linspace = linspace,\n    k = 5\n  ) |&gt; \n    mutate(method = \"True Prob.\", sample = \"Test\")\n  \n  ### Without Recalibration\n  #### Calibration Set\n  calib_metrics_without_calib &lt;- compute_metrics(\n    obs         = tb_calib$d, \n    scores      = scores_calib, \n    true_probas = true_probas_calib,\n    linspace = linspace,\n    k = 5\n  ) |&gt; \n    mutate(method = \"No Calibration\", sample = \"Calibration\")\n  #### Test Set\n  calib_metrics_without_test &lt;- compute_metrics(\n    obs         = tb_test$d, \n    scores      = scores_test, \n    true_probas = true_probas_test,\n    linspace = linspace,\n    k = 5\n  ) |&gt; \n    mutate(method = \"No Calibration\", sample = \"Test\")\n  \n  calib_metrics &lt;- \n    calib_metrics_true_calib |&gt; \n    bind_rows(calib_metrics_true_test) |&gt; \n    bind_rows(calib_metrics_without_calib) |&gt; \n    bind_rows(calib_metrics_without_test)\n  \n  ### With Recalibration: loop on methods\n  for (method in method_names) {\n    res_recalibration_current &lt;- res_recalibration[[method]]\n    #### Calibration Set\n    calib_metrics_without_calib &lt;- compute_metrics(\n      obs         = tb_calib$d, \n      scores      = res_recalibration_current$tb_score_c_calib$p_c, \n      true_probas = true_probas_calib,\n      linspace = linspace,\n      k = 5\n    ) |&gt; \n      mutate(method = method, sample = \"Calibration\")\n    #### Test Set\n    calib_metrics_without_test &lt;- compute_metrics(\n      obs         = tb_test$d, \n      scores      = res_recalibration_current$tb_score_c_test$p_c, \n      true_probas = true_probas_test,\n      linspace = linspace,\n      k = 5\n    ) |&gt; \n      mutate(method = method, sample = \"Test\")\n    \n    calib_metrics &lt;- \n      calib_metrics |&gt; \n      bind_rows(calib_metrics_without_calib) |&gt; \n      bind_rows(calib_metrics_without_test)\n  }\n  \n  calib_metrics &lt;- \n    calib_metrics |&gt; \n    mutate(\n      seed = seed\n    )\n  \n  list(\n    res_recalibration = res_recalibration,\n    linspace = linspace,\n    calib_metrics = calib_metrics,\n    seed = seed,\n    n_obs = n_obs\n  )\n}"
  },
  {
    "objectID": "rf-recalibration.html#recalibration",
    "href": "rf-recalibration.html#recalibration",
    "title": "5  Recalibration of Random Forests",
    "section": "5.6 Recalibration",
    "text": "5.6 Recalibration\nLet us apply the different techniques to recalibrate the predicted values of the random forests, both for the regression forests and the classification forests.\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf))\n  simul_recalib_rf &lt;- furrr::future_map(\n    .x = scores_simul_rf,\n    .f = ~{\n      p()\n      f_simul_recalib_rf(simul = .x,)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(scores_simul_rf_vote))\n  simul_recalib_rf_vote &lt;- furrr::future_map(\n    .x = scores_simul_rf_vote,\n    .f = ~{\n      p()\n      f_simul_recalib_rf(simul = .x,)\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})"
  },
  {
    "objectID": "rf-recalibration.html#recalibration-standard-metrics",
    "href": "rf-recalibration.html#recalibration-standard-metrics",
    "title": "5  Recalibration of Random Forests",
    "section": "5.7 Recalibration Standard Metrics",
    "text": "5.7 Recalibration Standard Metrics\nWe (re)define the function compute_gof_simul() to apply compute_gof(), defined above, to compute the different standard performance metrics on recalibrated predicted scores (see Section 1.4 in Chapter 1):\n\n#' Computes goodness of fit metrics for a replication\n#'\n#' @param n_obs desired number of observation\n#' @param seed random seed to use\n#' @param type of Random Forest to use (either `regression` or `classification`) \n#' @param linspace vector of alues at which to compute the recalibrated scores\ncompute_gof_simul &lt;- function(n_obs,\n                              seed,\n                              type = c(\"regression\", \"classification\"),\n                              linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 100)\n  \n  current_seed &lt;- seed\n  \n  # Generate Data\n  current_data &lt;- get_samples(\n    n_obs = n_obs, seed = current_seed\n  )\n  \n  # Get the calib/test datasets with true probabilities\n  data_all_train &lt;- current_data$tb_train\n  data_all_calib &lt;- current_data$tb_calib\n  data_all_test &lt;- current_data$tb_test\n  \n  # Test set\n  true_prob_test &lt;- current_data$true_probas_test$p\n  obs_test &lt;- data_all_test$d\n  ## Fit the RF\n  if (type == \"regression\"){\n    scores &lt;- apply_rf(\n    train_data = data_all_train,\n    calib_data = data_all_calib,\n    test_data = data_all_test\n    )\n  } else if (type == \"classification\"){\n    scores &lt;- apply_rf_vote(\n    train_data = data_all_train,\n    calib_data = data_all_calib,\n    test_data = data_all_test\n    )\n  } else {\n    stop(\"Random Forest type should be either regression or classification.\")\n  }\n  \n  pred_calib &lt;- scores$scores_calib\n  pred_test &lt;- scores$scores_test\n  \n  # Recalibration\n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n\n  res_recalibration &lt;- map2(\n    .x = methods,\n    .y = params,\n    .f = ~recalibrate(\n      obs_calib = data_all_calib$d, \n      pred_calib = pred_calib, \n      obs_test = data_all_test$d, \n      pred_test = pred_test,\n      method = .x,\n      params = .y,\n      linspace = linspace\n    )\n  )\n  names(res_recalibration) &lt;- method_names\n  \n  # Initialisation\n  gof_metrics_simul_test &lt;- tibble()\n  \n  # Calculate standard metrics\n  ## With Recalibration: loop on methods\n  for (method in method_names) {\n    res_recalibration_current &lt;- res_recalibration[[method]]\n    ### Computation of metrics only on the test set\n    metrics_simul_test &lt;- map(\n    .x = seq(0, 1, by = .01), # we vary the probability threshold\n    .f = ~compute_gof(\n      true_prob = true_prob_test,\n      obs = obs_test,\n      #### the predictions are now recalibrated:\n      pred = res_recalibration_current$tb_score_c_test$p_c,\n      threshold = .x\n      )\n    ) |&gt;\n      list_rbind()\n    \n    metrics_simul_test &lt;- metrics_simul_test |&gt;\n      mutate(\n        seed = current_seed,\n        method = method\n      )\n    \n    gof_metrics_simul_test &lt;- gof_metrics_simul_test |&gt;\n      bind_rows(metrics_simul_test)\n  }\n\n  gof_metrics_simul_test\n}\n\n\nRegressionClassification\n\n\nLet us apply this function to the different simulations obtained with a Random Forest regressor:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  regression_recalib_metrics &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      p()\n      compute_gof_simul(\n        n_obs = n_obs,\n        seed = .x,\n        type = \"regression\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nregression_recalib_metrics &lt;- list_rbind(regression_recalib_metrics)\n\n\n\nLet us apply this function to the different simulations obtained with a Random Forest classifier:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  classification_recalib_metrics &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      p()\n      compute_gof_simul(\n        n_obs = n_obs,\n        seed = .x,\n        type = \"classification\"\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nclassification_recalib_metrics &lt;- list_rbind(classification_recalib_metrics)\n\n\n\n\n\n5.7.1 Results\nLet us visualize how the performance metrics vary from one replication to another, on the test set.\n\n#' Boxplots for the simulations to visualize the distribution of some \n#' traditional metrics as a function of the probability threshold.\n#' And, ROC curves\n#' The resulting figure is a panel of graphs, with vayring values for the \n#' transformation applied to the probabilities (in columns) and different \n#' metrics (in rows).\n#' \n#' @param tb_metrics tibble with computed metrics for the simulations\n#' @param metrics names of the metrics computed\nboxplot_simuls_metrics &lt;- function(tb_metrics,\n                                   metrics) {\n  \n  par(mfrow = c(2, length(metrics)%/%2+1))\n  for (i_metric in 1:length(metrics)) {\n    metric &lt;- metrics[i_metric]\n    tb_metrics_current &lt;- tb_metrics\n    if (metric == \"roc\") {\n      seeds &lt;- unique(tb_metrics_current$seed)\n      par(mar = c(4.1, 4.1, 2.1, 2.1))\n      plot(\n        0:1, 0:1,\n        type = \"l\", col = NULL,\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"False Positive Rate\",\n        ylab = \"True Positive Rate\",\n        main = \"\"\n      )\n      for (i_seed in 1:length(seeds)) {\n        tb_metrics_current_seed &lt;- \n          tb_metrics_current |&gt; \n          filter(seed == seeds[i_seed])\n        lines(\n          x = tb_metrics_current_seed$FPR,\n          y = tb_metrics_current_seed$sensitivity,\n          lwd = 2, col = adjustcolor(\"black\", alpha.f = .04)\n        )\n      }\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n    } else {\n      # not ROC\n      tb_metrics_current &lt;- \n        tb_metrics_current |&gt; \n        filter(threshold %in% seq(0, 1, by = .1))\n      form &lt;- str_c(metric, \"~threshold\")\n      par(mar = c(4.1, 4.1, 2.1, 2.1))\n      boxplot(\n        formula(form), data = tb_metrics_current,\n        xlab = \"Threshold\", ylab = metric\n      )\n    }\n  }\n}\n\n\nRegressionClassification\n\n\nWe aim to create a set of boxplots to visually assess the Random Forest in the case of regression, after the application of recalibration methods.\n\nmetrics &lt;- c(\"mse\", \"accuracy\", \"sensitivity\", \"specificity\", \"roc\")\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.7: Standard metrics on Random Forest predicted scores (regression) after recalibration using Platt scaling.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.8: Standard metrics on Random Forest predicted scores (regression) after recalibration using isotonic regression.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.9: Standard metrics on Random Forest predicted scores (regression) after recalibration using Beta calibration.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.10: Standard metrics on Random Forest predicted scores (regression) after recalibration using local regression (degree 0).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.11: Standard metrics on Random Forest predicted scores (regression) after recalibration using local regression (degree 1).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = regression_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.12: Standard metrics on Random Forest predicted scores (regression) after recalibration using local regression (degree 2).\n\n\n\n\n\n\n\n\n\n\nWe aim to create a set of boxplots to visually assess the Random Forest in the case of classification, when the predicted scores are recalibrated.\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.13: Standard metrics on Random Forest predicted scores (classification) after recalibration using Platt scaling.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.14: Standard metrics on Random Forest predicted scores (classification) after recalibration using isotonic regression.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.15: Standard metrics on Random Forest predicted scores (classification) after recalibration using Beta calibration.\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.16: Standard metrics on Random Forest predicted scores (classification) after recalibration using local regression (degree 0).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.17: Standard metrics on Random Forest predicted scores (classification) after recalibration using local regression (degree 1).\n\n\n\n\n\n\n\n\n\nDisplay the R codes to produce the Figure.\nboxplot_simuls_metrics(tb_metrics = classification_recalib_metrics, metrics = metrics)\n\n\n\n\nFigure 5.18: Standard metrics on Random Forest predicted scores (classification) after recalibration using local regression (degree 2)."
  },
  {
    "objectID": "rf-recalibration.html#recalibration-metrics",
    "href": "rf-recalibration.html#recalibration-metrics",
    "title": "5  Recalibration of Random Forests",
    "section": "5.8 Recalibration Metrics",
    "text": "5.8 Recalibration Metrics\n\nRegressionClassificationBoth\n\n\n\n\nDisplay R codes used to create the Figure.\ncalibration_rf_simuls_c &lt;- \n  map(simul_recalib_rf, \"calib_metrics\") |&gt; \n  list_rbind() |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"Test\", \"Calibration\")\n    )\n  ) |&gt; \n  mutate(\n    method = factor(\n      method,\n      levels = c(\n        \"True Prob.\", \"No Calibration\",\n        \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n      ) |&gt; rev(),\n      labels = c(\n        \"True Prob.\", \"No Calibration\", \n        \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n        \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\"\n      ) |&gt; rev()\n    )\n  ) |&gt; \n  arrange(method, sample)\n\nlabs_y &lt;- \n  calibration_rf_simuls_c |&gt; select(method, sample) |&gt; unique() |&gt; \n  mutate(label = str_c(method, \" (\", sample, \")\")) |&gt; \n  pull(label)\n\nmetrics &lt;- c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\")\nmetrics_lab &lt;- c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\ncolours_samples &lt;- c(\n  \"Test\" = \"#009E73\", \"Calibration\" = \"#D55E00\"\n)\n\npar(mfrow = c(3,2))\nfor (i in 1:length(metrics)) {\n  metric &lt;- metrics[i]\n  title &lt;- metrics_lab[i]\n  form &lt;- str_c(metric, \"~sample + method\") |&gt; as.formula()\n  par(mar = c(2.1, 15.1, 2.1, 2.1))\n  boxplot(\n    form, data = calibration_rf_simuls_c,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    horizontal = TRUE,\n    las = 1,\n    col = colours_samples,\n    yaxt = \"n\"\n  )\n  axis(\n    side = 2, at = 1:length(labs_y), \n    labels = latex2exp::TeX(labs_y),\n    las = 1,\n    # col.axis = \"black\"\n  )\n  for (i in seq(1, 2*(length(labs_y) - 1), by = 2) + 1.5) {\n    abline(h = i, lty = 1, col = \"gray\")\n  }\n}\n\n\n\n\nFigure 5.19: Calibration metrics for various recalibration techniques applied to the Random Forest scores (regression); results from the 200 replications.\n\n\n\n\n\n\n\n\n\nDisplay R codes used to create the Figure.\ncalibration_rf_simuls_vote_c &lt;- \n  map(simul_recalib_rf_vote, \"calib_metrics\") |&gt; \n  list_rbind() |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"Test\", \"Calibration\")\n    )\n  ) |&gt; \n  mutate(\n    method = factor(\n      method,\n      levels = c(\n        \"True Prob.\", \"No Calibration\",\n        \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n      ) |&gt; rev(),\n      labels = c(\n        \"True Prob.\", \"No Calibration\", \n        \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n        \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\"\n      ) |&gt; rev()\n    )\n  ) |&gt; \n  arrange(method, sample)\n\nlabs_y &lt;- \n  calibration_rf_simuls_vote_c |&gt; select(method, sample) |&gt; unique() |&gt; \n  mutate(label = str_c(method, \" (\", sample, \")\")) |&gt; \n  pull(label)\n\nmetrics &lt;- c(\"mse\", \"brier\", \"ece\", \"qmse\", \"wmse\")\nmetrics_lab &lt;- c(\"True MSE\", \"Brier Score\", \"ECE\", \"QMSE\", \"WMSE\")\ncolours_samples &lt;- c(\n  \"Test\" = \"#009E73\", \"Calibration\" = \"#D55E00\"\n)\n\npar(mfrow = c(3,2))\nfor (i in 1:length(metrics)) {\n  metric &lt;- metrics[i]\n  title &lt;- metrics_lab[i]\n  form &lt;- str_c(metric, \"~sample + method\") |&gt; as.formula()\n  par(mar = c(2.1, 15.1, 2.1, 2.1))\n  boxplot(\n    form, data = calibration_rf_simuls_vote_c,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    horizontal = TRUE,\n    las = 1,\n    col = colours_samples,\n    yaxt = \"n\"\n  )\n  axis(\n    side = 2, at = 1:length(labs_y), \n    labels = latex2exp::TeX(labs_y),\n    las = 1,\n    # col.axis = \"black\"\n  )\n  for (i in seq(1, 2*(length(labs_y) - 1), by = 2) + 1.5) {\n    abline(h = i, lty = 1, col = \"gray\")\n  }\n}\n\n\n\n\nFigure 5.20: Calibration metrics for various recalibration techniques applied to the Random Forest scores (classification); results from the 200 replications.\n\n\n\n\n\n\n\nWe bind the results and define a plotting function.\n\n\nDisplay the R Codes\ncalibration_rf_simuls_both &lt;- \n  map(simul_recalib_rf, \"calib_metrics\") |&gt; \n  list_rbind() |&gt; \n  mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    map(simul_recalib_rf_vote, \"calib_metrics\") |&gt; \n      list_rbind() |&gt; \n      mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"Test\", \"Calibration\")\n    ),\n    model = factor(model, levels= c(\"Regression\", \"Classification\") |&gt; rev()),\n    method = factor(\n      method,\n      levels = c(\n        \"True Prob.\", \"No Calibration\",\n        \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n      ) |&gt; rev(),\n      labels = c(\n        \"True Prob.\", \"No Calibration\", \n        \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n        \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\"\n      ) |&gt; rev()\n    )\n  ) |&gt; \n  arrange(method, model, sample)\n\n\n\nplot_metric_simuls &lt;- function(metric) {\n  metrics_labs &lt;- c(\n    \"mse\" = \"True MSE\",\n    \"brier\" = \"Brier Score\", \n    \"ece\" = \"ECE\", \n    \"qmse\" = \"QMSE\", \n    \"wmse\" = \"WMSE\"\n  )\n  colours_samples &lt;- c(\n    \"Test\" = \"#009E73\", \"Calibration\" = \"#D55E00\"\n  )\n  title &lt;- metrics_labs[metric]\n  form &lt;- str_c(metric, \"~sample + model + method\") |&gt; as.formula()\n  \n  labs_y &lt;- \n    calibration_rf_simuls_both |&gt; \n    select(sample, method, model) |&gt; \n    unique() |&gt; \n    group_by(model, method) |&gt; \n    mutate(\n      label = ifelse(\n        row_number() == 1,\n        yes = str_c(method, \", \", model),\n        no = \"\"\n      )\n    ) |&gt; \n    group_by(method) |&gt; \n    mutate(\n      label = ifelse(\n        row_number() == 1,\n        yes = str_c(model),\n        no = label\n      )\n    ) |&gt; \n    \n    ungroup() |&gt; \n    pull(label)\n  labs_y &lt;- labs_y[!labs_y == \"\"]\n  \n  par(mar = c(2.1, 15.1, 2.1, 2.1))\n  boxplot(\n    form, data = calibration_rf_simuls_both,\n    xlab = \"\", ylab = \"\",\n    main = title,\n    horizontal = TRUE,\n    las = 1,\n    col = colours_samples,\n    yaxt = \"n\"\n  )\n  axis(\n    side = 2, at = seq(1, 2*(length(labs_y)), by = 2) + .5, \n    labels = latex2exp::TeX(labs_y),\n    las = 1,\n  )\n  for (i in seq(1, 2*(length(labs_y) - 1), by = 2) + 1.5) {\n    abline(h = i, lty = 1, col = \"gray\")\n  }\n  for (i in seq(1, 2*(length(labs_y) + 1), by = 4) - .5) {\n    abline(h = i, lty = 1, col = \"black\")\n  }\n  \n}\n\n\n\nMSEBrier ScoreECEQMSEQMSE\n\n\n\n\nDisplay R codes used to create the Figure.\nplot_metric_simuls(metric = \"mse\")\n\n\n\n\nFigure 5.21: True MSE for the Random Forest (regression or classification) scores with various scores used as predictions (true probabilities, uncalibrated scores, recalibreated scores), on the calibration set and on the test set; results from the 200 replications.\n\n\n\n\n\n\n\n\n\nDisplay R codes used to create the Figure.\nplot_metric_simuls(metric = \"brier\")\n\n\n\n\nFigure 5.22: Brier Score for the Random Forest (regression or classification) scores with various scores used as predictions (true probabilities, uncalibrated scores, recalibreated scores), on the calibration set and on the test set; results from the 200 replications.\n\n\n\n\n\n\n\n\n\nDisplay R codes used to create the Figure.\nplot_metric_simuls(metric = \"ece\")\n\n\n\n\nFigure 5.23: Expected Calibration Error for the Random Forest (regression or classification) scores with various scores used as predictions (true probabilities, uncalibrated scores, recalibreated scores), on the calibration set and on the test set; results from the 200 replications.\n\n\n\n\n\n\n\n\n\nDisplay R codes used to create the Figure.\nplot_metric_simuls(metric = \"qmse\")\n\n\n\n\nFigure 5.24: Quantile-based Mean Squared Error for the Random Forest (regression or classification) scores with various scores used as predictions (true probabilities, uncalibrated scores, recalibreated scores), on the calibration set and on the test set; results from the 200 replications.\n\n\n\n\n\n\n\n\n\nDisplay R codes used to create the Figure.\nplot_metric_simuls(metric = \"wmse\")\n\n\n\n\nFigure 5.25: Weighted Mean Squared Error for the Random Forest (regression or classification) scores with various scores used as predictions (true probabilities, uncalibrated scores, recalibreated scores), on the calibration set and on the test set; results from the 200 replications."
  },
  {
    "objectID": "rf-recalibration.html#recalibration-visualizations",
    "href": "rf-recalibration.html#recalibration-visualizations",
    "title": "5  Recalibration of Random Forests",
    "section": "5.9 Recalibration Visualizations",
    "text": "5.9 Recalibration Visualizations\n\n5.9.1 Helper Functions\n\n5.9.1.1 Functions for Quantile-based Bins\nFirst, we define a function to get the calibration curves for a single simulation, and for a single recalibration method.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the quantile-based approach, for recalibrated results\n#' for a single recalibration method\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\n#' @param method name of the recalibration method to focus on\ncalib_curve_method_quant_simul_c_rf &lt;- function(simul,\n                                                linspace = NULL,\n                                                method) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Uncalibrated Scores estimated by the RF\n  scores_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_u\n  scores_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_u\n  # Recalibrated Scores of the RF, for the current method\n  scores_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_c\n  scores_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_c\n  \n  # Summary on each bin, uncalibrated scores\n  summary_bins_calib &lt;- get_summary_bins(\n    obs = tb_calib$d,\n    scores = scores_calib, \n    k = 10, threshold = .5)\n  summary_bins_test &lt;- get_summary_bins(\n    obs = tb_test$d,\n    scores = scores_test, \n    k = 10, threshold = .5)\n  # Summary on each bin, recalibrated scores\n  if (length(unique(scores_c_calib)) == 1) {\n    summary_bins_c_calib &lt;- tibble()\n  } else {\n    summary_bins_c_calib &lt;- get_summary_bins(\n      obs = tb_calib$d,\n      scores = scores_c_calib, \n      k = 10, threshold = .5)\n  }\n  \n  if (length(unique(scores_c_test)) == 1) {\n    summary_bins_c_test &lt;- tibble()\n  } else {\n    summary_bins_c_test &lt;- get_summary_bins(\n      obs = tb_test$d,\n      scores = scores_c_test, \n      k = 10, threshold = .5)\n  }\n  \n  \n  summary_bins_calib |&gt; mutate(sample = \"calibration\", type = \"uncalibrated\") |&gt; \n    bind_rows(\n      summary_bins_test |&gt; mutate(sample = \"test\", type = \"uncalibrated\")\n    ) |&gt; \n    bind_rows(\n      summary_bins_c_calib |&gt; mutate(sample = \"calibration\", type = \"recalibrated\")\n    ) |&gt; \n    bind_rows(\n      summary_bins_c_test |&gt; mutate(sample = \"test\", type = \"recalibrated\")\n    ) |&gt; \n    select(score_class, mean_score, mean_obs, sample, type) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed,\n      method = method\n    )\n}\n\nThen, we define a function that applies the first one to all recalibration methods for a single simulation.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the quantile-based approach, for recalibrated results\n#' for all recalibration methods performed in the simulation\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_quant_simul_c_rf &lt;- function(simul, linspace = NULL) {\n  methods &lt;- names(simul$res_recalibration)\n  map(\n    .x = methods, \n    .f = ~calib_curve_method_quant_simul_c_rf(\n      simul = simul, linspace = linspace, method = .x\n    )\n  ) |&gt; \n    list_rbind()\n}\n\n\n\n5.9.1.2 Functions for Local Regression\nFirst, we define a function to get the calibration curves for a single simulation, and for a single recalibration method.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach, for recalibrated results\n#' for a single recalibration method\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\n#' @param method name of the recalibration method to focus on\ncalib_curve_method_locfit_simul_c_rf &lt;- function(simul,\n                                                 linspace = NULL,\n                                                 method) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Uncalibrated Scores estimated by the RF\n  scores_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_u\n  scores_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_u\n  # Recalibrated Scores of the RF, for the current method\n  scores_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_c\n  scores_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_c\n  \n  # Add a little noise (otherwise, R may crash...)\n  scores_calib &lt;- scores_calib + rnorm(length(scores_calib), 0, .001)\n  scores_test &lt;- scores_test + rnorm(length(scores_test), 0, .001)\n  scores_c_calib &lt;- scores_c_calib + rnorm(length(scores_c_calib), 0, .001)\n  scores_c_test &lt;- scores_c_test + rnorm(length(scores_c_test), 0, .001)\n  \n  # Local Regression, uncalibrated scores\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  # Make sure to have values in [0,1]\n  score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n  score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n  score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n  score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n  \n  # Local Regression, recalibrated scores\n  if (length(unique(scores_c_calib)) == 1) {\n    score_c_locfit_0_c_calib &lt;- rep(NA, length(linspace))\n  } else {\n    locfit_0_c_calib &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_calib$d, score = scores_c_calib)\n    )\n    score_c_locfit_0_c_calib &lt;- predict(locfit_0_c_calib, newdata = linspace)\n    # Make sure to have values in [0,1]\n    score_c_locfit_0_c_calib[score_c_locfit_0_c_calib &gt; 1] &lt;- 1\n    score_c_locfit_0_c_calib[score_c_locfit_0_c_calib &lt; 0] &lt;- 0\n  }\n  \n  if (length(unique(scores_c_test)) == 1) {\n    score_c_locfit_0_c_test &lt;- rep(NA, length(linspace))\n  } else {\n    locfit_0_c_test &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_test$d, score = scores_c_test)\n    )\n    score_c_locfit_0_c_test &lt;- predict(locfit_0_c_test, newdata = linspace)\n    # Make sure to have values in [0,1]\n    score_c_locfit_0_c_test[score_c_locfit_0_c_test &gt; 1] &lt;- 1\n    score_c_locfit_0_c_test[score_c_locfit_0_c_test &lt; 0] &lt;- 0\n  }\n  \n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_calib,\n    sample = \"calibration\",\n    type = \"uncalibrated\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_test,\n    sample = \"test\",\n    type = \"uncalibrated\"\n  )\n  res_c_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_c_calib,\n    sample = \"calibration\",\n    type = \"recalibrated\"\n  )\n  res_c_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_c_test,\n    sample = \"test\",\n    type = \"recalibrated\"\n  )\n  \n  res_calib |&gt; \n    bind_rows(res_test) |&gt; \n    bind_rows(res_c_calib) |&gt; \n    bind_rows(res_c_test) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed,\n      method = method\n    )\n}\n\nThen, we define a function that applies the first one to all recalibration methods for a single simulation.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach, for recalibrated results\n#' for all recalibration methods performed in the simulation\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_locfit_simul_c_rf &lt;- function(simul, linspace = NULL) {\n  methods &lt;- names(simul$res_recalibration)\n  map(\n    .x = methods, \n    .f = ~calib_curve_method_locfit_simul_c_rf(\n      simul = simul, linspace = linspace, method = .x\n    )\n  ) |&gt; \n    list_rbind()\n}\n\n\n\n5.9.1.3 Functions for Moving Average\nFirst, we define a function to get the calibration curves for a single simulation, and for a single recalibration method.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using moving averages\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\n#' @param method name of the recalibration method to focus on\ncalib_curve_method_ma_simul_c_rf &lt;- function(simul,\n                                      linspace = NULL,\n                                      method\n) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Uncalibrated Scores estimated by the RF\n  scores_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_u\n  scores_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_u\n  # Recalibrated Scores of the RF, for the current method\n  scores_c_test &lt;- simul$res_recalibration[[method]]$tb_score_c_test$p_c\n  scores_c_calib &lt;- simul$res_recalibration[[method]]$tb_score_c_calib$p_c\n  \n  calib_ma_calib &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = tb_calib$d,\n      scores = scores_calib,\n      tau = .x, \n      nn = .15, prob = .5, method = \"probit\")\n  ) |&gt; \n    bind_rows() |&gt; \n    mutate(sample = \"calibration\", type = \"uncalibrated\")\n  \n  \n  calib_ma_test &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = tb_test$d,\n      scores = scores_test,\n      tau = .x, \n      nn = .15, prob = .5, method = \"probit\")\n  ) |&gt; \n    bind_rows() |&gt; \n    mutate(sample = \"test\", type = \"uncalibrated\")\n  \n  if (length(unique(scores_c_calib)) == 1) {\n    calib_c_ma_calib &lt;- tibble()\n  } else {\n    calib_c_ma_calib &lt;- map(\n      .x = linspace,\n      .f = ~local_ci_scores(\n        obs = tb_calib$d,\n        scores = scores_c_calib,\n        tau = .x, \n        nn = .15, prob = .5, method = \"probit\")\n    ) |&gt; bind_rows() |&gt; \n      mutate(sample = \"calibration\", type = \"recalibrated\")\n  }\n  \n  if (length(unique(scores_c_test)) == 1) {\n    calib_c_ma_test &lt;- tibble()\n  } else {\n    calib_c_ma_test &lt;- map(\n      .x = linspace,\n      .f = ~local_ci_scores(\n        obs = tb_test$d,\n        scores = scores_c_test,\n        tau = .x, \n        nn = .15, prob = .5, method = \"probit\")\n    ) |&gt; bind_rows() |&gt; \n      mutate(sample = \"test\", type = \"recalibrated\")\n  }\n  \n  calib_ma_calib |&gt; \n    bind_rows(calib_ma_test) |&gt; \n    bind_rows(calib_c_ma_calib) |&gt; \n    bind_rows(calib_c_ma_test) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed,\n      method = method\n    )\n}\n\nThen, we define a function that applies the first one to all recalibration methods for a single simulation.\n\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach, for recalibrated results\n#' for all recalibration methods performed in the simulation\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_ma_simul_c_rf &lt;- function(simul, linspace = NULL) {\n  methods &lt;- names(simul$res_recalibration)\n  map(\n    .x = methods, \n    .f = ~calib_curve_method_ma_simul_c_rf(\n      simul = simul, linspace = linspace, method = .x\n    )\n  ) |&gt; \n    list_rbind()\n}\n\n\n\n\n5.9.2 Quantile-based Bins\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf))\n  tb_calibration_curve_quant_c_rf &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf),\n    .f = ~{\n      p()\n      calib_curve_quant_simul_c_rf(simul = simul_recalib_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_quant_c_rf &lt;- list_rbind(tb_calibration_curve_quant_c_rf)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve. When the recalibrated scores are constant, it is not possible to define bins.\n\ntb_calibration_curve_quant_c_rf |&gt; \n  filter(score_class == 1) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   188       40      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   188       31      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf))\n  tb_calibration_curve_quant_c_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf),\n    .f = ~{\n      p()\n      calib_curve_quant_simul_c_rf(simul = simul_recalib_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_quant_c_rf_vote &lt;- list_rbind(tb_calibration_curve_quant_c_rf_vote)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve. When the recalibrated scores are constant, it is not possible to define bins.\n\ntb_calibration_curve_quant_c_rf_vote |&gt; \n  filter(score_class == 1) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   167       51      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   167       46      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\nNow, we can plot the calibration curve on each sample (calibration, test). Each line corresponds to the calibration curve of a single replicaiton of the simulations. The curves are shown in Figure 5.26 (regression), in Figure 5.27 (classification). The comparison between the two types of random forests are then made on a single grapg for each recalibration method from Figure 5.28 to Figure 5.33.\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      plot(\n        0:1, 0:1,\n        type = \"l\", col = NULL,\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Pred. prob.\", ylab = \"Mean pred. prob.\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      for (i_simul in unique(tb_calibration_curve_quant_c_rf$seed)) {\n        tb_current &lt;- tb_calibration_curve_quant_c_rf |&gt; \n          filter(seed == !!i_simul, sample == !!sample, type == !!type, method == !!method)\n        lines(\n          tb_current$mean_score, tb_current$mean_obs,\n          lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n        )\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n      }\n    }\n  }\n}\n\n\n\n\nFigure 5.26: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      plot(\n        0:1, 0:1,\n        type = \"l\", col = NULL,\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Pred. prob.\", ylab = \"Mean pred. prob.\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      for (i_simul in unique(tb_calibration_curve_quant_c_rf_vote$seed)) {\n        tb_current &lt;- tb_calibration_curve_quant_c_rf_vote |&gt; \n          filter(seed == !!i_simul, sample == !!sample, type == !!type, method == !!method)\n        lines(\n          tb_current$mean_score, tb_current$mean_obs,\n          lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n        )\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n      }\n    }\n  }\n}\n\n\n\n\nFigure 5.27: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay code to show the function used to create the Figures.\ntb_calibration_curve_quant_c_rf_both &lt;- \n  tb_calibration_curve_quant_c_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_quant_c_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\")))\n\nplot_quant_both &lt;- function(method) {\n  samples &lt;- c(\"calibration\", \"test\")\n  sample_labels &lt;- c(\"Calibration\", \"Test\")\n  \n  sample &lt;- \"train\"\n  colours_samples &lt;- c(\n    \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n  )\n  \n  types &lt;- c(\"uncalibrated\", \"recalibrated\")\n  types_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\n  methods_labels &lt;- c(\n    \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n    \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n  method_label &lt;- methods_labels[methods == method]\n  par(mfrow = c(2,4))\n  \n  for (model in c(\"Regression\", \"Classification\")) {\n    for(i_type in 1:2) {\n      type &lt;- types[i_type]\n      type_label &lt;- types_labels[i_type]\n      for (i_sample in 1:2) {\n        sample &lt;- samples[i_sample]\n        sample_label &lt;- sample_labels[i_sample]\n        \n        par(mar = c(4.1, 4.1, 3.1, 2.1))\n        plot(\n          0:1, 0:1,\n          type = \"l\", col = NULL,\n          xlim = 0:1, ylim = 0:1,\n          xlab = \"Pred. prob.\", ylab = \"Mean pred. prob.\",\n          main = str_c(model, \", \", type_label, \"\\n\", sample_label)\n        )\n        for (i_simul in unique(tb_calibration_curve_quant_c_rf_both$seed)) {\n          tb_current &lt;- tb_calibration_curve_quant_c_rf_both |&gt; \n            filter(\n              seed == !!i_simul, \n              sample == !!sample, \n              type == !!type, \n              method == !!method,\n              model == !!model)\n          lines(\n            tb_current$mean_score, tb_current$mean_obs,\n            lwd = 2, col = adjustcolor(colours_samples[sample_label], alpha.f = 0.1), t = \"b\",\n          )\n          segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        }\n      }\n    }\n  }\n}\n\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"platt\")\n\n\n\n\nFigure 5.28: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using Platt-scaling (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"isotonic\")\n\n\n\n\nFigure 5.29: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using Isotonic regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"beta\")\n\n\n\n\nFigure 5.30: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using Beta regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"locfit_0\")\n\n\n\n\nFigure 5.31: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using local regression with deg=0 (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"locfit_1\")\n\n\n\n\nFigure 5.32: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using local regression with deg=1 (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_quant_both(method = \"locfit_2\")\n\n\n\n\nFigure 5.33: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with quantile-based bins, using uncalibrated scores (left) or recalibrated scores using local regression with deg=2 (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.9.3 Local Regression\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf))\n  tb_calibration_curve_locfit_c_rf &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf),\n    .f = ~{\n      p()\n      calib_curve_locfit_simul_c_rf(simul = simul_recalib_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_locfit_c_rf &lt;- list_rbind(tb_calibration_curve_locfit_c_rf)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve.\n\ntb_calibration_curve_locfit_c_rf |&gt; \n  filter(xlim == 0, !is.na(locfit_pred)) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   188      200      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   188      200      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf_vote))\n  tb_calibration_curve_locfit_c_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf_vote),\n    .f = ~{\n      p()\n      calib_curve_locfit_simul_c_rf(simul = simul_recalib_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_locfit_c_rf_vote &lt;- list_rbind(tb_calibration_curve_locfit_c_rf_vote)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve. When the recalibrated scores are constant, it is not possible to define bins.\n\ntb_calibration_curve_locfit_c_rf_vote |&gt; \n  filter(xlim == 0, !is.na(locfit_pred)) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   167      200      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   167      200      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\nNewt, we can plot the calibration curves obtained with local regressions on each sample (calibration, test), contrasting between the two types of random forests (regression or classification), and also contrasting between the different recalibration techniques used (none, Platt-scaling, Isotonic regression, Beta regression, Local reggression with varying degrees.\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\n\ndf_plot &lt;- tb_calibration_curve_locfit_c_rf |&gt; \n  group_by(type, sample, method, xlim) |&gt; \n  summarise(\n    mean = mean(locfit_pred, na.rm = TRUE),\n    lower = quantile(locfit_pred, probs = .025, na.rm = TRUE),\n    upper = quantile(locfit_pred, probs = .975, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  \n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    \n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      df_plot_current &lt;- df_plot |&gt; \n        filter(sample == !!sample, type == !!type, method == !!method)\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      \n      plot(\n        df_plot_current$xlim,\n        df_plot_current$mean,\n        type = \"l\", col = colours_samples[sample_label],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      polygon(\n        c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n        c(df_plot_current$lower, rev(df_plot_current$upper)),\n        col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\n\n\n\nFigure 5.34: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with local regression, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\n\ndf_plot &lt;- tb_calibration_curve_locfit_c_rf_vote |&gt; \n  group_by(type, sample, method, xlim) |&gt; \n  summarise(\n    mean = mean(locfit_pred, na.rm = TRUE),\n    lower = quantile(locfit_pred, probs = .025, na.rm = TRUE),\n    upper = quantile(locfit_pred, probs = .975, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  \n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    \n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      df_plot_current &lt;- df_plot |&gt; \n        filter(sample == !!sample, type == !!type, method == !!method)\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      \n      plot(\n        df_plot_current$xlim,\n        df_plot_current$mean,\n        type = \"l\", col = colours_samples[sample_label],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      polygon(\n        c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n        c(df_plot_current$lower, rev(df_plot_current$upper)),\n        col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\n\n\n\nFigure 5.35: Calibration curve for the 200 replications of the estimation of the Random Forest (classification), obtained with local regression, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay code to show the function used to create the Figures.\ntb_calibration_curve_locfit_c_rf_vote_both &lt;- \n  tb_calibration_curve_locfit_c_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_locfit_c_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\n\nplot_locfit_both &lt;- function(method) {\n  \n  samples &lt;- c(\"calibration\", \"test\")\n  sample_labels &lt;- c(\"Calibration\", \"Test\")\n  \n  sample &lt;- \"train\"\n  colours_samples &lt;- c(\n    \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n  )\n  \n  types &lt;- c(\"uncalibrated\", \"recalibrated\")\n  types_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n  \n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\n  methods_labels &lt;- c(\n    \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n    \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n  method_label &lt;- methods_labels[methods == method]\n  \n  df_plot &lt;- tb_calibration_curve_locfit_c_rf_vote_both |&gt; \n    filter(method == !!method) |&gt; \n    group_by(type, sample, method, model, xlim) |&gt; \n    summarise(\n      mean = mean(locfit_pred, na.rm = TRUE),\n      lower = quantile(locfit_pred, probs = .025, na.rm = TRUE),\n      upper = quantile(locfit_pred, probs = .975, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n  \n  par(mfrow = c(2,4))\n  \n  for (model in c(\"Regression\", \"Classification\")) {\n    for(i_type in 1:2) {\n      type &lt;- types[i_type]\n      type_label &lt;- types_labels[i_type]\n      for (i_sample in 1:2) {\n        sample &lt;- samples[i_sample]\n        sample_label &lt;- sample_labels[i_sample]\n        \n        df_plot_current &lt;- df_plot |&gt; \n          filter(\n            sample == !!sample, \n            type == !!type, \n            method == !!method, \n            model == !!model\n          )\n        \n        plot(\n          df_plot_current$xlim,\n          df_plot_current$mean,\n          type = \"l\", col = colours_samples[sample_label],\n          xlim = 0:1, ylim = 0:1,\n          xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n          main = str_c(method_label, \", \", type_label, \"\\n\", model)\n        )\n        polygon(\n          c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n          c(df_plot_current$lower, rev(df_plot_current$upper)),\n          col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n          border = NA\n        )\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n      }\n    }\n  }\n  \n}\n\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"platt\")\n\n\n\n\nFigure 5.36: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Platt-scaling (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"isotonic\")\n\n\n\n\nFigure 5.37: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Isotonic regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"beta\")\n\n\n\n\nFigure 5.38: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Beta regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"locfit_0\")\n\n\n\n\nFigure 5.39: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Local regression (with deg=0) (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"locfit_1\")\n\n\n\n\nFigure 5.40: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Local regression (with deg=1) (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_locfit_both(method = \"locfit_2\")\n\n\n\n\nFigure 5.41: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with local regression, using uncalibrated scores (left) or recalibrated scores using Local regression (with deg=2) (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.9.4 Moving Average\n\nRegressionClassification\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf))\n  tb_calibration_curve_ma_c_rf &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf),\n    .f = ~{\n      p()\n      calib_curve_ma_simul_c_rf(simul = simul_recalib_rf[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_ma_c_rf &lt;- list_rbind(tb_calibration_curve_ma_c_rf)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve.\n\ntb_calibration_curve_ma_c_rf |&gt; \n  filter(xlim == 0, !is.na(mean)) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   188      199      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   188      198      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = length(simul_recalib_rf_vote))\n  tb_calibration_curve_ma_c_rf_vote &lt;- furrr::future_map(\n    .x = 1:length(simul_recalib_rf_vote),\n    .f = ~{\n      p()\n      calib_curve_ma_simul_c_rf(simul = simul_recalib_rf_vote[[.x]])\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ntb_calibration_curve_ma_c_rf_vote &lt;- list_rbind(tb_calibration_curve_ma_c_rf_vote)\n\nWe can have a look at how many simulations did not cause any error while computing the values for the calibration curve. When the recalibrated scores are constant, it is not possible to define bins.\n\ntb_calibration_curve_ma_c_rf_vote |&gt; \n  filter(xlim == 0, !is.na(mean)) |&gt; \n  count(sample, type, method) |&gt; \n  pivot_wider(names_from = method, values_from = n)\n\n# A tibble: 4 × 8\n  sample      type          beta isotonic locfit_0 locfit_1 locfit_2 platt\n  &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 calibration recalibrated   167      200      200      200      200   200\n2 calibration uncalibrated   200      200      200      200      200   200\n3 test        recalibrated   167      200      200      200      200   200\n4 test        uncalibrated   200      200      200      200      200   200\n\n\n\n\n\n\nRegressionClassificationBoth\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\ndf_plot &lt;- tb_calibration_curve_ma_c_rf |&gt; \n  group_by(sample, type, method, xlim) |&gt; \n  summarise(\n    lower = quantile(mean, probs = .025, na.rm = TRUE),\n    upper = quantile(mean, probs = .975, na.rm = TRUE),\n    mean = mean(mean, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  \n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    \n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      df_plot_current &lt;- df_plot |&gt; \n        filter(sample == !!sample, type == !!type, method == !!method)\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      \n      plot(\n        df_plot_current$xlim,\n        df_plot_current$mean,\n        type = \"l\", col = colours_samples[sample_label],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      \n      polygon(\n        c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n        c(df_plot_current$lower, rev(df_plot_current$upper)),\n        col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\n\n\n\nFigure 5.42: Calibration curve for the 200 replications of the estimation of the Random Forest (regression), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nsamples &lt;- c(\"calibration\", \"test\")\nsample_labels &lt;- c(\"Calibration\", \"Test\")\n\nsample &lt;- \"train\"\ncolours_samples &lt;- c(\n  \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\n\n\ntypes &lt;- c(\"uncalibrated\", \"recalibrated\")\ntypes_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n\nmethods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\nmethods_labels &lt;- c(\n  \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n  \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n\ndf_plot &lt;- tb_calibration_curve_ma_c_rf_vote |&gt; \n  group_by(sample, type, method, xlim) |&gt; \n  summarise(\n    lower = quantile(mean, probs = .025, na.rm = TRUE),\n    upper = quantile(mean, probs = .975, na.rm = TRUE),\n    mean = mean(mean, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\npar(mfrow = c(length(methods),4))\n\nfor (i_method in 1:length(methods)) {\n  method &lt;- methods[i_method]\n  method_label &lt;- methods_labels[i_method]\n  \n  for(i_type in 1:2) {\n    type &lt;- types[i_type]\n    type_label &lt;- types_labels[i_type]\n    \n    for (i_sample in 1:2) {\n      sample &lt;- samples[i_sample]\n      sample_label &lt;- sample_labels[i_sample]\n      \n      df_plot_current &lt;- df_plot |&gt; \n        filter(sample == !!sample, type == !!type, method == !!method)\n      \n      par(mar = c(4.1, 4.1, 3.1, 2.1))\n      \n      plot(\n        df_plot_current$xlim,\n        df_plot_current$mean,\n        type = \"l\", col = colours_samples[sample_label],\n        xlim = 0:1, ylim = 0:1,\n        xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n        main = str_c(method_label, \", \", type_label, \"\\n\", sample_label)\n      )\n      \n      polygon(\n        c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n        c(df_plot_current$lower, rev(df_plot_current$upper)),\n        col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n}\n\n\n\n\nFigure 5.43: Calibration curve for the 200 replications of the estimation of the Random Forest (classification), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay code to show the function used to create the Figures.\ntb_calibration_curve_ma_c_rf_vote_both &lt;- \n  tb_calibration_curve_ma_c_rf |&gt; mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    tb_calibration_curve_ma_c_rf_vote |&gt; mutate(model = \"Classification\")\n  ) |&gt; \n  mutate(model = factor(model, levels= c(\"Regression\", \"Classification\"))) |&gt; \n  mutate(lab_y = str_c(sample, \" (\", model, \")\")) |&gt; \n  arrange(model, sample)\n\nplot_ma_both &lt;- function(method) {\n  \n  samples &lt;- c(\"calibration\", \"test\")\n  sample_labels &lt;- c(\"Calibration\", \"Test\")\n  \n  sample &lt;- \"train\"\n  colours_samples &lt;- c(\n    \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n  )\n  \n  types &lt;- c(\"uncalibrated\", \"recalibrated\")\n  types_labels &lt;- c(\"Uncalib.\", \"Recalib.\")\n  \n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\")\n  methods_labels &lt;- c(\n    \"Platt Scaling\", \"Isotonic Reg.\", \"Beta Calib.\",\n    \"Local Reg. (deg = 0)\", \"Local Reg. (deg = 1)\", \"Local Reg (deg = 2)\")\n  method_label &lt;- methods_labels[methods == method]\n  \n  df_plot &lt;- tb_calibration_curve_ma_c_rf_vote_both |&gt; \n    filter(method == !!method) |&gt; \n    group_by(type, sample, method, model, xlim) |&gt; \n    summarise(\n      lower = quantile(mean, probs = .025, na.rm = TRUE),\n      upper = quantile(mean, probs = .975, na.rm = TRUE),\n      mean = mean(mean, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n  \n  par(mfrow = c(2,4))\n  \n  for (model in c(\"Regression\", \"Classification\")) {\n    for(i_type in 1:2) {\n      type &lt;- types[i_type]\n      type_label &lt;- types_labels[i_type]\n      for (i_sample in 1:2) {\n        sample &lt;- samples[i_sample]\n        sample_label &lt;- sample_labels[i_sample]\n        \n        df_plot_current &lt;- df_plot |&gt; \n          filter(\n            sample == !!sample, \n            type == !!type, \n            method == !!method, \n            model == !!model\n          )\n        \n        plot(\n          df_plot_current$xlim,\n          df_plot_current$mean,\n          type = \"l\", col = colours_samples[sample_label],\n          xlim = 0:1, ylim = 0:1,\n          xlab = \"Predicted probability\", ylab = \"Mean predicted probability\",\n          main = str_c(method_label, \", \", type_label, \"\\n\", model)\n        )\n        \n        polygon(\n          c(df_plot_current$xlim, rev(df_plot_current$xlim)),\n          c(df_plot_current$lower, rev(df_plot_current$upper)),\n          col = adjustcolor(col = colours_samples[sample_label], alpha.f = .4),\n          border = NA\n        )\n        segments(0, 0, 1, 1, col = \"black\", lty = 2)\n        \n      }\n    }\n  }\n}\n\n\n\nPlattIsotonicBetaLocfit (deg = 0)Locfit (deg = 1)Locfit (deg = 2)\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"platt\")\n\n\n\n\nFigure 5.44: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Platt-scaling (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"isotonic\")\n\n\n\n\nFigure 5.45: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Isotonic regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"beta\")\n\n\n\n\nFigure 5.46: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Beta regression (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"locfit_0\")\n\n\n\n\nFigure 5.47: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Local Regression (with deg=0) (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"locfit_1\")\n\n\n\n\nFigure 5.48: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Local Regression (with deg=1) (right), on the calibration set and on the test set.\n\n\n\n\n\n\n\n\n\nDisplay the R codes used to create the Figure.\nplot_ma_both(method = \"locfit_2\")\n\n\n\n\nFigure 5.49: Calibration curve for the 200 replications of the estimation of the Random Forest (regression on top, classification at the bottom), obtained with moving averages, using uncalibrated scores (left) or recalibrated scores using Local Regression (with deg=2) (right), on the calibration set and on the test set."
  },
  {
    "objectID": "rf-grid-search.html#data",
    "href": "rf-grid-search.html#data",
    "title": "6  Grid Search",
    "section": "6.1 Data",
    "text": "6.1 Data\nWe use data obtained from UCI Yeh (2016), presenting research customers’ default payments in Taiwan. This dataset contains \\(n = 30,000\\) instances and 23 numeric features. The outcome variable, corresponding to the observed default payment in next month, is positive in 22.12% of cases.\nLet us load the data.\n\ndata_credit &lt;- read.csv2(\n  \"data/default_credit.csv\", \n  header = TRUE, skip = 1\n)\ncolnames(data_credit)[25] &lt;- \"d\"\ndata_credit &lt;- data_credit |&gt; select(-ID)\ndata_credit &lt;- data_credit |&gt; as_tibble( )\ndata_credit &lt;- data_credit |&gt; \n  mutate(\n    across(\n      c(\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \n        \"PAY_4\", \"PAY_5\", \"PAY_6\"), \n      ~as.factor(.x)\n    )\n  )\n\nFollowing the methodology outlined in Subasi A. (2019), we employ the Synthetic Minority Over-sampling Technique (SMOTE) at a rate of 200% to rebalance the data. To do so, we use the smote() function from {performanceEstimation}.\n\nlibrary(performanceEstimation)\n\n\n# need of a factor variable to apply SMOTE function\nset.seed(123)\ndata_credit$d &lt;- as.factor(data_credit$d)\nnew_df &lt;- performanceEstimation::smote(d ~ ., data_credit, perc.over = 2)\n\ndata_credit &lt;- new_df\ndata_credit$d &lt;- as.numeric(data_credit$d) # change in the labels\ndata_credit &lt;- data_credit |&gt; mutate(d = ifelse(d == 1, 1, 2))\ndata_credit &lt;- data_credit |&gt; mutate(d = d-1)\n\nLet us save this dataset for later use in the simulations.\n\nwrite.csv(\n  data_credit, \n  file = \"data/data_credit_smote.csv\",\n  row.names = FALSE\n)\n\nWe split the data into two sets:\n\na training set on which we will train random forests (with 50% of observations)\nthe remaining data which will be further split into a calibration set and a test set.\n\n\nset.seed(1234)\nind_train &lt;- sample(1:nrow(data_credit), size = .5*nrow(data_credit))\ntb_train &lt;- data_credit |&gt; slice(ind_train)\ntb_rest &lt;- data_credit |&gt; slice(-ind_train)\n\nLet us save those files for later use in the smimulations.\n\nwrite.csv(tb_train, \"data/data_credit_smote_train.csv\", row.names=FALSE)\nwrite.csv(tb_rest, \"data/data_credit_smote_rest.csv\", row.names=FALSE)"
  },
  {
    "objectID": "rf-grid-search.html#grid-with-hyperparameters",
    "href": "rf-grid-search.html#grid-with-hyperparameters",
    "title": "6  Grid Search",
    "section": "6.2 Grid With Hyperparameters",
    "text": "6.2 Grid With Hyperparameters\nWe conduct a grid search to find the set of hyperparameters that optimize a criterion:\n\nthe out-of-bag mean squared error (MSE) for the regressor\nthe error rate for the classifier.\n\n\ngrid_params &lt;- \n  expand_grid(\n    num_trees = c(100,300, 500),\n    mtry = seq(1,(ncol(tb_train)/2)),\n    nodesize = c(5, 10, 15, 20),\n    splitrule = \"gini\"\n  )\ngrid_params\n\n# A tibble: 144 × 4\n   num_trees  mtry nodesize splitrule\n       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;    \n 1       100     1        5 gini     \n 2       100     1       10 gini     \n 3       100     1       15 gini     \n 4       100     1       20 gini     \n 5       100     2        5 gini     \n 6       100     2       10 gini     \n 7       100     2       15 gini     \n 8       100     2       20 gini     \n 9       100     3        5 gini     \n10       100     3       10 gini     \n# ℹ 134 more rows"
  },
  {
    "objectID": "rf-grid-search.html#helper-functions",
    "href": "rf-grid-search.html#helper-functions",
    "title": "6  Grid Search",
    "section": "6.3 Helper Functions",
    "text": "6.3 Helper Functions\nLet us define some helper functions to compute the MSE, the accuracy (with a probability threshold of .5) and the AUC (using {pROC}).\n\n#' Mean Squared Error\nmse_function &lt;- function(pred, obs) mean((pred - obs)^2)\n\n#' Accuracy with threshold 0.5\naccuracy_function &lt;- function(pred, obs, threshold = 0.5) {\n  mean((as.numeric(as.character(pred)) &gt; 0.5) == obs)\n}\n\n#' AUC (no threshold)\nauc_function &lt;- function(pred, obs){\n  auc(obs, pred)\n}"
  },
  {
    "objectID": "rf-grid-search.html#estimations",
    "href": "rf-grid-search.html#estimations",
    "title": "6  Grid Search",
    "section": "6.4 Estimations",
    "text": "6.4 Estimations\nLet us now go through the hyperparameter grid.\n\n6.4.1 Regression\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  mse_oob_rf_reg &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        d ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      out_of_bag &lt;- map(.x = 1:nrow(tb_train), .f = ~which(rf[[\"inbag\"]][.x,] == 0))\n      rf_pred_all &lt;- predict(rf, tb_train,\n                             predict.all = TRUE,\n                             type = \"response\")$individual\n      rf_pred &lt;- unlist(map(.x = 1:nrow(tb_train), .f = ~mean(rf_pred_all[.x,out_of_bag[[.x]]])))\n      \n      oob_err &lt;- mse_function(pred = rf_pred, obs = tb_train |&gt; pull(d))\n      mse_oob &lt;- oob_err\n      \n      # Progress bar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        mse_oob = mse_oob\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nLet us order the computed out-of-bag MSE by ascending values:\n\nbest_params_rf_reg &lt;- \n  mse_oob_rf_reg |&gt; list_rbind() |&gt; \n  arrange(mse_oob)\n\nWe save the results for later use.\n\nwrite.csv(\n  best_params_rf_reg, \n  file = \"output/best_params_rf_reg.csv\",\n  row.names = FALSE\n)\n\n\nbest_params_rf_reg\n\n    mtry nodesize num_trees splitrule   mse_oob\n1     12        5       500      gini 0.1161355\n2     11        5       500      gini 0.1167286\n3     10        5       500      gini 0.1167594\n4     11        5       300      gini 0.1168635\n5     10        5       300      gini 0.1170744\n6      9        5       300      gini 0.1172085\n7      9        5       500      gini 0.1172791\n8     12        5       300      gini 0.1173561\n9      8        5       500      gini 0.1175019\n10     8        5       300      gini 0.1181500\n11     7        5       500      gini 0.1182748\n12     6        5       500      gini 0.1184314\n13     7        5       300      gini 0.1187873\n14     6        5       300      gini 0.1194305\n15     5        5       500      gini 0.1195314\n16    11        5       100      gini 0.1195434\n17    12        5       100      gini 0.1198150\n18     9        5       100      gini 0.1198567\n19    10        5       100      gini 0.1201343\n20     4        5       500      gini 0.1206271\n21     7        5       100      gini 0.1206874\n22     5        5       300      gini 0.1206969\n23    12       10       500      gini 0.1208994\n24    11       10       500      gini 0.1210198\n25    11       10       300      gini 0.1211552\n26     8        5       100      gini 0.1211788\n27    12       10       300      gini 0.1212391\n28    10       10       500      gini 0.1213147\n29     4        5       300      gini 0.1214632\n30     9       10       500      gini 0.1217760\n31    10       10       300      gini 0.1220816\n32     9       10       300      gini 0.1222290\n33     6        5       100      gini 0.1222443\n34     8       10       300      gini 0.1222482\n35     8       10       500      gini 0.1225779\n36     7       10       500      gini 0.1226872\n37     3        5       500      gini 0.1227032\n38     5        5       100      gini 0.1230636\n39     3        5       300      gini 0.1230863\n40    11       10       100      gini 0.1232123\n41    12       10       100      gini 0.1234719\n42     4        5       100      gini 0.1234766\n43     7       10       300      gini 0.1236268\n44     6       10       500      gini 0.1236749\n45    10       10       100      gini 0.1238468\n46     6       10       300      gini 0.1241986\n47    12       15       500      gini 0.1243726\n48     8       10       100      gini 0.1245897\n49     5       10       500      gini 0.1248403\n50    11       15       500      gini 0.1248550\n51     9       10       100      gini 0.1248996\n52    11       15       300      gini 0.1250312\n53    12       15       300      gini 0.1251028\n54    10       15       500      gini 0.1252682\n55     5       10       300      gini 0.1253665\n56     7       10       100      gini 0.1255997\n57     3        5       100      gini 0.1257361\n58     9       15       500      gini 0.1258641\n59     6       10       100      gini 0.1259584\n60    10       15       300      gini 0.1259758\n61     4       10       500      gini 0.1263621\n62     9       15       300      gini 0.1264203\n63    12       15       100      gini 0.1265025\n64     8       15       500      gini 0.1265721\n65     2        5       500      gini 0.1266220\n66     8       15       300      gini 0.1267140\n67     4       10       300      gini 0.1267876\n68    10       15       100      gini 0.1268396\n69    11       15       100      gini 0.1269556\n70     2        5       300      gini 0.1271676\n71     7       15       500      gini 0.1273421\n72     7       15       300      gini 0.1276667\n73     5       10       100      gini 0.1279446\n74    12       20       500      gini 0.1280429\n75     9       15       100      gini 0.1280750\n76    12       20       300      gini 0.1281329\n77     6       15       500      gini 0.1281506\n78    11       20       300      gini 0.1281588\n79     3       10       500      gini 0.1281836\n80    10       20       500      gini 0.1282266\n81    11       20       500      gini 0.1282600\n82     6       15       300      gini 0.1282876\n83     3       10       300      gini 0.1285083\n84     8       15       100      gini 0.1289850\n85     5       15       500      gini 0.1290990\n86     9       20       500      gini 0.1291231\n87     5       15       300      gini 0.1291516\n88    10       20       300      gini 0.1291917\n89     4       10       100      gini 0.1292167\n90     2        5       100      gini 0.1296366\n91     8       20       500      gini 0.1298449\n92     9       20       300      gini 0.1299142\n93     7       15       100      gini 0.1302080\n94     6       15       100      gini 0.1302180\n95     8       20       300      gini 0.1302658\n96    11       20       100      gini 0.1303368\n97    12       20       100      gini 0.1303956\n98     7       20       500      gini 0.1305237\n99     4       15       500      gini 0.1306045\n100    7       20       300      gini 0.1307034\n101    4       15       300      gini 0.1314075\n102    3       10       100      gini 0.1314194\n103   10       20       100      gini 0.1314446\n104    9       20       100      gini 0.1314797\n105    6       20       500      gini 0.1316348\n106    6       20       300      gini 0.1318650\n107    5       15       100      gini 0.1322313\n108    2       10       500      gini 0.1323110\n109    8       20       100      gini 0.1323435\n110    7       20       100      gini 0.1327727\n111    2       10       300      gini 0.1328086\n112    5       20       500      gini 0.1329164\n113    4       15       100      gini 0.1329842\n114    3       15       500      gini 0.1330115\n115    5       20       300      gini 0.1330483\n116    3       15       300      gini 0.1333896\n117    6       20       100      gini 0.1334507\n118    4       20       500      gini 0.1343606\n119    5       20       100      gini 0.1346723\n120    4       20       300      gini 0.1348001\n121    3       15       100      gini 0.1352052\n122    2       10       100      gini 0.1355645\n123    3       20       500      gini 0.1366326\n124    4       20       100      gini 0.1369212\n125    3       20       300      gini 0.1369492\n126    2       15       500      gini 0.1370411\n127    2       15       300      gini 0.1371923\n128    3       20       100      gini 0.1389632\n129    2       15       100      gini 0.1396311\n130    2       20       500      gini 0.1408191\n131    2       20       300      gini 0.1411691\n132    2       20       100      gini 0.1429509\n133    1        5       300      gini 0.1517564\n134    1        5       500      gini 0.1517581\n135    1        5       100      gini 0.1533601\n136    1       10       500      gini 0.1552974\n137    1       10       300      gini 0.1556152\n138    1       10       100      gini 0.1567145\n139    1       15       300      gini 0.1579995\n140    1       15       500      gini 0.1585141\n141    1       20       500      gini 0.1600574\n142    1       20       300      gini 0.1609307\n143    1       15       100      gini 0.1613020\n144    1       20       100      gini 0.1623087\n\n\n\n\n6.4.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  mse_oob_rf_classif &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        as.factor(d) ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      err_oob &lt;- rf$err.rate[num_trees,1]\n      \n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        err_oob = err_oob\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nLet us order the computed out-of-bag error rate by ascending values:\n\nbest_params_rf_classif &lt;- \n  mse_oob_rf_classif |&gt; list_rbind() |&gt; \n  arrange(err_oob)\n\nWe save the results for later use.\n\nwrite.csv(\n  best_params_rf_classif,\n  file = \"output/best_params_rf_classif.csv\", \n  row.names = FALSE\n  )\n\n\nbest_params_rf_classif\n\n    mtry nodesize num_trees splitrule   err_oob\n1     12        5       300      gini 0.1567209\n2     11        5       500      gini 0.1580126\n3     10        5       500      gini 0.1580556\n4      9        5       300      gini 0.1585723\n5     10        5       300      gini 0.1586584\n6     11        5       300      gini 0.1591751\n7      9        5       500      gini 0.1595195\n8     12        5       500      gini 0.1596056\n9      7        5       500      gini 0.1596917\n10     8        5       500      gini 0.1600792\n11     6        5       300      gini 0.1614570\n12     6        5       500      gini 0.1621459\n13     8        5       300      gini 0.1627056\n14    12        5       100      gini 0.1631361\n15     7        5       300      gini 0.1634375\n16     5        5       500      gini 0.1643417\n17    10        5       100      gini 0.1652458\n18     9        5       100      gini 0.1653320\n19    11       10       300      gini 0.1661069\n20     5        5       300      gini 0.1664944\n21     6        5       100      gini 0.1665375\n22     4        5       500      gini 0.1666236\n23    12       10       500      gini 0.1672264\n24    11        5       100      gini 0.1673125\n25     4        5       300      gini 0.1673555\n26    10       10       500      gini 0.1677000\n27     7        5       100      gini 0.1677430\n28    11       10       500      gini 0.1677861\n29    12       10       300      gini 0.1682167\n30     8        5       100      gini 0.1685180\n31     9       10       300      gini 0.1692930\n32     8       10       500      gini 0.1692930\n33     9       10       500      gini 0.1693791\n34     3        5       500      gini 0.1698958\n35     5        5       100      gini 0.1706708\n36     8       10       300      gini 0.1712736\n37     3        5       300      gini 0.1715319\n38    10       10       300      gini 0.1717902\n39     7       10       500      gini 0.1721777\n40     7       10       300      gini 0.1730388\n41    11       10       100      gini 0.1731680\n42     6       10       300      gini 0.1731680\n43     6       10       500      gini 0.1732972\n44    12       10       100      gini 0.1736416\n45     4        5       100      gini 0.1742444\n46    10       10       100      gini 0.1749333\n47    11       15       500      gini 0.1754930\n48    11       15       300      gini 0.1758805\n49    12       15       300      gini 0.1760958\n50     9       15       500      gini 0.1762249\n51    10       15       500      gini 0.1762680\n52     9       10       100      gini 0.1763110\n53    12       15       500      gini 0.1763541\n54     8       10       100      gini 0.1766124\n55     5       10       500      gini 0.1766555\n56     5       10       300      gini 0.1770860\n57    10       15       300      gini 0.1772152\n58     2        5       500      gini 0.1777749\n59     3        5       100      gini 0.1778180\n60     4       10       500      gini 0.1778180\n61     7       10       100      gini 0.1779041\n62     6       10       100      gini 0.1781624\n63     9       15       300      gini 0.1785499\n64     2        5       300      gini 0.1794110\n65     8       15       300      gini 0.1799707\n66     8       15       500      gini 0.1800138\n67     7       15       500      gini 0.1801429\n68     4       10       300      gini 0.1804443\n69     7       15       300      gini 0.1809610\n70    12       15       100      gini 0.1820804\n71    12       20       500      gini 0.1821235\n72     5       10       100      gini 0.1822957\n73    11       15       100      gini 0.1823818\n74     4       10       100      gini 0.1826401\n75    10       15       100      gini 0.1827693\n76     6       15       300      gini 0.1829846\n77     3       10       500      gini 0.1831568\n78     2        5       100      gini 0.1833290\n79    12       20       300      gini 0.1835443\n80    11       20       300      gini 0.1835874\n81     6       15       500      gini 0.1838887\n82    11       20       500      gini 0.1838887\n83     5       15       500      gini 0.1840610\n84     8       15       100      gini 0.1841471\n85     9       20       300      gini 0.1843193\n86     7       15       100      gini 0.1844054\n87     9       20       500      gini 0.1844915\n88     3       10       300      gini 0.1850512\n89    10       20       300      gini 0.1851373\n90     9       15       100      gini 0.1852665\n91    10       20       500      gini 0.1855679\n92     4       15       500      gini 0.1862998\n93    12       20       100      gini 0.1864721\n94     8       20       500      gini 0.1865582\n95    11       20       100      gini 0.1867304\n96     8       20       300      gini 0.1873332\n97     5       15       300      gini 0.1876345\n98     6       15       100      gini 0.1877637\n99     7       20       500      gini 0.1881512\n100    7       20       300      gini 0.1883665\n101    9       20       100      gini 0.1884095\n102    3       10       100      gini 0.1885818\n103    2       10       500      gini 0.1897443\n104    4       15       300      gini 0.1900887\n105   10       20       100      gini 0.1901317\n106    8       20       100      gini 0.1905192\n107    6       20       500      gini 0.1905192\n108    5       15       100      gini 0.1906484\n109    4       15       100      gini 0.1915095\n110    7       20       100      gini 0.1919401\n111    6       20       300      gini 0.1921553\n112    3       15       500      gini 0.1928012\n113    2       10       300      gini 0.1929303\n114    3       15       300      gini 0.1930164\n115    5       20       500      gini 0.1933178\n116    5       20       300      gini 0.1940067\n117    6       20       100      gini 0.1955136\n118    4       20       500      gini 0.1959442\n119    3       15       100      gini 0.1970206\n120    4       20       300      gini 0.1973650\n121    2       10       100      gini 0.1974942\n122    5       20       100      gini 0.1983983\n123    4       20       100      gini 0.1990011\n124    2       15       300      gini 0.2003789\n125    3       20       500      gini 0.2010678\n126    2       15       500      gini 0.2013692\n127    3       20       300      gini 0.2017567\n128    3       20       100      gini 0.2037802\n129    2       15       100      gini 0.2047275\n130    2       20       300      gini 0.2083441\n131    2       20       500      gini 0.2083441\n132    2       20       100      gini 0.2106260\n133    1        5       500      gini 0.2295703\n134    1        5       100      gini 0.2300870\n135    1        5       300      gini 0.2349092\n136    1       10       300      gini 0.2349522\n137    1       10       500      gini 0.2386980\n138    1       10       100      gini 0.2398174\n139    1       15       300      gini 0.2410230\n140    1       15       500      gini 0.2412813\n141    1       15       100      gini 0.2456730\n142    1       20       500      gini 0.2462757\n143    1       20       100      gini 0.2473952\n144    1       20       300      gini 0.2490743\n\n\n\n\n\n\nSubasi A., Cankur S. 2019. “Prediction of Default Payment of Credit Card Clients Using Data Mining Techniques.” Fifth International Engineering Conference on Developments in Civil & Computer Engineering Applications 2019 - (IEC2019) - Erbil - IRAQ. https://doi.org/10.1109/IEC47844.2019.8950597.\n\n\nYeh, I-Cheng. 2016. “Default of credit card clients.” UCI Machine Learning Repository."
  },
  {
    "objectID": "rf-simulations.html#helper-functions",
    "href": "rf-simulations.html#helper-functions",
    "title": "7  Simulations",
    "section": "7.1 Helper Functions",
    "text": "7.1 Helper Functions\nBefore running the simulations, we need some helper functions.\n\n7.1.1 Calibration/Test Splits\nWe define get_samples() which is used to create a partition of the data to create a calibration and a test set.\n\n#' Get calibration/test samples from the DGP\n#'\n#' @param seed seed to use to generate the data\n#' @param n_obs number of desired observations\nget_samples &lt;- function(seed,\n                        data) {\n  set.seed(seed)\n  n_obs &lt;- nrow(data)\n  \n  # Calibration/test sets----\n  calib_index &lt;- sample(1:nrow(data), size = .5 * nrow(data), replace = FALSE)\n  tb_calib &lt;- data |&gt; slice(calib_index)\n  tb_test &lt;- data |&gt; slice(-calib_index)\n  \n  list(\n    data = data,\n    tb_calib = tb_calib,\n    tb_test = tb_test,\n    calib_index = calib_index,\n    seed = seed,\n    n_obs = n_obs\n  )\n}\n\n\n\n7.1.2 Standard Metrics For Classification / Regression\nWe define the compute_gof() function wich computes multiple goodness-of-fit criteria.\n\n\nDisplay the codes used to define the standard metrics\n#' Computes goodness of fit metrics\n#' \n#' @param true_prob true probabilities. If `NULL` (default), True MSE is not \n#'   computed and the `NA` value is returned for this metric\n#' @param obs observed values (binary outcome)\n#' @param pred predicted scores\n#' @param threshold classification threshold (default to `.5`)\n#' @returns tibble with MSE, accuracy, missclassification rate, sensititity \n#'  (TPR), specificity (TNR), FPR, and used threshold\ncompute_gof &lt;- function(true_prob = NULL,\n                        obs, \n                        pred, \n                        threshold = .5) {\n  \n  # MSE\n  if (!is.null(true_prob)) {\n    mse &lt;- mean((true_prob - pred)^2)\n  } else {\n    mse = NA\n  }\n  \n  pred_class &lt;- as.numeric(pred &gt; threshold)\n  confusion_tb &lt;- tibble(\n    obs = obs,\n    pred = pred_class\n  ) |&gt; \n    count(obs, pred)\n  \n  TN &lt;- confusion_tb |&gt; filter(obs == 0, pred == 0) |&gt; pull(n)\n  TP &lt;- confusion_tb |&gt; filter(obs == 1, pred == 1) |&gt; pull(n)\n  FP &lt;- confusion_tb |&gt; filter(obs == 0, pred == 1) |&gt; pull(n)\n  FN &lt;- confusion_tb |&gt; filter(obs == 1, pred == 0) |&gt; pull(n)\n  \n  if (length(TN) == 0) TN &lt;- 0\n  if (length(TP) == 0) TP &lt;- 0\n  if (length(FP) == 0) FP &lt;- 0\n  if (length(FN) == 0) FN &lt;- 0\n  \n  n_pos &lt;- sum(obs == 1)\n  n_neg &lt;- sum(obs == 0)\n  \n  # Accuracy\n  acc &lt;- (TP + TN) / (n_pos + n_neg)\n  # Missclassification rate\n  missclass_rate &lt;- 1 - acc\n  # Sensitivity (True positive rate)\n  # proportion of actual positives that are correctly identified as such\n  TPR &lt;- TP / n_pos\n  # Specificity (True negative rate)\n  # proportion of actual negatives that are correctly identified as such\n  TNR &lt;- TN / n_neg\n  # False positive Rate\n  FPR &lt;- FP / n_neg\n  # AUC\n  AUC &lt;- as.numeric(pROC::auc(obs, pred, levels = c(\"0\", \"1\"), direction = \"&gt;\"))\n  \n  # roc &lt;- pROC::roc(obs, pred, levels = c(\"0\", \"1\"), direction = \"&gt;\")\n  # AUC &lt;- as.numeric(pROC::auc(roc))\n  \n  tibble(\n    mse = mse,\n    accuracy = acc,\n    missclass_rate = missclass_rate,\n    sensitivity = TPR,\n    specificity = TNR,\n    threshold = threshold,\n    FPR = FPR,\n    AUC = AUC\n  )\n}\n\n\nWe define functions to estimate calibration, as in Section 1.2 from Chapter 1 (brier_score(), get_summary_bins(), e_calib_error(), qmse_error(), local_ci_scores(), weighted_mse(), local_calib_score(), and a wrapper function, compute_metrics()).\n\n\nDisplay the codes used to define calibration metrics functions.\n## Brier Score----\n\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n\n## Expected Calibration Error (ECE)----\n\n\n#' Computes summary statistics for binomial observed data and predicted scores\n#' returned by a model\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\n#' @return a tibble where each row correspond to a bin, and each columns are:\n#' - `score_class`: level of the decile that the bin represents\n#' - `nb`: number of observation\n#' - `mean_obs`: average of obs (proportion of positive events)\n#' - `mean_score`: average predicted score (confidence)\n#' - `sum_obs`: number of positive events (number of positive events)\n#' - `accuracy`: accuracy (share of correctly predicted, using the\n#'    threshold)\nget_summary_bins &lt;- function(obs,\n                             scores,\n                             k = 10, \n                             threshold = .5) {\n  breaks &lt;- quantile(scores, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- tibble(\n    obs = obs,\n    score = scores,\n  ) |&gt;\n    mutate(\n      score_class = cut(\n        score,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      ),\n      pred_class = ifelse(score &gt; threshold, 1, 0),\n      correct_pred = obs == pred_class\n    )\n  \n  x_with_class |&gt;\n    group_by(score_class) |&gt;\n    summarise(\n      nb = n(),\n      mean_obs = mean(obs),\n      mean_score = mean(score), # confidence\n      sum_obs = sum(obs),\n      accuracy = mean(correct_pred)\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(\n      score_class = as.character(score_class) |&gt; as.numeric()\n    ) |&gt;\n    arrange(score_class)\n}\n\n#' Expected Calibration Error\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\ne_calib_error &lt;- function(obs,\n                          scores, \n                          k = 10, \n                          threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(ece_bin = nb * abs(accuracy - mean_score)) |&gt;\n    summarise(ece = 1 / sum(nb) * sum(ece_bin)) |&gt;\n    pull(ece)\n}\n\n## Quantile-based Mean Squared Error----\n\n#' Quantile-Based MSE\n#'\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param k number of classes to create (quantiles, default to `10`)\n#' @param threshold classification threshold (default to `.5`)\nqmse_error &lt;- function(obs,\n                       scores, \n                       k = 10, \n                       threshold = .5) {\n  summary_bins &lt;- get_summary_bins(\n    obs = obs, scores = scores, k = k, threshold = .5\n  )\n  summary_bins |&gt;\n    mutate(qmse_bin = nb * (mean_obs - mean_score)^2) |&gt;\n    summarise(qmse = 1/sum(nb) * sum(qmse_bin)) |&gt;\n    pull(qmse)\n}\n\n## Weighted Mean Squared Error----\n\n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\n#' @param tau value at which to compute the confidence interval\n#' @param nn fraction of nearest neighbors\n#' @prob level of the confidence interval (default to `.95`)\n#' @param method Which method to use to construct the interval. Any combination\n#'  of c(\"exact\", \"ac\", \"asymptotic\", \"wilson\", \"prop.test\", \"bayes\", \"logit\",\n#'  \"cloglog\", \"probit\") is allowed. Default is \"all\".\n#' @return a tibble with a single row that corresponds to estimations made in\n#'   the neighborhood of a probability $p=\\tau$`, using the fraction `nn` of\n#'   neighbors, where the columns are:\n#'  - `score`: score tau in the neighborhood of which statistics are computed\n#'  - `mean`: estimation of $E(d | s(x) = \\tau)$\n#'  - `lower`: lower bound of the confidence interval\n#'  - `upper`: upper bound of the confidence interval\n#' @importFrom binom binom.confint\nlocal_ci_scores &lt;- function(obs,\n                            scores,\n                            tau,\n                            nn,\n                            prob = .95,\n                            method = \"probit\") {\n  \n  # Identify the k nearest neighbors based on hat{p}\n  k &lt;- round(length(scores) * nn)\n  rgs &lt;- rank(abs(scores - tau), ties.method = \"first\")\n  idx &lt;- which(rgs &lt;= k)\n  \n  binom.confint(\n    x = sum(obs[idx]),\n    n = length(idx),\n    conf.level = prob,\n    methods = method\n  )[, c(\"mean\", \"lower\", \"upper\")] |&gt;\n    tibble() |&gt;\n    mutate(xlim = tau) |&gt;\n    relocate(xlim, .before = mean)\n}\n\n#' Compute the Weighted Mean Squared Error to assess the calibration of a model\n#'\n#' @param local_scores tibble with expected scores obtained with the \n#'   `local_ci_scores()` function\n#' @param scores vector of raw predicted probabilities\nweighted_mse &lt;- function(local_scores, scores) {\n  # To account for border bias (support is [0,1])\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(local_scores$xlim)\n  )\n  # The weights\n  weights &lt;- dens$y\n  local_scores |&gt;\n    mutate(\n      wmse_p = (xlim - mean)^2,\n      weight = !!weights\n    ) |&gt;\n    summarise(wmse = sum(weight * wmse_p) / sum(weight)) |&gt;\n    pull(wmse)\n}\n\n## Local regression score\n#' Calibration score using Local Regression\n#' \n#' @param obs vector of observed events\n#' @param scores vector of predicted probabilities\nlocal_calib_score &lt;- function(obs, \n                              scores) {\n  \n  # Add a little noise to the scores, to avoir crashing R\n  scores &lt;- scores + rnorm(length(scores), 0, .001)\n  locfit_0 &lt;- locfit(\n    formula = d ~ lp(scores, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(\n      d = obs,\n      scores = scores\n    )\n  )\n  # Predictions on [0,1]\n  linspace_raw &lt;- seq(0, 1, length.out = 100)\n  # Restricting this space to the range of observed scores\n  keep_linspace &lt;- which(linspace_raw &gt;= min(scores) & linspace_raw &lt;= max(scores))\n  linspace &lt;- linspace_raw[keep_linspace]\n  \n  locfit_0_linspace &lt;- predict(locfit_0, newdata = linspace)\n  locfit_0_linspace[locfit_0_linspace &gt; 1] &lt;- 1\n  locfit_0_linspace[locfit_0_linspace &lt; 0] &lt;- 0\n  \n  # Squared difference between predicted value and the bissector, weighted by the density of values\n  scores_reflected &lt;- c(-scores, scores, 2 - scores)\n  dens &lt;- density(\n    x = scores_reflected, from = 0, to = 1, \n    n = length(linspace_raw)\n  )\n  # The weights\n  weights &lt;- dens$y[keep_linspace]\n  \n  weighted.mean((linspace - locfit_0_linspace)^2, weights)\n}\n\n## Wrapper----\n\n#' Computes the calibration metrics for a set of observed and predicted \n#' probabilities\n#'  - mse: Mean Squared Error based on true proba\n#'  - brier: Brier score\n#'  - ece: Expectec Calibration Error\n#'  - qmse: MSE on bins defined by the quantiles of the predicted scores\n#'  - wmse: MSE weighted by the density of the predicted scores\n#' \n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#' @param linspace vector of values at which to compute the WMSE\n#' @param k number of classes (bins) to create (default to `10`)\ncompute_metrics &lt;- function(obs, \n                            scores, \n                            true_probas = NULL,\n                            linspace = NULL,\n                            k = 10) {\n  \n  ## True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n  \n  \n  brier &lt;- brier_score(obs = obs, scores = scores)\n  if (length(unique(scores)) &gt; 1) {\n    ece &lt;- e_calib_error(obs = obs, scores = scores, k = k, threshold = .5)\n    qmse &lt;- qmse_error(obs = obs, scores = scores, k = k, threshold = .5)\n  } else {\n    ece &lt;- NA\n    qmse &lt;- NA\n  }\n  \n  locfit_score &lt;- local_calib_score(obs = obs, scores = scores)\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  expected_events &lt;- map(\n    .x = linspace,\n    .f = ~local_ci_scores(\n      obs = obs, \n      scores = scores,\n      tau = .x, nn = .15, prob = .95, method = \"probit\")\n  ) |&gt; \n    bind_rows()\n  wmse &lt;- weighted_mse(local_scores = expected_events, scores = scores)\n  tibble(\n    mse = mse, \n    locfit_score = locfit_score,\n    brier = brier, \n    ece = ece, \n    qmse = qmse, \n    wmse = wmse\n  )\n}\n\n\n\n\n7.1.3 Functions to Train Random Forests\nWe define two functions to train a random forest on the train set, apply_rf() (for a regressor) and apply_rf_vote() (for a classifier). Both functions return the estimated scores on the train set, the calibration set, and the test set.\n\n#' Apply Random Forest algorithm\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf &lt;- function(train_data, \n                     calib_data, \n                     test_data,\n                     mtry = max(floor(ncol(train_data)/3), 1),\n                     nodesize = 1,\n                     ntree = 500,\n                     splitrule = \"gini\") {\n  \n  rf &lt;- randomForest(\n    d ~ ., \n    data = train_data, \n    mtry = mtry, \n    nodesize = nodesize, \n    ntree = ntree,\n    splitrule =  splitrule\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"response\")\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"response\")\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"response\")\n  \n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n#' Apply Random Forest algorithm (classification task)\n#' \n#' @param train_data train dataset\n#' @param calib_data calibration dataset\n#' @param test_data test dataset\napply_rf_vote &lt;- function(train_data, \n                          calib_data, \n                          test_data,\n                          mtry = 1,\n                          nodesize = 0.1 * nrow(train_data),\n                          ntree = 500,\n                          splitrule = \"gini\") {\n  rf &lt;- randomForest(\n    d ~ ., \n    data = train_data |&gt; mutate(d = factor(d)), \n    mtry = mtry, \n    nodesize = nodesize, \n    ntree = ntree,\n    splitrule =  splitrule,\n  )\n  \n  scores_train &lt;- predict(rf, newdata = train_data, type = \"vote\")[, \"1\"]\n  scores_calib &lt;- predict(rf, newdata = calib_data, type = \"vote\")[, \"1\"]\n  scores_test &lt;- predict(rf, newdata = test_data, type = \"vote\")[, \"1\"]\n  list(\n    scores_train = scores_train,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n\n7.1.4 Calibration Curve\nWe define the calib_curve_locfit_simul_rf() function to compute the calibration curve for a simulation.\n\n\nCode\n#' Get the calibration curve for one simulation for the random forest,\n#' using the local regression approach\n#' \n#' @param simul results of a simulation obtained with simul_rf\n#' @param linspace values at which to compute the mean observed event when \n#'   computing the WMSE\ncalib_curve_locfit_simul_rf &lt;- function(simul,\n                                        linspace = NULL) {\n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  \n  n_obs &lt;- simul$n_obs\n  seed &lt;- simul$seed\n  # Get the data used to train the forest\n  data &lt;- get_samples(n_obs = n_obs, seed = seed)\n  tb_train &lt;- data$tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # Scores estimated by the RF\n  scores_train &lt;- simul$scores$scores_train\n  scores_calib &lt;- simul$scores$scores_calib\n  scores_test &lt;- simul$scores$scores_test\n  \n  locfit_0_train &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_train$d, score = scores_train)\n  )\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_c_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n  score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  \n  # Make sure to have values in [0,1]\n  score_c_locfit_0_train[score_c_locfit_0_train &gt; 1] &lt;- 1\n  score_c_locfit_0_train[score_c_locfit_0_train &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n  score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n  \n  score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n  score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n  \n  res_train &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_c_locfit_0_test,\n    sample = \"test\"\n  )\n  \n  res_train |&gt; \n    bind_rows(\n      res_calib\n    ) |&gt; \n    bind_rows(\n      res_test\n    ) |&gt; \n    mutate(\n      n_obs = n_obs,\n      seed = seed\n    )\n}\n\n\n\n\n7.1.5 Recalibration\nWe define a core function, recalibrate() which recalibrates the scores using a recalibrator (we use, as in Chapter 2, the following recalibrator: Platt scaling, isotonic regression, beta calibration, and locfit).\n\n#' Recalibrates scores using a calibrator\n#' \n#' @param obs_train vector of observed events in the train set\n#' @param scores_train vector of predicted probabilities in the train set\n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression, `\"beta\"` for beta calibration, \n#'   `\"locfit\"` for local regression)\n#' @param params list of named parameters to use in the local regression \n#'   (`nn` for fraction of nearest neighbors to use, `deg` for degree)\n#' @param linspace vector of alues at which to compute the recalibrated scores\n#' @returns list of three elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set, and recalibrated scores on a segment \n#'   of values\nrecalibrate &lt;- function(obs_train,\n                        pred_train, \n                        obs_calib,\n                        pred_calib,\n                        obs_test,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\", \"beta\", \"locfit\"),\n                        params = NULL,\n                        linspace = NULL) {\n  \n  if (is.null(linspace)) linspace &lt;- seq(0, 1, length.out = 101)\n  data_train &lt;- tibble(d = obs_train, scores = pred_train)\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  # Recalibrator trained on calibration data\n  if (method == \"platt\") {\n    # Recalibrator\n    lr &lt;- glm(\n      d ~ scores, family = binomial(link = 'logit'), data = data_calib\n    )\n    # Recalibrated scores on calib/test sets\n    score_c_train &lt;- predict(lr, newdata = data_train, type = \"response\")\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n    # Recalibrated scores on [0,1]\n    score_c_linspace &lt;- predict(\n      lr, newdata = tibble(scores = linspace), type = \"response\"\n    )\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_train &lt;- fit_iso(data_train$scores)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    score_c_linspace &lt;- fit_iso(linspace)\n  } else if (method == \"beta\") {\n    capture.output({\n      bc &lt;- try(beta_calibration(\n        p = data_calib$scores, \n        y = data_calib$d, \n        parameters = \"abm\" # 3 parameters a, b & m\n      ))\n    })\n    if (!inherits(bc, \"try-error\")) {\n      score_c_train &lt;- beta_predict(p = data_train$scores, bc)\n      score_c_calib &lt;- beta_predict(p = data_calib$scores, bc)\n      score_c_test &lt;- beta_predict(p = data_test$scores, bc)\n      score_c_linspace &lt;- beta_predict(p = linspace, bc)\n    } else {\n      score_c_train &lt;- score_c_calib &lt;- score_c_test &lt;- score_c_linspace &lt;- NA\n    }\n    \n  } else if (method == \"locfit\") {\n    noise_scores &lt;- data_calib$scores + rnorm(nrow(data_calib), 0, 0.01)\n    noise_data_calib &lt;- data_calib %&gt;% mutate(scores = noise_scores)\n    locfit_reg &lt;- locfit(\n      formula = d ~ lp(scores, nn = params$nn, deg = params$deg), \n      kern = \"rect\", maxk = 200, data = noise_data_calib\n    )\n    score_c_train &lt;- predict(locfit_reg, newdata = data_train)\n    score_c_train[score_c_train &lt; 0] &lt;- 0\n    score_c_train[score_c_train &gt; 1] &lt;- 1\n    score_c_calib &lt;- predict(locfit_reg, newdata = data_calib)\n    score_c_calib[score_c_calib &lt; 0] &lt;- 0\n    score_c_calib[score_c_calib &gt; 1] &lt;- 1\n    score_c_test &lt;- predict(locfit_reg, newdata = data_test)\n    score_c_test[score_c_test &lt; 0] &lt;- 0\n    score_c_test[score_c_test &gt; 1] &lt;- 1\n    score_c_linspace &lt;- predict(locfit_reg, newdata = linspace)\n    score_c_linspace[score_c_linspace &lt; 0] &lt;- 0\n    score_c_linspace[score_c_linspace &gt; 1] &lt;- 1\n  } else {\n    stop(str_c(\n      'Wrong method. Use one of the following:',\n      '\"platt\", \"isotonic\", \"beta\", \"locfit\"'\n    ))\n  }\n  \n  # Format results in tibbles:\n  # For train set\n  tb_score_c_train &lt;- tibble(\n    d = obs_train,\n    p_u = pred_train,\n    p_c = score_c_train\n  )\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  # For linear space\n  tb_score_c_linspace &lt;- tibble(\n    linspace = linspace,\n    p_c = score_c_linspace\n  )\n  \n  list(\n    tb_score_c_train = tb_score_c_train,\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test,\n    tb_score_c_linspace = tb_score_c_linspace\n  )\n  \n}\n\n\n\n7.1.6 Wrapper Function\nLastly, we define a wrapper function to perform a single replication of the simulations, for a type of problem (regression or classification).\n\nsimul_calib_rf &lt;- function(seed,\n                           type = c(\"regression\", \"classification\"),\n                           tuning = TRUE){\n  \n  set.seed(seed)\n  linspace &lt;- seq(0, 1, length.out = 101)\n  \n  # 1. Get and Split the dataset----\n  \n  data &lt;- get_samples(seed = seed, data = tb_rest)\n  tb_train &lt;- tb_train\n  tb_calib &lt;- data$tb_calib\n  tb_test &lt;- data$tb_test\n  \n  # 2. Train RF----\n  \n  if (type == \"regression\") {\n    # RF regressor parameters tuned\n    if (tuning == TRUE) {\n      # Use hyperparameters selected in `rf-grid-search.R`\n      nodesize &lt;- best_params_rf_reg$nodesize\n      mtry &lt;- best_params_rf_reg$mtry\n      ntree &lt;- best_params_rf_reg$num_trees\n      splitrule &lt;- best_params_rf_reg$splitrule\n    } else {\n      # Use default values from `randomForest()`\n      nodesize &lt;- 5\n      # nodesize &lt;- 0.1 * nrow(tb_train)\n      mtry &lt;- max(floor(ncol(tb_train)/3), 1)\n      ntree &lt;- 500\n      splitrule &lt;- \"gini\"\n    }\n    \n    # Regression\n    scores_reg &lt;- apply_rf(\n      train_data = tb_train,\n      calib_data = tb_calib, \n      test_data = tb_test,\n      mtry = mtry,\n      nodesize = nodesize,\n      ntree = ntree, \n      splitrule = splitrule\n    )\n    scores &lt;- scores_reg\n  } else {\n    if (tuning == TRUE) {\n      # Use hyperparameters selected in `rf-grid-search.R`\n      nodesize &lt;- best_params_rf_classif$nodesize\n      mtry &lt;- best_params_rf_classif$mtry\n      ntree &lt;- best_params_rf_classif$num_trees\n      splitrule &lt;- best_params_rf_classif$splitrule\n    } else {\n      # Use default values from `randomForest()`\n      nodesize &lt;- 0.1 * nrow(tb_train)\n      mtry &lt;- max(floor(ncol(tb_train)/3), 1)\n      ntree &lt;- 500\n      splitrule &lt;- \"gini\"\n    }\n    # Classification\n    scores_classif &lt;- apply_rf_vote(\n      train_data = tb_train, \n      calib_data = tb_calib, \n      test_data = tb_test,\n      mtry = mtry,\n      nodesize = nodesize,\n      ntree = ntree, \n      splitrule = splitrule\n    )\n    scores &lt;- scores_classif\n  }\n  \n  # 3. Calibration----\n  \n  ## 3.1. Metrics----\n  \n  # For regression\n  calibration_train &lt;- compute_metrics(\n    obs = tb_train$d, \n    scores = scores$scores_train,\n    k = 10\n  ) |&gt; \n    mutate(sample = \"train\")\n  \n  calibration_calib &lt;- compute_metrics(\n    obs = tb_calib$d, \n    scores = scores$scores_calib,\n    k = 5\n  ) |&gt; \n    mutate(sample = \"calibration\")\n  \n  calibration_test &lt;- compute_metrics(\n    obs = tb_test$d, \n    scores = scores$scores_test,\n    k = 5\n  ) |&gt; \n    mutate(sample = \"test\")\n  \n  calibration &lt;- \n    calibration_train |&gt; \n    bind_rows(calibration_calib) |&gt; \n    bind_rows(calibration_test) |&gt;\n    mutate(model = type)\n  \n  # Comparison with standard metrics\n  gof_train &lt;- compute_gof(\n    obs = tb_train$d, \n    pred = scores$scores_train\n  ) |&gt; mutate(sample = \"train\")\n  \n  gof_calib &lt;- compute_gof( \n    obs = tb_calib$d, \n    pred = scores$scores_calib\n  ) |&gt; mutate(sample = \"calibration\")\n  \n  gof_test &lt;- compute_gof(\n    obs = tb_test$d, \n    pred = scores$scores_test\n  ) |&gt; mutate(sample = \"test\")\n  \n  gof &lt;- \n    gof_train |&gt; \n    bind_rows(gof_calib) |&gt; \n    bind_rows(gof_test) |&gt;\n    mutate(model = type)\n  \n  summary_metrics &lt;- \n    gof |&gt; \n    left_join(calibration, by = c(\"mse\", \"sample\", \"model\")) |&gt; \n    relocate(sample, model, .before = \"mse\") |&gt;\n    mutate(seed = seed)\n  \n  # 5. Calibration curves with locfit ----\n  \n  scores_train &lt;- scores$scores_train\n  scores_calib &lt;- scores$scores_calib\n  scores_test &lt;- scores$scores_test\n  # Add a little noise to the scores, to avoir crashing R\n  scores_train &lt;- scores_train + rnorm(length(scores_train), 0, .001)\n  scores_calib &lt;- scores_calib + rnorm(length(scores_calib), 0, .001)\n  scores_test &lt;- scores_test + rnorm(length(scores_test), 0, .001)\n  \n  locfit_0_train &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_train$d, score = scores_train)\n  )\n  locfit_0_calib &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_calib$d, score = scores_calib)\n  )\n  locfit_0_test &lt;- locfit(\n    formula = d ~ lp(score, nn = 0.15, deg = 0), \n    kern = \"rect\", maxk = 200, \n    data = tibble(d = tb_test$d, score = scores_test)\n  )\n  \n  score_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n  score_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n  score_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n  # Make sure to have values in [0,1]\n  score_locfit_0_train[score_locfit_0_train &gt; 1] &lt;- 1\n  score_locfit_0_train[score_locfit_0_train &lt; 0] &lt;- 0\n  \n  score_locfit_0_calib[score_locfit_0_calib &gt; 1] &lt;- 1\n  score_locfit_0_calib[score_locfit_0_calib &lt; 0] &lt;- 0\n  \n  score_locfit_0_test[score_locfit_0_test &gt; 1] &lt;- 1\n  score_locfit_0_test[score_locfit_0_test &lt; 0] &lt;- 0\n  \n  res_train &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_train,\n    sample = \"train\"\n  )\n  res_calib &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_calib,\n    sample = \"calibration\"\n  )\n  res_test &lt;- tibble(\n    xlim = linspace,\n    locfit_pred = score_locfit_0_test,\n    sample = \"test\"\n  )\n  \n  calibration_curves &lt;- \n    res_train |&gt; \n    bind_rows(\n      res_calib\n    ) |&gt; \n    bind_rows(\n      res_test\n    ) |&gt; \n    mutate(\n      seed = seed,\n      type = type,\n      method = \"No Calibration\"\n    )\n  \n  # 6. Recalibration----\n  \n  methods &lt;- c(\"platt\", \"isotonic\", \"beta\", \"locfit\", \"locfit\", \"locfit\")\n  params &lt;- list(\n    NULL, NULL, NULL, \n    list(nn = .15, deg = 0), list(nn = .15, deg = 1), list(nn = .15, deg = 2)\n  )\n  method_names &lt;- c(\n    \"platt\", \"isotonic\", \"beta\", \"locfit_0\", \"locfit_1\", \"locfit_2\"\n  )\n  \n  scores_c &lt;- vector(mode = \"list\", length = length(methods))\n  names(scores_c) &lt;- method_names\n  res_c &lt;- tibble()\n  \n  for (i_method in 1:length(methods)) {\n    method &lt;- methods[i_method]\n    params_current &lt;- params[[i_method]]\n    \n    ## 6.1 Recalibrated Scores----\n    scores_c[[i_method]] &lt;- recalibrate(\n      obs_train = tb_train$d, \n      pred_train = scores$scores_train, \n      obs_calib = tb_calib$d, \n      pred_calib = scores$scores_calib, \n      obs_test = tb_test$d, \n      pred_test = scores$scores_test, \n      method = method,\n      params = params_current\n    )\n    ## 6.2 Recalibration Curves----\n    scores_c_train &lt;- scores_c[[i_method]]$tb_score_c_train$p_c\n    scores_c_calib &lt;- scores_c[[i_method]]$tb_score_c_calib$p_c\n    scores_c_test &lt;- scores_c[[i_method]]$tb_score_c_test$p_c\n    # Add a little noise to the scores, to avoir crashing R\n    scores_c_train &lt;- scores_c_train + rnorm(length(scores_c_train), 0, .001)\n    scores_c_calib &lt;- scores_c_calib + rnorm(length(scores_c_calib), 0, .001)\n    scores_c_test &lt;- scores_c_test + rnorm(length(scores_c_test), 0, .001)\n    \n    locfit_0_train &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_train$d, score = scores_c_train)\n    )\n    locfit_0_calib &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_calib$d, score = scores_c_calib)\n    )\n    locfit_0_test &lt;- locfit(\n      formula = d ~ lp(score, nn = 0.15, deg = 0), \n      kern = \"rect\", maxk = 200, \n      data = tibble(d = tb_test$d, score = scores_c_test)\n    )\n    \n    score_c_locfit_0_train &lt;- predict(locfit_0_train, newdata = linspace)\n    score_c_locfit_0_calib &lt;- predict(locfit_0_calib, newdata = linspace)\n    score_c_locfit_0_test &lt;- predict(locfit_0_test, newdata = linspace)\n    \n    # Make sure to have values in [0,1]\n    score_c_locfit_0_train[score_c_locfit_0_train &gt; 1] &lt;- 1\n    score_c_locfit_0_train[score_c_locfit_0_train &lt; 0] &lt;- 0\n    \n    score_c_locfit_0_calib[score_c_locfit_0_calib &gt; 1] &lt;- 1\n    score_c_locfit_0_calib[score_c_locfit_0_calib &lt; 0] &lt;- 0\n    \n    score_c_locfit_0_test[score_c_locfit_0_test &gt; 1] &lt;- 1\n    score_c_locfit_0_test[score_c_locfit_0_test &lt; 0] &lt;- 0\n    \n    res_c_train &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_train,\n      sample = \"train\",\n      method = method_names[i_method]\n    )\n    res_c_calib &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_calib,\n      sample = \"calibration\",\n      method = method_names[i_method]\n    )\n    res_c_test &lt;- tibble(\n      xlim = linspace,\n      locfit_pred = score_c_locfit_0_test,\n      sample = \"test\",\n      method = method_names[i_method]\n    )\n    \n    res_c &lt;- res_c  |&gt;\n      bind_rows(\n        res_c_train) |&gt;\n      bind_rows(\n        res_c_calib\n      ) |&gt; \n      bind_rows(\n        res_c_test\n      )\n  }\n  \n  res_c &lt;- res_c |&gt;\n    mutate(\n      seed = seed,\n      type = type\n    )\n  \n  recalibration_curves &lt;- calibration_curves |&gt; bind_rows(res_c)\n  \n  # 7. Recalibration Metrics----\n  \n  ## 7.1. Standard Metrics----\n  \n  gof_c_train &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_train$d, \n      pred = .x$tb_score_c_train$p_c\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"train\")\n  \n  gof_c_calib &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_calib$d, \n      pred = .x$tb_score_c_calib$p_c\n    ) \n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"calibration\")\n  \n  gof_c_test &lt;- map(\n    .x = scores_c,\n    .f = ~compute_gof(\n      obs = tb_test$d, \n      pred = .x$tb_score_c_test$p_c\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"test\")\n  \n  gof_c &lt;- \n    gof_c_train |&gt; \n    bind_rows(gof_c_calib) |&gt; \n    bind_rows(gof_c_test) |&gt;\n    mutate(model = type)\n  \n  ## 7.2. Calibration Metrics----\n  \n  calibration_c_train &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_train$d, \n      scores = .x$tb_score_c_train$p_c,\n      k = 10\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"train\")\n  \n  calibration_c_calib &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_calib$d, \n      scores = .x$tb_score_c_calib$p_c,\n      k = 5\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"calibration\")\n  \n  calibration_c_test &lt;- map(\n    .x = scores_c,\n    .f = ~compute_metrics(\n      obs = tb_test$d, \n      scores = .x$tb_score_c_test$p_c,\n      k = 5\n    )\n  ) |&gt; \n    list_rbind(names_to = \"method\") |&gt; \n    mutate(sample = \"test\")\n  \n  \n  calibration_c &lt;- \n    calibration_c_train |&gt; \n    bind_rows(calibration_c_calib) |&gt; \n    bind_rows(calibration_c_test) |&gt;\n    mutate(model = type)\n  \n  summary_metrics_c &lt;- \n    gof_c |&gt; \n    left_join(calibration_c, by = c(\"mse\", \"sample\", \"model\", \"method\")) |&gt; \n    relocate(sample, model, method, .before = \"mse\") |&gt;\n    mutate(seed = seed)\n  \n  summary_metrics &lt;- summary_metrics |&gt; mutate(method = \"No Calibration\")\n  \n    list(\n      recalibration_curves = recalibration_curves,\n      summary_metrics = list_rbind(list(summary_metrics, summary_metrics_c)),\n      seed = seed,\n      scores = scores,\n      scores_c = scores_c,\n      type = type,\n      tuning = tuning\n    )\n}"
  },
  {
    "objectID": "rf-simulations.html#import-data",
    "href": "rf-simulations.html#import-data",
    "title": "7  Simulations",
    "section": "7.2 Import Data",
    "text": "7.2 Import Data\nLet us load the data obtained in Chapter 6.\n\ntb_train &lt;- read.csv(\n  \"data/data_credit_smote_train.csv\")\ntb_rest &lt;- read.csv(\n  \"data/data_credit_smote_rest.csv\")\n\nThe set of hyperparameters and the associated oob MSE/oob error rate:\n\nbest_params_rf_reg &lt;- read_csv(\n  \"output/best_params_rf_reg.csv\")\nbest_params_rf_classif &lt;- read_csv(\n  \"output/best_params_rf_classif.csv\")\n\nWe select the hyperparameters with:\n\nthe lowest MSE (for regression)\nthe highest accuracy (for classification).\n\nThe set of hyperparameters for the regression task:\n\nbest_params_rf_reg &lt;- \n  best_params_rf_reg |&gt; \n  arrange(mse_oob) |&gt; \n  slice(1)\nbest_params_rf_reg\n\n# A tibble: 1 × 5\n   mtry nodesize num_trees splitrule mse_oob\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1    12        5       500 gini        0.116\n\n\nThe set of hyperparameters for the classification task:\n\nbest_params_rf_classif &lt;- \n  best_params_rf_classif |&gt; \n  arrange(err_oob) |&gt; \n  slice(1)\nbest_params_rf_classif\n\n# A tibble: 1 × 5\n   mtry nodesize num_trees splitrule err_oob\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1    12        5       300 gini        0.157"
  },
  {
    "objectID": "rf-simulations.html#running-the-simulations",
    "href": "rf-simulations.html#running-the-simulations",
    "title": "7  Simulations",
    "section": "7.3 Running the Simulations",
    "text": "7.3 Running the Simulations\nWe consider 200 different splits of the data on which a recalibration technique will be applied.\n\nn_repl &lt;- 200\nseed &lt;- 1:n_repl\n\n\n7.3.1 Regression\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  metrics_rf_tuned_reg &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      resul &lt;- simul_calib_rf(\n        seed = .x,\n        type = \"regression\",\n        tuning = TRUE\n      )\n      p()\n      resul\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use.\n\nsave(\n  metrics_rf_tuned_reg, \n  file = \"/output/simul-rf/metrics_rf_tuned_reg.rda\")\n\n\n\n7.3.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = n_repl)\n  metrics_rf_tuned_class &lt;- furrr::future_map(\n    .x = 1:n_repl,\n    .f = ~{\n      resul &lt;- simul_calib_rf(\n        seed = .x,\n        type = \"classification\",\n        tuning = TRUE\n      )\n      p()\n      resul\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\nWe save the results for later use.\n\nsave(\n  metrics_rf_tuned_class, \n  file = \"/output/simul-rf/metrics_rf_tuned_class.rda\"\n  )"
  },
  {
    "objectID": "rf-simulations.html#sec-calib-vs-gof-simul",
    "href": "rf-simulations.html#sec-calib-vs-gof-simul",
    "title": "7  Simulations",
    "section": "7.4 Calibration vs. Goodness-of-fit",
    "text": "7.4 Calibration vs. Goodness-of-fit\nWe now explore the relationship between calibration and goodness-of-fit.\nLet us consider different set of hyperparameters for the random forests. We will train a forest on each set and compute calibration and goodness-of-fit metrics on both the training set and on the remaining set (we no longer split the remaining data into a calibration set and a test set).\n\ngrid_params &lt;- \n  expand_grid(\n    num_trees = c(100, 300, 500),\n    mtry = seq(1, (ncol(tb_train)/2)),\n    nodesize = c(5, 10, 15, 20),\n    splitrule = \"gini\"\n  )\n\nLet us run the simulations.\n\n7.4.1 Regression\n\n# Grid search : regression\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  compare_cal_gof_reg &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        d ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      out_of_bag &lt;- map(.x = 1:nrow(tb_train), .f = ~which(rf[[\"inbag\"]][.x,] == 0))\n      rf_pred_all &lt;- predict(rf, tb_train,\n                             predict.all = TRUE,\n                             type = \"response\")$individual\n      rf_pred &lt;- unlist(map(.x = 1:nrow(tb_train), .f = ~mean(rf_pred_all[.x,out_of_bag[[.x]]])))\n      \n      oob_err &lt;- mse_function(pred = rf_pred, obs = tb_train |&gt; pull(d))\n      mse_oob &lt;- oob_err\n      \n      # Predict RF on train/rest dataset\n      scores_train &lt;- predict(rf, newdata = tb_train, type = \"response\")\n      scores_rest &lt;- predict(rf, newdata = tb_rest, type = \"response\")\n      \n      # Calibration metrics (Brier Score and LCS) on train/rest dataset\n      calib_metrics_train &lt;- compute_metrics(obs = tb_train$d, scores = scores_train)\n      #calib_metrics_train &lt;- calib_metrics_train |&gt; \n      #  select(locfit_score, brier)\n      calib_metrics_rest &lt;- compute_metrics(obs = tb_rest$d, scores = scores_rest)\n      #calib_metrics_rest &lt;- calib_metrics_rest |&gt; \n      #  select(locfit_score, brier)\n      \n      # GOF metrics on train/rest dataset\n      gof_metrics_train &lt;- compute_gof(obs = tb_train$d, pred = scores_train)\n      #gof_metrics_train &lt;- gof_metrics_train |&gt; \n      #  select(accuracy, AUC)\n      gof_metrics_rest &lt;- compute_gof(obs = tb_rest$d, pred = scores_rest)\n      #gof_metrics_rest &lt;- gof_metrics_rest |&gt; \n      #  select(accuracy, AUC)\n      \n      # Update progressbar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        mse_oob = mse_oob,\n        brier_train = calib_metrics_train$brier,\n        LCS_train = calib_metrics_train$locfit_score,\n        ece_train = calib_metrics_train$ece,\n        qmse_train = calib_metrics_train$qmse,\n        wmse_train = calib_metrics_train$wmse,\n        sensitivity_train = gof_metrics_train$sensitivity,\n        specificity_train = gof_metrics_train$specificity,\n        AUC_train = gof_metrics_train$AUC,\n        accuracy_train = gof_metrics_train$accuracy,\n        brier_rest = calib_metrics_rest$brier,\n        LCS_rest = calib_metrics_rest$locfit_score,\n        ece_rest = calib_metrics_rest$ece,\n        qmse_rest = calib_metrics_rest$qmse,\n        wmse_rest = calib_metrics_rest$wmse,\n        sensitivity_rest = gof_metrics_rest$sensitivity,\n        specificity_rest = gof_metrics_rest$specificity,\n        AUC_rest = gof_metrics_rest$AUC,\n        accuracy_rest = gof_metrics_rest$accuracy\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\ncompare_cal_gof_reg &lt;- list_rbind(compare_cal_gof_reg)\n\nWe save the results for later use.\n\nwrite.csv(\n  compare_cal_gof_reg, \n  file = \"/output/compare_cal_gof_reg.csv\", \n  row.names = FALSE\n)\n\n\n\n7.4.2 Classification\n\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(grid_params))\n  compare_cal_gof_class &lt;- furrr::future_map(\n    .x = 1:nrow(grid_params),\n    .f = ~{\n      # Estim random forest and get the evaluation metric\n      rf &lt;- randomForest(\n        as.factor(d) ~ ., \n        data = tb_train, \n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        ntree = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        keep.inbag = TRUE\n      )\n      \n      num_trees &lt;- grid_params$num_trees[.x]\n      \n      # Identify out of bag observations in each tree\n      err_oob &lt;- rf$err.rate[num_trees,1]\n      \n      # Predict RF on train/rest dataset\n      scores_train &lt;- predict(rf, newdata = tb_train, type = \"vote\")[, \"1\"]\n      scores_rest &lt;- predict(rf, newdata = tb_rest, type = \"vote\")[, \"1\"]\n      \n      # Calibration metrics (Brier Score and LCS) on train/rest dataset\n      calib_metrics_train &lt;- compute_metrics(obs = tb_train$d, scores = scores_train)\n      #calib_metrics_train &lt;- calib_metrics_train |&gt; \n      #  select(locfit_score, brier)\n      calib_metrics_rest &lt;- compute_metrics(obs = tb_rest$d, scores = scores_rest)\n      #calib_metrics_rest &lt;- calib_metrics_rest |&gt; \n      #  select(locfit_score, brier)\n      \n      # GOF metrics on train/rest dataset\n      gof_metrics_train &lt;- compute_gof(obs = tb_train$d, pred = scores_train)\n      #gof_metrics_train &lt;- gof_metrics_train |&gt; \n      #  select(accuracy, AUC)\n      gof_metrics_rest &lt;- compute_gof(obs = tb_rest$d, pred = scores_rest)\n      #gof_metrics_rest &lt;- gof_metrics_rest |&gt; \n      #  select(accuracy, AUC)\n      \n      # Update progressbar\n      p()\n      \n      # Return object:\n      tibble(\n        mtry = grid_params$mtry[.x], \n        nodesize = grid_params$nodesize[.x], \n        num_trees = grid_params$num_trees[.x],\n        splitrule =  grid_params$splitrule[.x],\n        err_oob = err_oob,\n        brier_train = calib_metrics_train$brier,\n        LCS_train = calib_metrics_train$locfit_score,\n        ece_train = calib_metrics_train$ece,\n        qmse_train = calib_metrics_train$qmse,\n        wmse_train = calib_metrics_train$wmse,\n        sensitivity_train = gof_metrics_train$sensitivity,\n        specificity_train = gof_metrics_train$specificity,\n        AUC_train = gof_metrics_train$AUC,\n        accuracy_train = gof_metrics_train$accuracy,\n        brier_rest = calib_metrics_rest$brier,\n        LCS_rest = calib_metrics_rest$locfit_score,\n        ece_rest = calib_metrics_rest$ece,\n        qmse_rest = calib_metrics_rest$qmse,\n        wmse_rest = calib_metrics_rest$wmse,\n        sensitivity_rest = gof_metrics_rest$sensitivity,\n        specificity_rest = gof_metrics_rest$specificity,\n        AUC_rest = gof_metrics_rest$AUC,\n        accuracy_rest = gof_metrics_rest$accuracy\n      )\n    },\n    .options = furrr::furrr_options(seed = FALSE)\n  )\n})\n\ncompare_cal_gof_class &lt;- list_rbind(compare_cal_gof_class)\n\nWe save the results for later use.\n\nwrite.csv(\n  compare_cal_gof_class, \n  file = \"/output/compare_cal_gof_class.csv\", \n  row.names = FALSE\n)"
  },
  {
    "objectID": "rf-simul-metrics.html#load-results",
    "href": "rf-simul-metrics.html#load-results",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.1 Load Results",
    "text": "8.1 Load Results\nLet us load the results from Chapter 7.\nFor the regression forests:\n\nload(\"output/simul-rf/metrics_rf_tuned_reg.rda\")\n\nFor the classification forests:\n\nload(\"output/simul-rf/metrics_rf_tuned_class.rda\")"
  },
  {
    "objectID": "rf-simul-metrics.html#metrics",
    "href": "rf-simul-metrics.html#metrics",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.2 Metrics",
    "text": "8.2 Metrics\nWe extract the summary metrics from these.\n\nsummary_metrics_tuned_reg &lt;- \n  map(metrics_rf_tuned_reg, \"summary_metrics\") |&gt; \n  list_rbind()\nsummary_metrics_tuned_class &lt;- map(metrics_rf_tuned_class, \"summary_metrics\") |&gt; \n  list_rbind()\n\nLet us bind the results in a single tibble.\n\nmetrics_rf &lt;- summary_metrics_tuned_reg |&gt; \n  mutate(model = \"Regression\") |&gt; \n  bind_rows(\n    summary_metrics_tuned_class |&gt; \n      mutate(model = \"Classification\")\n  ) |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    AUC = 1-AUC,\n    model = factor(\n      model, \n      levels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = c(\"No Calibration\", \"platt\", \"isotonic\", \"beta\", \n                 \"locfit_0\", \"locfit_1\", \"locfit_2\") |&gt; rev(),\n      labels = c(\"No Calibration\", \"Platt Scaling\", \"Isotonic\", \"Beta\", \n                 \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\") |&gt; rev()\n    ),\n    sample = factor(\n      sample,\n      levels = c(\"calibration\", \"test\") |&gt; rev(),\n      labels = c(\"Calibration\", \"Test\") |&gt; rev())\n  )\nmetrics_rf\n\n# A tibble: 5,600 × 17\n   sample  model mse   accuracy missclass_rate sensitivity specificity threshold\n   &lt;fct&gt;   &lt;fct&gt; &lt;lgl&gt;    &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 Calibr… Regr… NA       0.852          0.148       0.758       0.923       0.5\n 2 Test    Regr… NA       0.852          0.148       0.755       0.923       0.5\n 3 Calibr… Regr… NA       0.860          0.140       0.804       0.902       0.5\n 4 Calibr… Regr… NA       0.861          0.139       0.812       0.897       0.5\n 5 Calibr… Regr… NA       0.860          0.140       0.825       0.887       0.5\n 6 Calibr… Regr… NA       0.860          0.140       0.813       0.895       0.5\n 7 Calibr… Regr… NA       0.860          0.140       0.822       0.890       0.5\n 8 Calibr… Regr… NA       0.860          0.140       0.818       0.892       0.5\n 9 Test    Regr… NA       0.857          0.143       0.794       0.903       0.5\n10 Test    Regr… NA       0.858          0.142       0.803       0.899       0.5\n# ℹ 5,590 more rows\n# ℹ 9 more variables: FPR &lt;dbl&gt;, AUC &lt;dbl&gt;, locfit_score &lt;dbl&gt;, brier &lt;dbl&gt;,\n#   ece &lt;dbl&gt;, qmse &lt;dbl&gt;, wmse &lt;dbl&gt;, seed &lt;int&gt;, method &lt;fct&gt;"
  },
  {
    "objectID": "rf-simul-metrics.html#boxplots",
    "href": "rf-simul-metrics.html#boxplots",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.3 Boxplots",
    "text": "8.3 Boxplots\nLet us define two functions to display the goodness-of-fit metrics (boxplot_metrics_gof()) and the calibration metrics (boxplot_metrics_calib()) in boxplots.\n\nboxplot_metrics_gof &lt;- function(data_metrics){\n  \n  gof_metrics &lt;- c(\"accuracy\", \"sensitivity\", \"specificity\")\n  gof_metrics_lab &lt;- c(\"Accuracy\", \"Sensitivity\", \"Specificity\")\n  \n  data_metrics_plot &lt;- \n    data_metrics |&gt; \n    select(-mse) |&gt; \n    select(sample, model, method, seed, !!!syms(gof_metrics)) |&gt; \n    pivot_longer(\n      cols = !!gof_metrics, \n      names_to = \"metric\", values_to = \"value\"\n    )\n  \n  mat &lt;- matrix(\n    c(1:(6), rep(7,3)), ncol = 3, byrow = TRUE\n  )\n  layout(mat, heights = c(rep(3, 4),1))\n  \n  range_val &lt;- data_metrics_plot |&gt; \n    group_by(metric) |&gt; \n    summarise(\n      min_val = min(value),\n      max_val = max(value)\n    )\n  \n  # Goodness of Fit----\n  for (i_model in 1:length(levels(data_metrics_plot$model))) {\n    model &lt;- levels(data_metrics_plot$model)[i_model]\n    data_metrics_gof &lt;- data_metrics_plot |&gt; \n      filter(model == !!model)\n    \n    for (i_metric in 1:length(gof_metrics)) {\n      \n      metric &lt;- gof_metrics[i_metric]\n      metric_lab &lt;- gof_metrics_lab[i_metric]\n      val_min &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(min_val)\n      val_max &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(max_val)\n      title &lt;- \"\"\n      if (i_metric == 2) {\n        title &lt;- str_c(model, \"\\n\", metric_lab)\n      } else {\n        title &lt;- str_c(\"\\n\", metric_lab)\n      }\n      \n      nb_methods &lt;- data_metrics_gof$method |&gt; levels() |&gt; length()\n      \n      data_metrics_gof_current &lt;- \n        data_metrics_gof |&gt; \n        filter(metric == !!metric)\n      \n      par(mar = c(2.5, 1, 4.1, 1))\n      boxplot(\n        value ~ sample + method,\n        data = data_metrics_gof_current,\n        col = colours,\n        horizontal = TRUE,\n        las = 1, xlab = \"\", ylab = \"\",\n        main = title,\n        border = c(\"black\", adjustcolor(\"black\", alpha.f = .5)),\n        yaxt = \"n\",\n        ylim = c(val_min, val_max)\n      )\n      # Horizontal lines\n      for (i in seq(3, nb_methods * 2, by = 2) - .5) {\n        abline(h = i, lty = 1, col = \"gray\")\n      }\n    }\n    \n  }\n  \n  # Legend----\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = rev(levels(data_metrics_plot$method)),\n    fill = rev(colours_calib),\n    # lwd = 2,\n    xpd = TRUE, ncol = 4\n  )\n}\n\n\nboxplot_metrics_calib &lt;- function(data_metrics){\n  \n  calib_metrics &lt;- c(\"brier\", \"ece\", \"locfit_score\")\n  calib_metrics_lab &lt;- c(\"Brier Score\", \"ECE\", \"LCS\")\n  \n  data_metrics_plot &lt;- \n    data_metrics |&gt; \n    select(-mse) |&gt; \n    select(sample, model, method, seed, !!!syms(calib_metrics)) |&gt; \n    pivot_longer(\n      cols = !!calib_metrics, \n      names_to = \"metric\", values_to = \"value\"\n    )\n  \n  range_val &lt;- data_metrics_plot |&gt; \n    group_by(metric) |&gt; \n    summarise(\n      min_val = min(value),\n      max_val = max(value)\n    )\n  \n  mat &lt;- matrix(\n    c(1:(6), rep(7,3)), ncol = 3, byrow = TRUE\n  )\n  layout(mat, heights = c(rep(3, 4),1))\n  \n  # Calibration Metrics----\n  for (i_model in 1:length(levels(data_metrics_plot$model))) {\n    model &lt;- levels(data_metrics_plot$model)[i_model]\n    data_metrics_calib &lt;- data_metrics_plot |&gt; \n      filter(model == !!model)\n    \n    for (i_metric in 1:length(calib_metrics)) {\n      metric &lt;- calib_metrics[i_metric]\n      metric_lab &lt;- calib_metrics_lab[i_metric]\n      val_min &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(min_val)\n      val_max &lt;- range_val |&gt; filter(metric == !!metric) |&gt; pull(max_val)\n      if (i_metric == 2) {\n        title &lt;- str_c(model, \"\\n\", metric_lab)\n      } else {\n        title &lt;- str_c(\"\\n\", metric_lab)\n      }\n      \n      nb_methods &lt;- data_metrics_calib$method |&gt; levels() |&gt; length()\n      \n      data_metrics_calib_current &lt;- \n        data_metrics_calib |&gt; \n        filter(metric == !!metric)\n      \n      par(mar = c(2.5, 1, 4.1, 1))\n      boxplot(\n        value ~ sample + method,\n        data = data_metrics_calib_current,\n        col = colours,\n        horizontal = TRUE,\n        las = 1, xlab = \"\", ylab = \"\",\n        main = title,\n        border = c(\"black\", adjustcolor(\"black\", alpha.f = .5)),\n        yaxt = \"n\",\n        ylim = c(val_min, val_max)\n      )\n      # Horizontal lines\n      for (i in seq(3, nb_methods * 2, by = 2) - .5) {\n        abline(h = i, lty = 1, col = \"gray\")\n      }\n    }\n    \n  }\n  \n  # Legend----\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = rev(levels(data_metrics_plot$method)),\n    fill = rev(colours_calib),\n    # lwd = 2,\n    xpd = TRUE, ncol = 4\n  )\n}\n\n\nboxplot_metrics_gof(metrics_rf)\n\n\n\nFigure 8.1: Goodness-of-fit Metrics Computed on 200 Replications, for the Regression Random Forest (top) and for the Classification Random Forest (bottom), on the Calibration (transparent colors) and on the Test Set (full colors). A probability threshold of \\(\\tau=0.5\\) was used to compute the sensivity and the specificity.\n\n\n\n\n\n\nboxplot_metrics_calib(metrics_rf)\n\n\n\nFigure 8.2: Calibration Metrics Computed on 200 Replications, for the Regression Random Forest (top) and for the Classification Random Forest (bottom), on the Calibration (transparent colors) and on the Test Set (full colors)."
  },
  {
    "objectID": "rf-simul-metrics.html#calibration-vs.-performance",
    "href": "rf-simul-metrics.html#calibration-vs.-performance",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.4 Calibration vs. Performance",
    "text": "8.4 Calibration vs. Performance\nLet us load the results obtaine in Section 7.4.\n\ncompare_class &lt;- read.csv(\"output/compare_cal_gof_class.csv\")\ncompare_reg &lt;- read.csv(\"output/compare_cal_gof_reg.csv\")\n\n\ncompare_class &lt;- compare_class |&gt; arrange(err_oob) |&gt; \n  mutate(AUC_train = 1-AUC_train,\n         AUC_rest = 1-AUC_rest)\ncompare_reg &lt;- compare_reg |&gt; arrange(mse_oob)|&gt; \n  mutate(AUC_train = 1-AUC_train,\n         AUC_rest = 1-AUC_rest)\n\nSome cosmetics:\n\nsamples &lt;- c(\"train\", \"test\")\nsample_labels &lt;- c(\"Train\", \"Test\")\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Test\" = \"#009E73\"\n)\n\nWe create a function to plot the calibration metrics vs goodness of fit metrics. Each point represent the result obtained in a simulation, computed either in the train set or in the test set.\n\nplot_compare &lt;- function(gof_metric, calib_metric) {\n  \n  gof_metrics &lt;- c(\"sensitivity\", \"specificity\", \"AUC\", \"accuracy\")\n  calib_metrics &lt;- c(\"brier\", \"LCS\", \"ece\", \"qmse\", \"wmse\")\n  types &lt;- c(\"regression\", \"classification\")\n  types_labs &lt;- c(\"Regression\", \"Classification\")\n  \n  mat &lt;- c(1,2,3,3) |&gt; matrix(ncol=2, byrow = TRUE)\n  layout(mat, heights = c(3,.5))\n  \n  for (i_type in 1:length(types)) {\n    type &lt;- types[i_type]\n    type_lab &lt;- types_labs[i_type]\n    \n    if (type == \"regression\"){\n      data &lt;- compare_reg\n    } else if(type == \"classification\") {\n      data &lt;- compare_class\n    } else {\n      stop(\"wrong type\")\n    }\n    \n    train_gof &lt;- paste(gof_metric, \"train\", sep = \"_\")\n    rest_gof &lt;- paste(gof_metric, \"rest\", sep = \"_\")\n    train_calib &lt;- paste(calib_metric, \"train\", sep = \"_\")\n    rest_calib &lt;- paste(calib_metric, \"rest\", sep = \"_\")\n    \n    # Plot boundaries\n    xmin &lt;- min(data[[train_gof]], data[[rest_gof]])\n    xmax &lt;- max(data[[train_gof]], data[[rest_gof]])\n    ymin &lt;- min(data[[train_calib]], data[[rest_calib]])\n    ymax &lt;- max(data[[train_calib]], data[[rest_calib]])\n    \n    \n    par(mar = c(4.1, 4.1, 2.1, 1))\n    \n    # From train to test\n    plot(\n      xmin, xmax,\n      xlim = c(xmin, xmax), ylim = c(ymin, ymax),\n      xlab = gof_metric, ylab = calib_metric,\n      main = type_lab,\n    )\n    segments(\n      x0 = data[[train_gof]], \n      y0 = data[[train_calib]], \n      x1 = data[[rest_gof]], \n      y1 = data[[rest_calib]],\n      col = adjustcolor(\"gray\", alpha.f = .3)\n    )\n    \n    # Train set\n    points(\n      x = data[[train_gof]], \n      y = data[[train_calib]],\n      pch = 19, cex = .8,\n      col = adjustcolor(colours_samples[[\"Train\"]], alpha.f = .3)\n    )\n    # Loess\n    lw_train &lt;- loess(data[[train_calib]] ~ data[[train_gof]])\n    ord &lt;- order(data[[train_gof]])\n    lines(data[[train_gof]][ord],lw_train$fitted[ord], col = colours_samples[[\"Train\"]],lwd = 3, lty = 3)\n    # Test set\n    points(\n      x = data[[rest_gof]], \n      y = data[[rest_calib]],\n      col = adjustcolor(colours_samples[[\"Test\"]], alpha.f = .3),\n      pch = 19, cex = .8\n    )\n    # Loess\n    lw_rest &lt;- loess(data[[rest_calib]] ~ data[[rest_gof]])\n    ord &lt;- order(data[[rest_gof]])\n    lines(data[[rest_gof]][ord],lw_rest$fitted[ord], col = colours_samples[[\"Test\"]],lwd = 3, lty = 3)\n    \n  }\n  # Legend\n  par(mar = c(0, 4.3, 0, 4.3))\n  plot.new()\n  legend(\n    \"center\", \n    legend = c(\"Train\", \"Test\"),\n    col = colours_samples,\n    pch = 19,\n    xpd = TRUE, ncol = 2\n  )\n  \n}\n\n\nLCS vs. AUCBrier vs. AUCECE vs. AUC\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"LCS\")\n\n\n\nFigure 8.3: LCS vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model.\n\n\n\n\n\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"brier\")\n\n\n\nFigure 8.4: Brier Score vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model.\n\n\n\n\n\n\n\n\nplot_compare(gof_metric = \"AUC\", calib_metric = \"ece\")\n\n\n\nFigure 8.5: Expected Calibration Error vs. AUC for Random Forest Regression (left) and Random Forest Classification (right). Each point represents an estimation obtained from a set of hyperparameters. The gray lines help identify the estimations made within each sample using the same model."
  },
  {
    "objectID": "rf-simul-metrics.html#calibration-curves",
    "href": "rf-simul-metrics.html#calibration-curves",
    "title": "8  Simulation Metrics and Viz",
    "section": "8.5 Calibration Curves",
    "text": "8.5 Calibration Curves\nLet us extract the calibration curves computed in Chapter 7.\n\ncurves_reg &lt;- \n  map(metrics_rf_tuned_reg, \"recalibration_curves\") |&gt; \n  list_rbind()\ncurves_class &lt;- \n  map(metrics_rf_tuned_class, \"recalibration_curves\") |&gt; \n  list_rbind()\n\nWe merge those in a single tibble:\n\ncalib_curves &lt;- \n  curves_reg |&gt; \n  bind_rows(curves_class) |&gt; \n  group_by(xlim, sample, type, method) |&gt; \n  summarise(\n    mean = mean(locfit_pred),\n    lower = quantile(locfit_pred, probs = .025),\n    upper = quantile(locfit_pred, probs = 0.975),\n    .groups = \"drop\"\n  )\n\nSome cosmetics:\n\nmethods &lt;- c(\n  \"No Calibration\",\n  \"platt\", \"isotonic\", \"beta\", \n  \"locfit_0\", \"locfit_1\", \"locfit_2\"\n)\nmethods_labs &lt;- c(\n  \"No Calibration\", \"Platt\", \"Isotonic\", \"Beta\", \n  \"Locfit (deg=0)\", \"Locfit (deg=1)\", \"Locfit (deg=2)\"\n)\n\nAnd we apply them:\n\ncalib_curves &lt;- \n  calib_curves |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"calibration\", \"test\"), \n      labels = c(\"Calibration\", \"Test\")\n    ),\n    type = factor(\n      type,\n      levels = c(\"regression\", \"classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = methods,\n      labels = methods_labs\n    )\n  )\n\nWe need to get the scores, to add boxplots on top of the curves.\n\n## Uncalibrated scores----\n\n### Train set----\nscores_no_calib_reg_train &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\nscores_no_calib_class_train &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\n\n### Calibration set----\nscores_no_calib_reg_calib &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_calib\")\nscores_no_calib_class_calib &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_calib\")\n\n### Test set----\nscores_no_calib_reg_test &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_test\")\nscores_no_calib_class_test &lt;- \n  map(metrics_rf_tuned_class, \"scores\") |&gt; \n  map(~.x$\"scores_test\")\n\n## Recalibrated Scores----\nscores_no_calib_reg_train &lt;- \n  map(metrics_rf_tuned_reg, \"scores\") |&gt; \n  map(~.x$\"scores_train\")\n\nNow, we will partition the [0,1] segment into equal-sized bins. In each bin, we will compute the number of observations with scores falling into that bin, for each sample of each simulation (and each recalibration technique).\n\n8.5.0.1 Helper Functions\nFirst, the count_scores() function, which counts the number of observations in each bin, for a vector of scores.\n\n#' For a vector of scores, compute the number of obs in each bin\n#' The bins are defined on evenly separated values in [0,1]\n#' \ncount_scores &lt;- function(scores) {\n  breaks &lt;- seq(0, 1, by = .05)\n  \n  if (!is.null(scores)) {\n    n_bins &lt;- table(cut(scores, breaks = breaks))\n  } else {\n    n_bins &lt;- NA_integer_\n  }\n  tibble(\n    bins = names(table(cut(breaks, breaks = breaks))),\n    n_bins = as.vector(n_bins)\n  )\n}\n\nThen, another function, count_scores_simul(), applies count_scores() to all the non-recalibrated scores in the simulations.\n\n#' Applies the count_scores() function to a list of simulations\n#' \ncount_scores_simul &lt;- function(scores_simul) {\n  map(scores_simul, count_scores) |&gt; \n    list_rbind() |&gt; \n    group_by(bins) |&gt; \n    summarise(\n      n_bins = mean(n_bins)\n    )\n}\n\nLastly, the count_scores_simul_method() function does the same for the all the recalibrated scores.\n\n#' Extract the recalibrated scores from a list of simulations and computes\n#' the number of scores in bins (see count_scores())\n#' For each simulation, there are multiple correction techniques\n#' \ncount_scores_simul_method &lt;- function(scores_simul_methods) {\n  map(scores_simul_methods, \"scores_c\") |&gt; \n    map(\n      .f = function(simul){\n        map(\n          simul, \n          .f = function(sim_method) {\n            count_scores_simul(sim_method$tb_score_c_train) |&gt; \n              mutate(sample = \"train\") |&gt; \n              bind_rows(\n                count_scores_simul(sim_method$tb_score_c_calib) |&gt; \n                  mutate(sample = \"calibration\")\n              ) |&gt; \n              bind_rows(\n                count_scores_simul(sim_method$tb_score_c_test) |&gt; \n                  mutate(sample = \"test\")\n              )\n          }\n        ) |&gt; \n          list_rbind(names_to = \"method\")\n      },\n      .progress = TRUE\n    ) |&gt; \n    list_rbind() |&gt; \n    group_by(method, sample, bins) |&gt; \n    summarise(\n      n_bins = mean(n_bins),\n      .groups = \"drop\"\n    )\n}\n\nLet us get the those values!\n\n## Regression----\n### No Calibration----\nn_bins_no_calib_reg &lt;- \n  count_scores_simul(scores_no_calib_reg_train) |&gt; \n  mutate(sample = \"train\", method = \"No Calibration\", type = \"Regression\") |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_reg_calib) |&gt; \n      mutate(sample = \"calibration\", method = \"No Calibration\", type = \"Regression\")\n  ) |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_reg_test) |&gt; \n      mutate(sample = \"test\", method = \"No Calibration\", type = \"Regression\")\n  )\n### With Recalibration methods----\nn_bins_recalib_reg &lt;- count_scores_simul_method(\n  scores_simul_methods = metrics_rf_tuned_reg) |&gt; \n  mutate(type = \"Regression\")\n\n## Regression----\n### No Calibration----\nn_bins_no_calib_class &lt;- \n  count_scores_simul(scores_no_calib_class_train) |&gt; \n  mutate(sample = \"train\", method = \"No Calibration\", type = \"Classification\") |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_class_calib) |&gt; \n      mutate(sample = \"calibration\", method = \"No Calibration\", type = \"Classification\")\n  ) |&gt; \n  bind_rows(\n    count_scores_simul(scores_no_calib_class_test) |&gt; \n      mutate(sample = \"test\", method = \"No Calibration\", type = \"Classification\")\n  )\n### With Recalibration methods----\nn_bins_recalib_class &lt;- count_scores_simul_method(\n  scores_simul_methods = metrics_rf_tuned_class) |&gt; \n  mutate(type = \"Classification\")\n\n## Merge all simulations----\nn_bins &lt;- \n  n_bins_no_calib_reg |&gt; \n  bind_rows(n_bins_recalib_reg) |&gt; \n  bind_rows(n_bins_no_calib_class) |&gt; \n  bind_rows(n_bins_recalib_class)\n\nn_bins &lt;- \n  n_bins |&gt; \n  filter(sample %in% c(\"calibration\", \"test\")) |&gt; \n  mutate(\n    sample = factor(\n      sample, \n      levels = c(\"calibration\", \"test\"), \n      labels = c(\"Calibration\", \"Test\")\n    ),\n    type = factor(\n      type,\n      levels = c(\"Regression\", \"Classification\"),\n      labels = c(\"Regression\", \"Classification\")\n    ),\n    method = factor(\n      method,\n      levels = !!methods,\n      labels = !!methods_labs\n    )\n  )\n\n\nload(\"output/n_bins.rda\")\n\n\n\n8.5.1 Visualization\nSome cosmetics:\n\ncolours_samples &lt;- c(\n  \"Train\" = \"#0072B2\", \"Calibration\" = \"#D55E00\", \"Test\" = \"#009E73\"\n)\ncolours_calib &lt;- c(\n  # \"#332288\", \n  \"#117733\",\n  \"#44AA99\", \"#88CCEE\",\n  \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\")\n\nThe plot_calib_locfit_simuls() allows us to visualize the calibration curves (on the train set and on the test set) for a type of forests (regression or classification), for uncalibrated scores and recalibrated ones.\n\nplot_calib_locfit_simuls &lt;- function(calib_curve, type) {\n  tb_calib_curve &lt;- calib_curves |&gt;filter(type == !!type)\n  tb_n_bins &lt;- n_bins |&gt; filter(type == !!type)\n  \n  nb_methods &lt;- tb_calib_curve$method |&gt; unique() |&gt; length()\n  \n  mat_init &lt;- c(1, 1, 2, 4, 3, 5) |&gt; matrix(ncol = 2, byrow = TRUE)\n  mat &lt;- mat_init\n  for (j in 2:(ceiling(nb_methods/2))) {\n    mat &lt;- rbind(mat, mat_init + (5*(j-1)))\n  }\n  mat &lt;- cbind(mat, mat + max(mat))\n  \n  layout(mat, height = rep(c(.35, 1, 3), nb_methods))\n  \n  y_lim &lt;- c(0, 1)\n  \n  samples &lt;- c(\"Calibration\", \"Test\")\n  \n  for (i_method in 1:length(methods)) {\n    method &lt;- methods_labs[i_method]\n    # Title for the models\n    par(mar = c(0, 0.5, 0, 0.5))\n    plot(\n      0:1, 0:1,\n      col = NULL,\n      type=\"n\",\n      xlim = c(0,2), ylim = 0:1,\n      xlab = \"\",\n      ylab = \"\",\n      main = \"\",\n      xaxt = \"n\",\n      yaxt = \"n\",\n      new = TRUE,\n      frame.plot = FALSE\n    )\n    # Get the center coordinates of the plot\n    plot_center &lt;- c(mean(par(\"usr\")[1:2]), mean(par(\"usr\")[3:4]))\n    # Get the width and height of the text\n    text_dim &lt;- strwidth(method, cex = 1.2, font = 2)\n    # Calculate the coordinates to center the text\n    text_x &lt;- plot_center[1] - text_dim / 2\n    text_y &lt;- plot_center[2]\n    \n    # Add text to the plot at the centered coordinates\n    text(\n      1, text_y,\n      method, col = colours_calib[i_method],\n      cex = 1, font = 2\n    )\n    for(i_sample in 1:length(samples)) {\n      sample &lt;- samples[i_sample]\n      tb_plot &lt;- tb_calib_curve |&gt; \n        filter(\n          sample == !!sample,\n          method == !!method,\n          type == !!type\n        )\n      n_bins_current &lt;- tb_n_bins |&gt; \n        filter(\n          sample == !!sample,\n          method == !!method,\n          type == !!type\n        )\n      n_bins_no_calib_current &lt;- tb_n_bins |&gt; \n        filter(\n          sample == !!sample,\n          method == \"No Calibration\",\n          type == !!type\n        )\n      # Barplots on top\n      par(mar = c(0.5, 4.3, 1.0, 0.5))\n      y_lim_bp &lt;- range(\n        c(n_bins_no_calib_current$n_bins,\n          n_bins_current$n_bins, na.rm = TRUE)\n      )\n      barplot(\n        n_bins_no_calib_current$n_bins,\n        col = adjustcolor(\"gray\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\"\n      )\n      barplot(\n        n_bins_current$n_bins,\n        col = adjustcolor(\"#0072B2\", alpha.f = .3),\n        ylim = y_lim_bp,\n        border = \"white\",\n        axes = FALSE,\n        xlab = \"\", ylab = \"\", main = \"\",\n        add = TRUE\n      )\n      \n      # Calib curve\n      par(mar = c(4.1, 4.3, 0.5, 0.5), mgp = c(2, 1, 0))\n      plot(\n        tb_plot$xlim, tb_plot$mean,\n        type = \"l\", col = colours_samples[sample],\n        xlim = 0:1, ylim = 0:1,\n        xlab = latex2exp::TeX(\"$p^u$\"), \n        ylab = latex2exp::TeX(\"$E(D | p^u = p^c)$\"),\n        main = \"\"\n      )\n      polygon(\n        c(tb_plot$xlim, rev(tb_plot$xlim)),\n        c(tb_plot$lower, rev(tb_plot$upper)),\n        col = adjustcolor(col = colours_samples[sample], alpha.f = .4),\n        border = NA\n      )\n      segments(0, 0, 1, 1, col = \"black\", lty = 2)\n    }\n  }\n  \n}\n\n\nplot_calib_locfit_simuls(\n  calib_curve = calib_curves, type = \"Regression\"\n)\n\n\n\nFigure 8.6: Calibration Curves Obtained with Local Regression, for the Regression Random Forest, for the  Calibration Set and for the Test Set. The curves are the averages values obtained on 200 different splits of the calibration and test datasets, and the color bands are the 95% bootstrap confidence intervals. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores.\n\n\n\n\n\n\nplot_calib_locfit_simuls(\n  calib_curve = calib_curves, type = \"Classification\"\n)\n\n\n\nFigure 8.7: Calibration Curves Obtained with Local Regression, for the Classification Random Forest, for the  Calibration Set and for the Test Set. The curves are the averages values obtained on 200 different splits of the calibration and test datasets, and the color bands are the 95% bootstrap confidence intervals. The histogram on top of each graph show the distribution of the uncalibrated scores, and that of the calibrated scores."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kull, Meelis, Telmo M. Silva Filho, and Peter Flach. 2017. “Beyond\nSigmoids: How to Obtain Well-Calibrated Probabilities from Binary\nClassifiers with Beta Calibration.” Electronic Journal of\nStatistics 11 (2). https://doi.org/10.1214/17-ejs1338si.\n\n\nPlatt, John et al. 1999. “Probabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods.”\nAdvances in Large Margin Classifiers 10 (3): 61–74.\n\n\nSubasi A., Cankur S. 2019. “Prediction of Default Payment of\nCredit Card Clients Using Data Mining Techniques.” Fifth\nInternational Engineering Conference on Developments in Civil &\nComputer Engineering Applications 2019 - (IEC2019) - Erbil - IRAQ.\nhttps://doi.org/10.1109/IEC47844.2019.8950597.\n\n\nYeh, I-Cheng. 2016. “Default of credit card\nclients.” UCI Machine Learning Repository.\n\n\nZadrozny, Bianca, and Charles Elkan. 2002. “Transforming\nClassifier Scores into Accurate Multiclass Probability\nEstimates.” In Proceedings of the Eighth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining.\nKDD02. ACM. https://doi.org/10.1145/775047.775151."
  }
]